(window.webpackJsonp=window.webpackJsonp||[]).push([["d0a3"],{"+a94":function(e){e.exports={bodyContent:"**Program Committee**: TEI 2021\n\n**Organizer:** CHI 2021 (Social Media Chair), UIST 2016 (Web and Social Media Chair),\n\n**Review:** CHI 2016--2020, UIST 2017--2019, SCF 2019, SIGGRAPH ETech 2019, IEEE VR 2020, GI 2020\n\n**Student Volunteer:** UIST 2016, CHI 2017",bodyHtml:"<p><strong>Program Committee</strong>: TEI 2021</p>\n<p><strong>Organizer:</strong> CHI 2021 (Social Media Chair), UIST 2016 (Web and Social Media Chair),</p>\n<p><strong>Review:</strong> CHI 2016--2020, UIST 2017--2019, SCF 2019, SIGGRAPH ETech 2019, IEEE VR 2020, GI 2020</p>\n<p><strong>Student Volunteer:</strong> UIST 2016, CHI 2017</p>\n",title:"<strong>Program Committee",dir:"content/output",base:"activities.json",ext:".json",sourceBase:"activities.md",sourceExt:".md"}},"3RXq":function(e){e.exports={id:"lift-tiles",name:"LiftTiles",description:"Constructive Building Blocks for Prototyping Room-scale Shape-changing Interfaces",title:"LiftTiles: Constructive Building Blocks for Prototyping Room-scale Shape-changing Interfaces",authors:["Ryo Suzuki","Ryosuke Nakayama","Dan Liu","Yasuaki Kakehi","Mark D. Gross","Daniel Leithinger"],year:2020,booktitle:"In Proceedings of the 14th ACM International Conference on Tangible, Embedded and Embodied Interaction (TEI '20)",publisher:"ACM, New York, NY, USA",pages:"143–151",doi:"https://doi.org/10.1145/3374920.3374941",conference:{name:"TEI 2020",fullname:"The ACM International Conference on Tangible, Embedded and Embodied Interaction (TEI 2020)",url:"https://tei.acm.org/2020/"},video:"https://www.youtube.com/watch?v=0LHeTkOMR84",embed:"https://www.youtube.com/embed/0LHeTkOMR84",pdf:"tei-2020-lift-tiles.pdf",slide:"tei-2020-lift-tiles-slide.pdf",poster:"uist-2019-lift-tiles-poster.pdf",pageCount:9,slideCount:37,related:{title:"LiftTiles: Modular and Reconfigurable Room-scale Shape Displays through Retractable Inflatable Actuators",authors:["Ryo Suzuki","Ryosuke Nakayama","Dan Liu","Yasuaki Kakehi","Mark D. Gross","Daniel Leithinger"],year:2019,booktitle:"In Adjunct Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology (UIST '19)",publisher:"ACM, New York, NY, USA",pages:"1-3",doi:"https://doi.org/10.1145/3332167.3357105",url:"http://uist.acm.org/uist2019",pdf:"uist-2019-lift-tiles.pdf",suffix:"adjunct",pageCount:3},bodyContent:'# Abstract\n\nLarge-scale shape-changing interfaces have great potential, but creating such systems requires substantial time, cost, space, and efforts, which hinders the research community to explore inter- actions beyond the scale of human hands. We introduce modular inflatable actuators as building blocks for prototyping room-scale shape-changing interfaces. Each actuator can change its height from 15cm to 150cm, actuated and controlled by air pressure. Each unit is low-cost (8 USD), lightweight (10 kg), compact (15 cm), and robust (e.g., can support 10 kg weight) making it well-suited for prototyping room-scale shape transformations. Moreover, our mod- ular and reconfigurable design allows researchers and designers to quickly construct different geometries and to explore various appli- cations. This paper contributes to the design and implementation of highly extendable inflatable actuators, and demonstrates a range of scenarios that can leverage this modular building block.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/lift-tiles/video/top.webm" type="video/webm"></source>\n  <source src="/static/projects/lift-tiles/video/top.mp4" type="video/mp4"></source>\n</video>\n\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-1-1.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-1-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-1-2.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-1-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-1-3.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-1-3.jpg" /></a>\n  </div>\n</div>\n\n\x3c!--\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-2-1.png" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-2-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-2-2.png" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-2-2.png" /></a>\n  </div>\n</div>\n --\x3e\n\n\n# LiftTiles: Room-scale Modular Inflatable Actuators\n\nThis paper introduces LiftTiles, modular inflatable actuators for prototyping room-scale shape-changing interfaces. Each inflatable actuator has a large footprint (e.g., 30 cm x 30 cm) and enables large-scale shape transformation. The ac- tuator is fabricated from a flexible plastic tube and constant force springs. It extends when inflated and retracts by the force of its spring when deflated. By controlling the internal air volume, the actuator can change its height from 15 cm to 150 cm. We designed each module as low cost (e.g., 8 USD), lightweight (e.g., 1.8kg), and robust (e.g., with- stand more than 10 kg weight), so that it is suitable for rapid prototyping of room-sized interfaces. Our design utilizes constant force springs to provide greater scalability, simplified fabrication, and stronger retraction force, all essential for large-scale shape-change.\n\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/lift-tiles/video/unit.webm" type="video/webm"></source>\n  <source src="/static/projects/lift-tiles/video/unit.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-3-1.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-3-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-3-2.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-3-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-3-3.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-3-3.jpg" /></a>\n  </div>\n</div>\n\nOur inflatable actuator leverages an extendable structure similar to a party horn. Each inflatable actuator consists of a flexible plastic tube and two constant force springs, which are rolled at their resting positions (Figure 2). When pumping air into the tube, the actuator extends as the internal air pressure increases and the end of the tube unwinds. When releasing air through a release valve, the inflatable tube retracts due to the force of the embedded spring returning to its resting position.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/lift-tiles/video/unit-animation.webm" type="video/webm"></source>\n  <source src="/static/projects/lift-tiles/video/unit-animation.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-4-1.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-4-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-4-2.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-4-2.jpg" /></a>\n  </div>\n</div>\n\nThe goal of our system is to provide an accessible prototyping building block for room-scale shape-changing interfaces. To achieve this goal, we set the following three design require- ments:\n\n- **Fast**: LIftTiles provide a means to quickly mock up a room-size shape-changing interface\n\n- **Extendable**:  range (from 15cm to 150cm),\n\n- **Low-cost** (8 USD per tile),\n\n- **Light**: (10 kg per tile),\n\n- **Compact**: (Each tile compresses to 15 cm)\n\n- **Modular**: (can reconfigure the arrangement)\n\n- **Strong**: (each tile supports up 10 kg)\n\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-5-1.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-5-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-5-2.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-5-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-5-3.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-5-3.jpg" /></a>\n  </div>\n</div>\n\n\x3c!--\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/lift-tiles/video/cad.webm" type="video/webm"></source>\n  <source src="/static/projects/lift-tiles/video/cad.mp4" type="video/mp4"></source>\n</video>\n --\x3e\n\n# Modular Design\n\nEach actuator is modular and can connect with the air supply of neighboring actuators. Each solenoid air intake valve is connected to a T-fitting. Adjacent actua- tors are pneumatically connected with a silicon tube between the T-fittings (Figure 6). This way, an array of actuators is connected to a shared pressurized supply line.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/lift-tiles/video/modular.webm" type="video/webm"></source>\n  <source src="/static/projects/lift-tiles/video/modular.mp4" type="video/mp4"></source>\n</video>\n\n</br>\n\nAlso, due to the relatively compact size and small weight of the modules, various ar- chitectural surfaces can be considered for installation, such as placed on the floor or installed sideways from a wall. The actuators can be arranged in a grid, a line, or as individ- ual, loosely arranged units. Figure 10 depicts a few example configurations, which can also be reconfigured by the person installing them or the end user. An individual actuator can be picked up and moved, which may let the user handle it like a traditional piece of furniture, but could also open up new possibilities for interaction, such as actuators with a motorized base that reconfigures the arrangement according to the current use case.\n\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/lift-tiles/video/tile-animation.webm" type="video/webm"></source>\n  <source src="/static/projects/lift-tiles/video/tile-animation.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-6-1.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-6-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-6-2.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-6-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-6-3.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-6-3.jpg" /></a>\n  </div>\n</div>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-7-1.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-7-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-7-2.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-7-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-7-3.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-7-3.jpg" /></a>\n  </div>\n</div>\n\n\n# Prototyping Applications\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-10.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-10.jpg" /></a>\n  </div>\n</div>\n\nTo gauge how well LiftTiles is suited to our goal of supporting rapid prototyping and user testing, we built several differ- ent example applications.\n\n- **Adaptive Floor Furniture**: The first application is a recon- figurable floor that creates adaptive furniture, inspired by an adaptive dynamic table (e.g., TRANSFORM [45]) and dy- namic physical affordances (e.g., inFORM [8]). We created a shape-changing floor by arranging 25 modules in a 5 x 5 grid. Each unit is individually actuated to provide a chair, a table, and a bed (Figure 1).\n\n- **Landscape Haptics for VR**: While hand-sized shape displays have rendered haptic sensations for VR [34]), we propose to render furniture-size props, walls, and game elements that users can feel through walking and touching while experi- encing immersive VR scenes. Similar to the applications pro- posed by HapticTurk [3], TurkDeck [4], and TilePop [44], In this application, the haptics increases the feeling of presence in games by providing room-scale dynamic haptic feedback. Also, this can aid the design process by rendering full-scale object mock-ups of large objects like a car.\n\n- **Shape-changing Wall**: By leveraging the block’s compact form factor, we also prototyped a horizontal shape-changing wall to adapt to user needs and display information. The wall becomes a space separator to make a temporary meet- ing space, while it can also act as public signage by displaying an arrow shape to guide a passer-by.\n\n- **Deployable Pop-Up Structure**: As the actuators are compact compared to traditional furniture and walls, they can be deployed in temporary use cases such as festivals, trade shows, concerts, and disaster relief. In these applications, the actuators are shipped in a compact box and laid out in an empty room, similar to tiles. After connecting them to a compressor and control computer, the operator selects a use case and the blocks render a layout of chairs, beds, tables, and partitions.\n\n\n<div class="figures ui four column grid">\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-8-1.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-8-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-8-2.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-8-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-8-3.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-8-3.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-8-4.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-8-4.jpg" /></a>\n  </div>\n</div>\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/lift-tiles/video/wall-animation.webm" type="video/webm"></source>\n  <source src="/static/projects/lift-tiles/video/wall-animation.mp4" type="video/mp4"></source>\n</video>\n\n\n<div class="figures ui four column grid">\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-2-1.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-2-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-2-2.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-2-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-9-1.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-9-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-9-2.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-9-2.jpg" /></a>\n  </div>\n</div>\n\n\n# Conclusion\n\nThis paper introduces constructive building blocks for proto- typing room-scale shape-changing interfaces. To this end, we designed a modular inflatable actuator that is highly extend- able (from 15cm to 150cm), low-cost (8 USD), lightweight (10 kg), compact (15 cm), and strong (e.g., can support 10 kg weight) making it well-suited for prototyping room-scale shape transformations. Moreover, our modular and recon- figurable design allows researchers and designers to quickly construct different geometries and to investigate various ap- plications. In this paper, we contributed to the design and implementation of highly extendable inflatable actuators, and demonstrates a range of scenarios that can leverage this modular building block. We plan to evaluate the potential of our system by inviting designers to use our blocks in their own designs and see whether and/or how our building blocks enhances their ideation process.',bodyHtml:'<h1>Abstract</h1>\n<p>Large-scale shape-changing interfaces have great potential, but creating such systems requires substantial time, cost, space, and efforts, which hinders the research community to explore inter- actions beyond the scale of human hands. We introduce modular inflatable actuators as building blocks for prototyping room-scale shape-changing interfaces. Each actuator can change its height from 15cm to 150cm, actuated and controlled by air pressure. Each unit is low-cost (8 USD), lightweight (10 kg), compact (15 cm), and robust (e.g., can support 10 kg weight) making it well-suited for prototyping room-scale shape transformations. Moreover, our mod- ular and reconfigurable design allows researchers and designers to quickly construct different geometries and to explore various appli- cations. This paper contributes to the design and implementation of highly extendable inflatable actuators, and demonstrates a range of scenarios that can leverage this modular building block.</p>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/lift-tiles/video/top.webm" type="video/webm"></source>\n  <source src="/static/projects/lift-tiles/video/top.mp4" type="video/mp4"></source>\n</video>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-1-1.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-1-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-1-2.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-1-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-1-3.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-1-3.jpg" /></a>\n  </div>\n</div>\n\x3c!--\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-2-1.png" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-2-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-2-2.png" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-2-2.png" /></a>\n  </div>\n</div>\n --\x3e\n<h1>LiftTiles: Room-scale Modular Inflatable Actuators</h1>\n<p>This paper introduces LiftTiles, modular inflatable actuators for prototyping room-scale shape-changing interfaces. Each inflatable actuator has a large footprint (e.g., 30 cm x 30 cm) and enables large-scale shape transformation. The ac- tuator is fabricated from a flexible plastic tube and constant force springs. It extends when inflated and retracts by the force of its spring when deflated. By controlling the internal air volume, the actuator can change its height from 15 cm to 150 cm. We designed each module as low cost (e.g., 8 USD), lightweight (e.g., 1.8kg), and robust (e.g., with- stand more than 10 kg weight), so that it is suitable for rapid prototyping of room-sized interfaces. Our design utilizes constant force springs to provide greater scalability, simplified fabrication, and stronger retraction force, all essential for large-scale shape-change.</p>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/lift-tiles/video/unit.webm" type="video/webm"></source>\n  <source src="/static/projects/lift-tiles/video/unit.mp4" type="video/mp4"></source>\n</video>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-3-1.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-3-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-3-2.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-3-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-3-3.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-3-3.jpg" /></a>\n  </div>\n</div>\n<p>Our inflatable actuator leverages an extendable structure similar to a party horn. Each inflatable actuator consists of a flexible plastic tube and two constant force springs, which are rolled at their resting positions (Figure 2). When pumping air into the tube, the actuator extends as the internal air pressure increases and the end of the tube unwinds. When releasing air through a release valve, the inflatable tube retracts due to the force of the embedded spring returning to its resting position.</p>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/lift-tiles/video/unit-animation.webm" type="video/webm"></source>\n  <source src="/static/projects/lift-tiles/video/unit-animation.mp4" type="video/mp4"></source>\n</video>\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-4-1.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-4-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-4-2.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-4-2.jpg" /></a>\n  </div>\n</div>\n<p>The goal of our system is to provide an accessible prototyping building block for room-scale shape-changing interfaces. To achieve this goal, we set the following three design require- ments:</p>\n<ul>\n<li>\n<p><strong>Fast</strong>: LIftTiles provide a means to quickly mock up a room-size shape-changing interface</p>\n</li>\n<li>\n<p><strong>Extendable</strong>:  range (from 15cm to 150cm),</p>\n</li>\n<li>\n<p><strong>Low-cost</strong> (8 USD per tile),</p>\n</li>\n<li>\n<p><strong>Light</strong>: (10 kg per tile),</p>\n</li>\n<li>\n<p><strong>Compact</strong>: (Each tile compresses to 15 cm)</p>\n</li>\n<li>\n<p><strong>Modular</strong>: (can reconfigure the arrangement)</p>\n</li>\n<li>\n<p><strong>Strong</strong>: (each tile supports up 10 kg)</p>\n</li>\n</ul>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-5-1.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-5-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-5-2.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-5-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-5-3.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-5-3.jpg" /></a>\n  </div>\n</div>\n\x3c!--\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/lift-tiles/video/cad.webm" type="video/webm"></source>\n  <source src="/static/projects/lift-tiles/video/cad.mp4" type="video/mp4"></source>\n</video>\n --\x3e\n<h1>Modular Design</h1>\n<p>Each actuator is modular and can connect with the air supply of neighboring actuators. Each solenoid air intake valve is connected to a T-fitting. Adjacent actua- tors are pneumatically connected with a silicon tube between the T-fittings (Figure 6). This way, an array of actuators is connected to a shared pressurized supply line.</p>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/lift-tiles/video/modular.webm" type="video/webm"></source>\n  <source src="/static/projects/lift-tiles/video/modular.mp4" type="video/mp4"></source>\n</video>\n</br>\n<p>Also, due to the relatively compact size and small weight of the modules, various ar- chitectural surfaces can be considered for installation, such as placed on the floor or installed sideways from a wall. The actuators can be arranged in a grid, a line, or as individ- ual, loosely arranged units. Figure 10 depicts a few example configurations, which can also be reconfigured by the person installing them or the end user. An individual actuator can be picked up and moved, which may let the user handle it like a traditional piece of furniture, but could also open up new possibilities for interaction, such as actuators with a motorized base that reconfigures the arrangement according to the current use case.</p>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/lift-tiles/video/tile-animation.webm" type="video/webm"></source>\n  <source src="/static/projects/lift-tiles/video/tile-animation.mp4" type="video/mp4"></source>\n</video>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-6-1.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-6-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-6-2.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-6-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-6-3.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-6-3.jpg" /></a>\n  </div>\n</div>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-7-1.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-7-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-7-2.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-7-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-7-3.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-7-3.jpg" /></a>\n  </div>\n</div>\n<h1>Prototyping Applications</h1>\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-10.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-10.jpg" /></a>\n  </div>\n</div>\n<p>To gauge how well LiftTiles is suited to our goal of supporting rapid prototyping and user testing, we built several differ- ent example applications.</p>\n<ul>\n<li>\n<p><strong>Adaptive Floor Furniture</strong>: The first application is a recon- figurable floor that creates adaptive furniture, inspired by an adaptive dynamic table (e.g., TRANSFORM [45]) and dy- namic physical affordances (e.g., inFORM [8]). We created a shape-changing floor by arranging 25 modules in a 5 x 5 grid. Each unit is individually actuated to provide a chair, a table, and a bed (Figure 1).</p>\n</li>\n<li>\n<p><strong>Landscape Haptics for VR</strong>: While hand-sized shape displays have rendered haptic sensations for VR [34]), we propose to render furniture-size props, walls, and game elements that users can feel through walking and touching while experi- encing immersive VR scenes. Similar to the applications pro- posed by HapticTurk [3], TurkDeck [4], and TilePop [44], In this application, the haptics increases the feeling of presence in games by providing room-scale dynamic haptic feedback. Also, this can aid the design process by rendering full-scale object mock-ups of large objects like a car.</p>\n</li>\n<li>\n<p><strong>Shape-changing Wall</strong>: By leveraging the block’s compact form factor, we also prototyped a horizontal shape-changing wall to adapt to user needs and display information. The wall becomes a space separator to make a temporary meet- ing space, while it can also act as public signage by displaying an arrow shape to guide a passer-by.</p>\n</li>\n<li>\n<p><strong>Deployable Pop-Up Structure</strong>: As the actuators are compact compared to traditional furniture and walls, they can be deployed in temporary use cases such as festivals, trade shows, concerts, and disaster relief. In these applications, the actuators are shipped in a compact box and laid out in an empty room, similar to tiles. After connecting them to a compressor and control computer, the operator selects a use case and the blocks render a layout of chairs, beds, tables, and partitions.</p>\n</li>\n</ul>\n<div class="figures ui four column grid">\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-8-1.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-8-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-8-2.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-8-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-8-3.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-8-3.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-8-4.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-8-4.jpg" /></a>\n  </div>\n</div>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/lift-tiles/video/wall-animation.webm" type="video/webm"></source>\n  <source src="/static/projects/lift-tiles/video/wall-animation.mp4" type="video/mp4"></source>\n</video>\n<div class="figures ui four column grid">\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-2-1.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-2-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-2-2.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-2-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-9-1.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-9-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/lift-tiles/figure-9-2.jpg" data-lightbox="lightbox"><img src="/static/projects/lift-tiles/figure-9-2.jpg" /></a>\n  </div>\n</div>\n<h1>Conclusion</h1>\n<p>This paper introduces constructive building blocks for proto- typing room-scale shape-changing interfaces. To this end, we designed a modular inflatable actuator that is highly extend- able (from 15cm to 150cm), low-cost (8 USD), lightweight (10 kg), compact (15 cm), and strong (e.g., can support 10 kg weight) making it well-suited for prototyping room-scale shape transformations. Moreover, our modular and recon- figurable design allows researchers and designers to quickly construct different geometries and to investigate various ap- plications. In this paper, we contributed to the design and implementation of highly extendable inflatable actuators, and demonstrates a range of scenarios that can leverage this modular building block. We plan to evaluate the potential of our system by inviting designers to use our blocks in their own designs and see whether and/or how our building blocks enhances their ideation process.</p>\n',dir:"content/output/projects",base:"lift-tiles.json",ext:".json",sourceBase:"lift-tiles.md",sourceExt:".md"}},"42TL":function(e,t,i){"use strict";i.r(t);var a=i("0iUn"),s=i("sLSF"),o=i("MI3g"),n=i("a7VT"),r=i("Tit0"),c=i("q1tI"),l=i.n(c),p=(i("IujW"),i("MyTI")),h=function(e){function t(){return Object(a.default)(this,t),Object(o.default)(this,Object(n.default)(t).apply(this,arguments))}return Object(r.default)(t,e),Object(s.default)(t,[{key:"render",value:function(){return l.a.createElement("div",{id:"updates",className:"ui relaxed divided list"},l.a.createElement("h3",null,"Research Experience"),p.map(function(e){return l.a.createElement("div",{className:"item",style:{padding:"20px 0"}},l.a.createElement("div",{className:"ui mini image"},l.a.createElement("img",{src:"/static/images/".concat(e.logo)})),l.a.createElement("div",{className:"middle aligned content"},l.a.createElement("div",{className:"header"},e.institute.name,l.a.createElement("br",null)),l.a.createElement("div",{className:"description"},l.a.createElement("a",{href:e.lab.url,target:"_blank"},l.a.createElement("b",null,e.lab.name)))),l.a.createElement("div",{className:"content"},l.a.createElement("div",{className:"ui list"},e.advisors.map(function(e){return l.a.createElement("div",{className:"item"},l.a.createElement("a",{href:e.url,target:"_blank"},e.name))}))),l.a.createElement("div",{className:"extra",style:{marginLeft:"0px",marginTop:"5px"}},l.a.createElement("div",{className:"ui label"},e.period)))}))}}]),t}(l.a.Component);t.default=h},"6T/A":function(e){e.exports={}},"9Gwv":function(e){e.exports=[{date:"2020-07-28",text:"**PhD Dissertation** is submitted and approved."},{date:"2020-07-01",text:"One [**IROS 2020**](https://www.iros2020.org/) full-paper is accepted (47%)."},{date:"2020-06-24",text:"One [**UIST 2020**](http://uist.acm.org/uist2020/) full-paper is accepted (21%)."},{date:"2020-05-18",text:"Start intern at [**Microsoft Research**](https://www.microsoft.com/en-us/research/) Redmond."},{date:"2020-05-13",text:"**PhD Defense** ... and Passed! :)"},{date:"2019-12-12",text:"One [**CHI 2020**](https://chi2020.acm.org/) full-paper is accepted (24%)."},{date:"2019-11-25",text:"Traveling to Boston to visit **MIT CSAIL** and **Media Lab**"},{date:"2019-10-19",image:"uist-2019.png",text:"Traveling to New Orleans for [**UIST 2019**](http://uist.acm.org/uist2019/)"},{date:"2019-10-10",image:"uist-2019.png",text:"Finished comprehensive exam and became a **PhD candidate** (committee: D Leithinger, M Gross, T Yeh, H Ishii, T Igarashi, E Do)"},{date:"2019-10-08",image:"uist-2019.png",text:"One [**TEI 2020**](https://tei.acm.org/2020/) full-paper is accepted (28%)."},{date:"2019-08-02",image:"uist-2019.png",text:"One [**UIST 2019**](http://uist.acm.org/uist2019/) poster and and doctoral consortium paper are accepted."},{date:"2019-07-01",image:"uist-2019.png",text:"One [**UIST 2019**](http://uist.acm.org/uist2019/) full-paper is accepted (24%)."},{date:"2019-06-27",image:"uist-2019.png",text:"Won [**Best Paper Award**](https://dis2019.com/overview/#track-4-shape-changing-interfaces) for [**DIS 2019**](https://dis2019.com/) (Top 1%)."},{date:"2019-05-20",image:"uist-2019.png",text:"Start intern at [**Adobe Research**](https://research.adobe.com/) in Seattle."},{date:"2019-03-26",image:"dis-2019.png",text:"One [**DIS 2019**](https://dis2019.com/) full-paper is accepted (25%)."},{date:"2018-08-15",text:"Award [**JST ACT-I**](https://www.jst.go.jp/kisoken/act-i/en/project/111C001/111C001_2018.html) funding (mentor [**Takeo Igarashi**](https://www-ui.is.s.u-tokyo.ac.jp/~takeo/))."},{date:"2018-08-06",image:"uist-2018.png",text:"One [**UIST 2018**](https://uist.acm.org/uist2018/) full-paper is accepted (21%)."},{date:"2018-08-03",image:"pg-2018.png",text:"One [**Pacific Graphics 2018**](http://sweb.cityu.edu.hk/pg2018/) short-paper is accepted (26%)."},{date:"2017-12-14",text:"Start intern at [**University of Tokyo**](http://www.jst.go.jp/erato/kawahara/)."},{date:"2017-12-11",image:"chi-2018.png",text:"Two [**CHI 2018**](https://chi2018.acm.org/) full-papers are accepted (25%)."},{date:"2017-06-27",image:"vlhcc-2017.png",text:"One [**VL/HCC 2017**](https://sites.google.com/site/vlhcc2017/) full-paper are accepted (29%)."},{date:"2017-06-21",image:"assets-2017.png",text:"One [**ASSETS 2017**](https://assets17.sigaccess.org/) full-paper are accepted (26%)."},{date:"2017-02-11",image:"chi-2017.png",text:"One [**CHI 2017**](https://chi2017.acm.org/) LBW paper is accepted (38%)."},{date:"2016-12-16",image:"",text:"One [**L@S 2017**](http://learningatscale.acm.org/las2017/) full paper is accepted (22%)."},{date:"2016-12-12",image:"icse-2017.png",text:"One [**ICSE 2017**](http://icse2017.gatech.edu/) full-paper is accepted (19%)."},{date:"2019-05-23",text:"Start intern at [**UC Berkeley**](http://bid.berkeley.edu/)."},{date:"2016-01-15",image:"chi-2016.png",text:"One [**CHI 2016**](https://chi2016.acm.org/wp/) full-paper is accepted (23%)."},{date:"2015-10-01",image:"uist-2016.jpg",text:"I will serve as a web and social media chair for [**UIST 2016**](http://uist.acm.org/uist2016/)"},{date:"2019-05-18",text:"Start intern at [**Stanford**](https://hci.stanford.edu/)."},{date:"2014-10-15",text:"The first day to start HCI research (previously economics/game theory)"}]},"9gtZ":function(e){e.exports={id:"roomshift",name:"RoomShift",description:"Room-scale Dynamic Haptics for VR with Furniture-moving Swarm Robots",title:"RoomShift: Room-scale Dynamic Haptics for VR with Furniture-moving Swarm Robots",authors:["Ryo Suzuki","Hooman Hedayati","Clement Zheng","James Bohn","Daniel Szafir","Ellen Yi-Luen Do","Mark D Gross","Daniel Leithinger"],year:2020,booktitle:"In Proceedings of the ACM CHI Conference on Human Factors in Computing Systems (CHI '20)",publisher:"ACM, New York, NY, USA",pages:"Paper 396, 11 pages",doi:"https://doi.org/10.1145/3313831.3376523",conference:{name:"CHI 2020",fullname:"The ACM CHI Conference on Human Factors in Computing Systems (CHI 2020)",url:"https://chi2020.acm.org/"},pdf:"chi-2020-roomshift.pdf",pageCount:11,slideCount:0,bodyContent:'video coming soon.\n\n# Abstract\n\nThis paper presents RoomShift, a room-scale dynamic haptic environment for virtual reality, using a small swarm of robots that can move furniture. RoomShift consists of nine shape-changing robots: Roombas with mechanical scissor lifts. These robots drive beneath a piece of furniture to lift, move and place it. By augmenting virtual scenes with physical objects, users can sit on, lean against, place and otherwise interact with furniture with their whole body; just as in the real world. When the virtual scene changes or users navigate within it, the swarm of robots dynamically reconfigures the physical environment to match the virtual content. We describe the hardware and software implementation, applications in virtual tours and architectural design and interaction techniques.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/roomshift/video/top.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-1-1.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-1-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-1-2.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-1-2.jpg" /></a>\n  </div>\n</div>\n\n# Introduction\n\nThere is a clear need to provide haptic sensations in virtual environments. Recent advances in display and tracking technologies promise immersive experience in virtual reality, but objects seen in VR such as walls and furniture are only visual: the user cannot touch, feel, sit on, or place objects on them. This limits the sense of full immersion in the virtual world. To overcome these limitations, various haptic interfaces have been explored. In the previous work, most haptic interfaces focus on finger-tip haptic feedback with actuated controllers or on-body haptic sensations with wearable devices. In contrast, encountered-type haptic feedback with a dynamic environment promises to increase the immersion of virtual experiences, which are difficult to achieve using an only handheld or wearable haptic devices. Through a dynamic haptic environment, users can touch and interact with the whole virtual scene with their bodies --- they can walk, sit on, and lean against objects in the VR environment. Existing approaches for actuated environments, however, are often limited in speed of transformation (e.g., slow transformation with inflatables) and the range of supported interactions (e.g., only walking).\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/roomshift/video/wall.mp4" type="video/mp4"></source>\n</video>\n\n\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-9-1.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-9-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-9-2.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-9-2.jpg" /></a>\n  </div>\n</div>\n\nThis paper introduces RoomShift, a room-scale dynamic haptic environment for virtual reality. RoomShift provides haptic sensations by reconfiguring physical environments using a small swarm of robot assistants. Inspired by shelf-moving robots that are used in robotic warehouses, we developed a swarm of shape-changing robots that can move a range of existing furniture. Each robot has a mechanical lift that extends from 30 cm to 100 cm to pick up, carry, and place objects such as chairs, tables, and walls. This way, users can touch, sit, place, and lean against objects in the virtual environment.\n\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-2-1.png" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-2-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-2-2.png" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-2-2.png" /></a>\n  </div>\n</div>\n\n\n# RoomShift: Furniture-moving Robots\n\nRoomShift consists of a small swarm of shape-changing robots; each robot uses a Roomba as a mobile base. On this base is mounted a custom mechanical scissor lift made of two linear actuators and a metal drying rack. As the mechanical lift is compact in its closed state, the robot can move under a table or chair with 30 cm clearance, and extend the scissor lift to pick it up.\n\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/roomshift/video/carry.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-3-1.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-3-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-3-2.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-3-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-3-3.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-3-3.jpg" /></a>\n  </div>\n</div>\n\nRoomShift comprises nine shape-changing swarm robots based on the Roomba Create 2. For the mechanical lift structure, we repurposed an off-the-shelf expandable laundry rack (Room Essentials Compact Drying Rack) and attached two linear actuators (Homend DC12V 8 inch Stroke Linear Actuator, which extends from 32 cm to 52 cm) at the base of the rack. The linear actuators are fixed to the endpoints of the scissor structure with 8 mm steel rods, so that when the actuator contracts, the mounted scissor structure extends vertically (from 30 cm to 100 cm). The scissor structure moves at a speed of 1.3 cm / sec. To mount the scissor structure, we fixed a 6mm acrylic bottom plate (35 cm x 35 cm) and four omni-directional casters (Dorhea Ball Transfer Bearing Unit) to relieve the Roomba of most of the weight that the robot carries. Each robot moves at 20 cm / sec. Figure 3 illustrates the mechanical design of each RoomShift robot.\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-4-1.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-4-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-4-2.png" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-4-2.png" /></a>\n  </div>\n</div>\n\nOne advantage of our approach is that the robot need not support the weight of the user. Once the robot places the furniture, it serves as a static object. Thus, when a user sits on or puts weight on it, all of the weight goes to the furniture, instead of the robot, which significantly reduces the possibility of a mechanical breakdown.\nAlthough the maximum load for the Roomba is 9 kg, the corner-mounted casters distribute and carry heavier loads. Thus, our robots can lift and carry heavier objects than an unmodified Roomba. The maximum weight the robot can lift and carry is 22 kg. When we put a heavier object than 23 kg, we observed the scissor structure started to break. The strength of the scissor structure suffices to lift lightweight chairs and tables, such as the IKEA honeycomb furniture used in our prototypes. The weight of the furniture we have tested (depicted in Figure) ranges from 3.5 to 11.2 kg. For heavier objects, multiple robots can also coordinate to lift a piece together if there is sufficient space under the furniture. Also, with a more robust scissor structure, we can carry heavier objects, as we observed the Roomba base itself (with the corner-mounted casters) can carry up to 30 kg load.\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-5-1.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-5-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-5-2.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-5-2.jpg" /></a>\n  </div>\n</div>\n\nThis approach also increases flexibility because different types of furniture can be actuated with the height-adjustable scissor lift. For example, Figure illustrates various static props that the RoomShift robot can actuate. These objects include furniture such as a desk, a long table, different chairs, and a side table. Note that due to the robot’s minimum collapsed size, objects must have at least 30 cm clearance below them, and enough horizontal space to fit the robot. A designer can also create custom props for specific applications, for instance, the styrofoam wall mounted to a side table seen in the Figure.\n\n\n# Tracking and Control\n\nTo accurately control the RoomShift robots, we require precise motion tracking that can cover the play area in which a user walks. We use an optical tracking system with 20 IR cameras (Qualisys Miqus 5) that can track objects in a 10 m × 10 m space. The system tracks six degrees of freedom (DOF) position of the objects\nwith retro-reflective spherical markers at 60 FPS frame rate.\nTo track the robots as well as physical props, we attached five 30 mm spherical retroreflective markers. For the robot, we attached markers to a pair of parallel bars, so that the markers’ relative positions remain constant regardless of the height of the scissor lift. We can also estimate the height of the scissor structure by measuring the orientation of the marker pattern (the pink plane surface depicted in the Figure).\n\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/roomshift/video/tracking.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-7-3.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-7-3.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-7-1.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-7-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-7-2.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-7-2.jpg" /></a>\n  </div>\n</div>\n\nTo control the robots’ movements, we use a simple path planning algorithm. The input is 1) the current positions of the robots, 2) the positions of obstacles (e.g., furniture, other robots, and users), and 3) the target locations. The algorithm outputs the goal of each robot at the next time step. The system continuously updates the path and drives them to their target locations. The main server continuously tracks the robot positions, calculates their wheel speeds, and sends commands at 30 Hz over WiFi.\n\nTo pick up and place these, the robot follows a predefined sequence, approaching the object from an angle where it will not collide with the object’s legs. To avoid the collision with the legs of furniture, each object has a user-defined entry and exit point (Figure 8). We also register the height of target furniture before the system starts (e.g., 70 cm for Table_A, 40 cm for Chair_B), so that it can extend the scissor lift to certain target height. We could also put a simple sensor on top of the scissor structure to make it a closed-loop system.\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-6-1.png" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-6-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-6-2.png" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-6-2.png" /></a>\n  </div>\n</div>\n\nThe main computer runs a Node.js server and the Qualisys tracking software. The 6DOF tracking data that the Qualisys tracking system captures is streamed to the Node.js server through the WebSocket protocol. Based on the tracking data, a web browser client renders the VR scene with A-Frame. The user experiences the VR scene using an Oculus Go head mounted display and its built-in VR browser. We synchronize the desktop computer and the Oculus Go browser with real-time communication through WebSocket. When the virtual scene changes, the system moves the robots to dynamically reconfigure the physical scene. First, the system computes the types of props and each target position based on the relative position from the user.\n\n\n# Interactions and Applications\n\nIn this paper, we specifically focus on architectural application scenarios, such as rendering physical room interiors for virtual real estate tours and collaborative architectural design, two increasingly common application areas for VR. Virtual real estate tours reduce the time and cost compared to on-site viewings, but currently lack the bodily experience of being able to touch surfaces and sit down. In architectural design, VR aids the communication between architects and clients, where proposed designs can be experienced, discussed and modified before building them. We are motivated by how RoomShift can enable people with various physical abilities to experience, test and co-design these environments with their bodies. Most of the elements in these applications can be covered with a finite set of furniture and props (e.g., chairs, desks, and walls). We discuss some of the basic interactions to support these applications.\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-8.png" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-8.png" /></a>\n  </div>\n</div>\n\nTo support these scenarios, we propose four types of basic interactions RoomShift can support, with the spectrum between embodied interactions and controller-based interactions, as illustrated in the Figure.\n\n1. Experiencing Architectural Spaces: Walking and Touching\n\n2. Architectural Co-Design: Physically Moving Furniture\n\n3. Navigating Large Spaces: Teleporting in VR\n\n4. Virtual Scene Editing: Virtually Moving Furniture\n\nEmbodied interactions refer to interaction with virtual scenes through physical movements and manipulation. The user can implicitly interact with the system by walking around or explicitly interact with the virtual scene by physically moving furniture. On the other hand, the user can also interact with the virtual scene with controller-based gestural interactions. An example is when the user relocates a distant piece of furniture or remove the wall in the room. The user can also virtually teleport their location to navigate through space.\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-10-1.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-10-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-10-2.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-10-2.jpg" /></a>\n  </div>\n</div>\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-11-1.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-11-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-11-2.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-11-2.jpg" /></a>\n  </div>\n</div>\n\nFor example, the most basic interaction is to render an architectural space that the user can walk around in and touch.  As the user walks around the space, the robots move the props to maintain the illusion of a larger number of objects.\nIn addition, the system can mimic larger objects with a single moving robot. For example, when the user is interacting with a large table, either new physical table segments can be added or a single robot can continually move the current table according to the user’s position to simulate touching a larger one.\n\nAlso, RoomShift supports teleportation by reconfiguring the room layout to match the new view location. When the user teleports to a new location in the VR scene, the system calculates the positions of the virtual objects relative to the new location and moves the furniture and robots in and out of the play area to enable a fast scene reconfiguration and to avoid collisions with the user and each other.',bodyHtml:'<p>video coming soon.</p>\n<h1>Abstract</h1>\n<p>This paper presents RoomShift, a room-scale dynamic haptic environment for virtual reality, using a small swarm of robots that can move furniture. RoomShift consists of nine shape-changing robots: Roombas with mechanical scissor lifts. These robots drive beneath a piece of furniture to lift, move and place it. By augmenting virtual scenes with physical objects, users can sit on, lean against, place and otherwise interact with furniture with their whole body; just as in the real world. When the virtual scene changes or users navigate within it, the swarm of robots dynamically reconfigures the physical environment to match the virtual content. We describe the hardware and software implementation, applications in virtual tours and architectural design and interaction techniques.</p>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/roomshift/video/top.mp4" type="video/mp4"></source>\n</video>\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-1-1.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-1-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-1-2.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-1-2.jpg" /></a>\n  </div>\n</div>\n<h1>Introduction</h1>\n<p>There is a clear need to provide haptic sensations in virtual environments. Recent advances in display and tracking technologies promise immersive experience in virtual reality, but objects seen in VR such as walls and furniture are only visual: the user cannot touch, feel, sit on, or place objects on them. This limits the sense of full immersion in the virtual world. To overcome these limitations, various haptic interfaces have been explored. In the previous work, most haptic interfaces focus on finger-tip haptic feedback with actuated controllers or on-body haptic sensations with wearable devices. In contrast, encountered-type haptic feedback with a dynamic environment promises to increase the immersion of virtual experiences, which are difficult to achieve using an only handheld or wearable haptic devices. Through a dynamic haptic environment, users can touch and interact with the whole virtual scene with their bodies --- they can walk, sit on, and lean against objects in the VR environment. Existing approaches for actuated environments, however, are often limited in speed of transformation (e.g., slow transformation with inflatables) and the range of supported interactions (e.g., only walking).</p>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/roomshift/video/wall.mp4" type="video/mp4"></source>\n</video>\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-9-1.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-9-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-9-2.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-9-2.jpg" /></a>\n  </div>\n</div>\n<p>This paper introduces RoomShift, a room-scale dynamic haptic environment for virtual reality. RoomShift provides haptic sensations by reconfiguring physical environments using a small swarm of robot assistants. Inspired by shelf-moving robots that are used in robotic warehouses, we developed a swarm of shape-changing robots that can move a range of existing furniture. Each robot has a mechanical lift that extends from 30 cm to 100 cm to pick up, carry, and place objects such as chairs, tables, and walls. This way, users can touch, sit, place, and lean against objects in the virtual environment.</p>\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-2-1.png" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-2-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-2-2.png" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-2-2.png" /></a>\n  </div>\n</div>\n<h1>RoomShift: Furniture-moving Robots</h1>\n<p>RoomShift consists of a small swarm of shape-changing robots; each robot uses a Roomba as a mobile base. On this base is mounted a custom mechanical scissor lift made of two linear actuators and a metal drying rack. As the mechanical lift is compact in its closed state, the robot can move under a table or chair with 30 cm clearance, and extend the scissor lift to pick it up.</p>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/roomshift/video/carry.mp4" type="video/mp4"></source>\n</video>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-3-1.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-3-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-3-2.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-3-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-3-3.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-3-3.jpg" /></a>\n  </div>\n</div>\n<p>RoomShift comprises nine shape-changing swarm robots based on the Roomba Create 2. For the mechanical lift structure, we repurposed an off-the-shelf expandable laundry rack (Room Essentials Compact Drying Rack) and attached two linear actuators (Homend DC12V 8 inch Stroke Linear Actuator, which extends from 32 cm to 52 cm) at the base of the rack. The linear actuators are fixed to the endpoints of the scissor structure with 8 mm steel rods, so that when the actuator contracts, the mounted scissor structure extends vertically (from 30 cm to 100 cm). The scissor structure moves at a speed of 1.3 cm / sec. To mount the scissor structure, we fixed a 6mm acrylic bottom plate (35 cm x 35 cm) and four omni-directional casters (Dorhea Ball Transfer Bearing Unit) to relieve the Roomba of most of the weight that the robot carries. Each robot moves at 20 cm / sec. Figure 3 illustrates the mechanical design of each RoomShift robot.</p>\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-4-1.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-4-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-4-2.png" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-4-2.png" /></a>\n  </div>\n</div>\n<p>One advantage of our approach is that the robot need not support the weight of the user. Once the robot places the furniture, it serves as a static object. Thus, when a user sits on or puts weight on it, all of the weight goes to the furniture, instead of the robot, which significantly reduces the possibility of a mechanical breakdown.\nAlthough the maximum load for the Roomba is 9 kg, the corner-mounted casters distribute and carry heavier loads. Thus, our robots can lift and carry heavier objects than an unmodified Roomba. The maximum weight the robot can lift and carry is 22 kg. When we put a heavier object than 23 kg, we observed the scissor structure started to break. The strength of the scissor structure suffices to lift lightweight chairs and tables, such as the IKEA honeycomb furniture used in our prototypes. The weight of the furniture we have tested (depicted in Figure) ranges from 3.5 to 11.2 kg. For heavier objects, multiple robots can also coordinate to lift a piece together if there is sufficient space under the furniture. Also, with a more robust scissor structure, we can carry heavier objects, as we observed the Roomba base itself (with the corner-mounted casters) can carry up to 30 kg load.</p>\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-5-1.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-5-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-5-2.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-5-2.jpg" /></a>\n  </div>\n</div>\n<p>This approach also increases flexibility because different types of furniture can be actuated with the height-adjustable scissor lift. For example, Figure illustrates various static props that the RoomShift robot can actuate. These objects include furniture such as a desk, a long table, different chairs, and a side table. Note that due to the robot’s minimum collapsed size, objects must have at least 30 cm clearance below them, and enough horizontal space to fit the robot. A designer can also create custom props for specific applications, for instance, the styrofoam wall mounted to a side table seen in the Figure.</p>\n<h1>Tracking and Control</h1>\n<p>To accurately control the RoomShift robots, we require precise motion tracking that can cover the play area in which a user walks. We use an optical tracking system with 20 IR cameras (Qualisys Miqus 5) that can track objects in a 10 m × 10 m space. The system tracks six degrees of freedom (DOF) position of the objects\nwith retro-reflective spherical markers at 60 FPS frame rate.\nTo track the robots as well as physical props, we attached five 30 mm spherical retroreflective markers. For the robot, we attached markers to a pair of parallel bars, so that the markers’ relative positions remain constant regardless of the height of the scissor lift. We can also estimate the height of the scissor structure by measuring the orientation of the marker pattern (the pink plane surface depicted in the Figure).</p>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/roomshift/video/tracking.mp4" type="video/mp4"></source>\n</video>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-7-3.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-7-3.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-7-1.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-7-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-7-2.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-7-2.jpg" /></a>\n  </div>\n</div>\n<p>To control the robots’ movements, we use a simple path planning algorithm. The input is 1) the current positions of the robots, 2) the positions of obstacles (e.g., furniture, other robots, and users), and 3) the target locations. The algorithm outputs the goal of each robot at the next time step. The system continuously updates the path and drives them to their target locations. The main server continuously tracks the robot positions, calculates their wheel speeds, and sends commands at 30 Hz over WiFi.</p>\n<p>To pick up and place these, the robot follows a predefined sequence, approaching the object from an angle where it will not collide with the object’s legs. To avoid the collision with the legs of furniture, each object has a user-defined entry and exit point (Figure 8). We also register the height of target furniture before the system starts (e.g., 70 cm for Table_A, 40 cm for Chair_B), so that it can extend the scissor lift to certain target height. We could also put a simple sensor on top of the scissor structure to make it a closed-loop system.</p>\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-6-1.png" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-6-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-6-2.png" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-6-2.png" /></a>\n  </div>\n</div>\n<p>The main computer runs a Node.js server and the Qualisys tracking software. The 6DOF tracking data that the Qualisys tracking system captures is streamed to the Node.js server through the WebSocket protocol. Based on the tracking data, a web browser client renders the VR scene with A-Frame. The user experiences the VR scene using an Oculus Go head mounted display and its built-in VR browser. We synchronize the desktop computer and the Oculus Go browser with real-time communication through WebSocket. When the virtual scene changes, the system moves the robots to dynamically reconfigure the physical scene. First, the system computes the types of props and each target position based on the relative position from the user.</p>\n<h1>Interactions and Applications</h1>\n<p>In this paper, we specifically focus on architectural application scenarios, such as rendering physical room interiors for virtual real estate tours and collaborative architectural design, two increasingly common application areas for VR. Virtual real estate tours reduce the time and cost compared to on-site viewings, but currently lack the bodily experience of being able to touch surfaces and sit down. In architectural design, VR aids the communication between architects and clients, where proposed designs can be experienced, discussed and modified before building them. We are motivated by how RoomShift can enable people with various physical abilities to experience, test and co-design these environments with their bodies. Most of the elements in these applications can be covered with a finite set of furniture and props (e.g., chairs, desks, and walls). We discuss some of the basic interactions to support these applications.</p>\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-8.png" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-8.png" /></a>\n  </div>\n</div>\n<p>To support these scenarios, we propose four types of basic interactions RoomShift can support, with the spectrum between embodied interactions and controller-based interactions, as illustrated in the Figure.</p>\n<ol>\n<li>\n<p>Experiencing Architectural Spaces: Walking and Touching</p>\n</li>\n<li>\n<p>Architectural Co-Design: Physically Moving Furniture</p>\n</li>\n<li>\n<p>Navigating Large Spaces: Teleporting in VR</p>\n</li>\n<li>\n<p>Virtual Scene Editing: Virtually Moving Furniture</p>\n</li>\n</ol>\n<p>Embodied interactions refer to interaction with virtual scenes through physical movements and manipulation. The user can implicitly interact with the system by walking around or explicitly interact with the virtual scene by physically moving furniture. On the other hand, the user can also interact with the virtual scene with controller-based gestural interactions. An example is when the user relocates a distant piece of furniture or remove the wall in the room. The user can also virtually teleport their location to navigate through space.</p>\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-10-1.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-10-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-10-2.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-10-2.jpg" /></a>\n  </div>\n</div>\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-11-1.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-11-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/roomshift/figure-11-2.jpg" data-lightbox="lightbox">\n      <img src="/static/projects/roomshift/figure-11-2.jpg" /></a>\n  </div>\n</div>\n<p>For example, the most basic interaction is to render an architectural space that the user can walk around in and touch.  As the user walks around the space, the robots move the props to maintain the illusion of a larger number of objects.\nIn addition, the system can mimic larger objects with a single moving robot. For example, when the user is interacting with a large table, either new physical table segments can be added or a single robot can continually move the current table according to the user’s position to simulate touching a larger one.</p>\n<p>Also, RoomShift supports teleportation by reconfiguring the room layout to match the new view location. When the user teleports to a new location in the VR scene, the system calculates the positions of the virtual objects relative to the new location and moves the furniture and robots in and out of the play area to enable a fast scene reconfiguration and to avoid collisions with the user and each other.</p>\n',dir:"content/output/projects",base:"roomshift.json",ext:".json",sourceBase:"roomshift.md",sourceExt:".md"}},CTYI:function(e){e.exports={id:"flux-marker",name:"FluxMarker",description:"Enhancing Tactile Graphics with Dynamic Tactile Markers for Blind People",title:"FluxMarker: Enhancing Tactile Graphics with Dynamic Tactile Markers",authors:["Ryo Suzuki","Abigale Stangl","Mark D. Gross","Tom Yeh"],year:2017,booktitle:"In Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS '17)",publisher:"ACM, New York, NY, USA",pages:"190-199",doi:"https://doi.org/10.1145/3132525.3132548",conference:{name:"ASSETS 2017",fullname:"The International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS 2017)",url:"https://assets17.sigaccess.org/"},pdf:"assets-2017-fluxmarker.pdf",video:"https://www.youtube.com/watch?v=VbwIZ9V6i_g",embed:"https://www.youtube.com/embed/VbwIZ9V6i_g",slide:"assets-2017-fluxmarker-slide.pdf","acm-dl":"https://dl.acm.org/citation.cfm?id=3132548",arxiv:"https://arxiv.org/abs/1708.03783",pageCount:10,slideCount:53,bodyContent:"# Abstract\n\nFor people with visual impairments, tactile graphics are an impor- tant means to learn and explore information. However, raised line tactile graphics created with traditional materials such as emboss- ing are static. While available refreshable displays can dynamically change the content, they are still too expensive for many users, and are limited in size. These factors limit wide-spread adoption and the representation of large graphics or data sets. In this paper, we present FluxMaker, an inexpensive scalable system that renders dy- namic information on top of static tactile graphics with movable tactile markers. These dynamic tactile markers can be easily re- configured and used to annotate static raised line tactile graphics, including maps, graphs, and diagrams. We developed a hardware prototype that actuates magnetic tactile markers driven by low-cost and scalable electromagnetic coil arrays, which can be fabricated with standard printed circuit board manufacturing. We evaluate our prototype with six participants with visual impairments and found positive results across four application areas: location finding or navigating on tactile maps, data analysis, and physicalization, fea- ture identification for tactile graphics, and drawing support. The user study confirms advantages in application domains such as ed- ucation and data exploration.",bodyHtml:"<h1>Abstract</h1>\n<p>For people with visual impairments, tactile graphics are an impor- tant means to learn and explore information. However, raised line tactile graphics created with traditional materials such as emboss- ing are static. While available refreshable displays can dynamically change the content, they are still too expensive for many users, and are limited in size. These factors limit wide-spread adoption and the representation of large graphics or data sets. In this paper, we present FluxMaker, an inexpensive scalable system that renders dy- namic information on top of static tactile graphics with movable tactile markers. These dynamic tactile markers can be easily re- configured and used to annotate static raised line tactile graphics, including maps, graphs, and diagrams. We developed a hardware prototype that actuates magnetic tactile markers driven by low-cost and scalable electromagnetic coil arrays, which can be fabricated with standard printed circuit board manufacturing. We evaluate our prototype with six participants with visual impairments and found positive results across four application areas: location finding or navigating on tactile maps, data analysis, and physicalization, fea- ture identification for tactile graphics, and drawing support. The user study confirms advantages in application domains such as ed- ucation and data exploration.</p>\n",dir:"content/output/projects",base:"flux-marker.json",ext:".json",sourceBase:"flux-marker.md",sourceExt:".md"}},"Dx8+":function(e,t,i){"use strict";i.r(t);var a=i("0iUn"),s=i("sLSF"),o=i("MI3g"),n=i("a7VT"),r=i("Tit0"),c=i("q1tI"),l=i.n(c),p=i("IujW"),h=i.n(p),d=i("9Gwv"),g=function(e){function t(){return Object(a.default)(this,t),Object(o.default)(this,Object(n.default)(t).apply(this,arguments))}return Object(r.default)(t,e),Object(s.default)(t,[{key:"render",value:function(){return l.a.createElement("div",{id:"updates",className:"ui relaxed divided list"},l.a.createElement("h3",null,"Recent Updates"),d.map(function(e){return l.a.createElement("div",{className:"item"},l.a.createElement("div",{className:"header"},e.date),l.a.createElement("div",{className:"content"},l.a.createElement(h.a,{source:e.text})))}))}}]),t}(l.a.Component);t.default=g},GbvX:function(e){e.exports={id:"dynablock",name:"Dynablock",description:"Dynamic 3D Printing for Instant and Reconstructable Shape Formation",title:"Dynablock: Dynamic 3D Printing for Instant and Reconstructable Shape Formation",authors:["Ryo Suzuki","Junichi Yamaoka","Daniel Leithinger","Tom Yeh","Mark D. Gross","Yoshihiro Kawahara","Yasuaki Kakehi"],year:2018,booktitle:"In Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology (UIST '18)",publisher:"ACM, New York, NY, USA",pages:"99-111",doi:"https://doi.org/10.1145/3242587.3242659",conference:{name:"UIST 2018",fullname:"The ACM Symposium on User Interface Software and Technology (UIST 2018)",url:"http://uist.acm.org/uist2018"},pdf:"uist-2018-dynablock.pdf",video:"https://www.youtube.com/watch?v=7nPlr3O9xu8",embed:"https://www.youtube.com/embed/7nPlr3O9xu8","short-video":"https://www.youtube.com/watch?v=92eGI-gYYc4",slide:"uist-2018-dynablock-slide.pdf","acm-dl":"https://dl.acm.org/citation.cfm?id=3242659",talk:"https://www.youtube.com/watch?v=R3FRUtOIiCQ",poster:"uist-2018-dynablock-poster.pdf",pageCount:12,slideCount:53,bodyContent:'\x3c!--\nLinks:\n[**[PDF](http://ryosuzuki.org/publications/uist-2018-dynablock.pdf)**]\n[**[ACM DL](https://dl.acm.org/citation.cfm?id=3242659)**]\n[**[Video](https://www.youtube.com/watch?v=7nPlr3O9xu8)**]\n[**[Slide](http://ryosuzuki.org/publications/uist-2018-dynablock-slide.pdf)**]\n[**[Talk](https://www.youtube.com/watch?v=R3FRUtOIiCQ)**]\n --\x3e\n\n# Abstract\n\nThis paper introduces Dynamic 3D Printing, a fast and re- constructable shape formation system. Dynamic 3D Printing assembles an arbitrary three-dimensional shape from a large number of small physical elements. It can also disassemble the shape back to elements and reconstruct a new shape. Dynamic 3D Printing combines the capabilities of 3D printers and shape displays: Like conventional 3D printing, it can generate arbi- trary and graspable three-dimensional shapes, while allowing shapes to be rapidly formed and reformed as in a shape display. To demonstrate the idea, we describe the design and imple- mentation of Dynablock, a working prototype of a dynamic 3D printer. Dynablock can form a three-dimensional shape in seconds by assembling 3,000 9 mm blocks, leveraging a 24 x 16 pin-based shape display as a parallel assembler. Dynamic 3D printing is a step toward achieving our long term vision in which 3D printing becomes an interactive medium, rather than the means for fabrication that it is today. In this paper we explore possibilities for this vision by illustrating application scenarios that are difficult to achieve with conventional 3D printing or shape display systems.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/dynablock/webm/top.webm" type="video/webm"></source>\n  <source src="/static/projects/dynablock/video/top.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-1-1.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-1-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-1-2.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-1-2.jpg" /></a>\n  </div>\n</div>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-2-1.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-2-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-2-2.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-2-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-2-3.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-2-3.jpg" /></a>\n  </div>\n</div>\n\n# Dynamic 3D Printing\n\nWhat if 3D printers could form a physical object in seconds? What if the object, once it is no longer needed, could quickly and easily be disassembled and reconstructed as a new object? Today’s 3D printers take hours to print objects, and output a single static object. However, we envision a future in which 3D printing could instantly create objects from reusable and reconstructable materials.\n\nThis paper develops this vision by proposing Dynamic 3D Printing, a class of systems for rapid and reconstructable shape formation. Dynamic 3D Printing assembles digital material elements to form recon- structable physical objects. Each element can be connected with and disconnected from neighboring elements, and ele- ments can be formed into an arbitrary three-dimensional phys- ical object. Dynamic 3D printing differs from existing 3D printing in speed and reconstructability: Dynamic 3D printing forms shapes in seconds, rather than minutes. In addition, because individual elements can be disconnected, the shape can be easily disassembled into its basic building blocks once the object is no longer needed.\n\nWe define Dynamic 3D Printing as a class of systems that have the following properties:\n\n- Immediate: The system can form a physical shape in sec- onds.\n\n- Reconstructable: Rendered shapes can be disassembled and reconstructed by hand or with the system, and the blocks are reusable.\n\n- Arbitrary Shapes: It can create arbitrary three dimensional shapes.\n\n- Graspable: The output shapes and structure are graspable and solid.\n\n# Parallel Assembler\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-3-1.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-3-1.png" /></a>\n  </div>\n</div>\n\nDynamic 3D printing deploys a large number of small dis- crete material elements, which are assembled to form arbitrary shaped macro-scale objects. Individual elements are passive, which requires an external actuator to perform the assembly. As illustrated in the above Figure, the assembler consists of an N x N grid of motorized pins and linear actuators. The elements, which are the same size as the pins, are stacked on top of the pins (Figure 3 A). When stacked, the elements are connected in vertical direction, while discon- nected with nearby elements in horizontal direction. Similar to existing pin-based shape displays, the assembler can incrementally generate 2.5D shapes by individually moving pins to push elements to the surface.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/dynablock/webm/mechanism.webm" type="video/webm"></source>\n  <source src="/static/projects/dynablock/video/mechanism.mp4" type="video/mp4"></source>\n</video>\n\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-4-1.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-4-1.png" /></a>\n  </div>\n  <p class="column">\n    This paper develops this vision by proposing Dynamic 3D Printing, a class of systems for rapid and reconstructable shape formation. Dynamic 3D Printing assembles digital material elements to form recon- structable physical objects. Each element can be connected with and disconnected from neighboring elements, and ele- ments can be formed into an arbitrary three-dimensional phys- ical object. Dynamic 3D printing differs from existing 3D printing in speed and reconstructability: Dynamic 3D printing forms shapes in seconds, rather than minutes. In addition, because individual elements can be disconnected, the shape can be easily disassembled into its basic building blocks once the object is no longer needed.\n  </p>\n</div>\n\n\n# Implementation\n\nThe assembler consists of a 24 x 16 array of motor-driven pins. Each pin moves up and down, driven by a small DC motor (TTMotors TGPP06-D700) and a 3D printed lead screw (2 mm pitch, 4 starts, 120 mm in length). TGPP06-D700 is 6 mm in diameter and 29 mm in length and can rotate 47 rpm with 1:700 gear ratio. The 2 mm 4 starts lead screw can travel 12 mm per second without load, and each motor consumes approximately 60 mA. The pins are 3D printed with a nut at the bottom to travel along the lead screw. Each pin is 120 mm long and has a 7mm square cross section with a 5 mm diameter hole from top to bottom, and an N45 disk magnet (φ 3mm x 2.4 mm thickness) is attached at the top. Guide grids at the top prevent pins from rotating and ensure that pins travel vertically. The 24 x 16 guide grids have 7.5 mm square holes with 10.16 mm pitch and are cut from a 5 mm acrylic plate. We fabricated the pins, the lead screws, and blocks with an inkjet 3D printer (Keyence Agilista 3200) with water soluble support material. In total, we fabricated 384 (= 24 x 16) pins and lead screws, and 3,072 (= 24 x 16 x 8 layers) blocks. To create the magnetic blocks, we embedded spherical magnets in each block by hand and inserted disk magnets using a bench vice.\n\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-5-1.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-5-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-5-2.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-5-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-5-3.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-5-3.png" /></a>\n  </div>\n</div>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-7-1.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-7-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-7-2.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-7-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-7-3.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-7-3.jpg" /></a>\n  </div>\n</div>\n\n\n# Future Vision\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/dynablock/webm/claytronics.webm" type="video/webm"></source>\n  <source src="/static/projects/dynablock/video/claytronics.mp4" type="video/mp4"></source>\n</video>\n[Video Credit: Carnegie Mellon University, Claytronics Vision]\n\n<br/>\n\nWith these capabilities, a 3D printer would become an inter- active medium, rather than merely a fabrication device. For example, such a 3D printer could be used in a Virtual Real- ity or Augmented Reality application to dynamically form a tangible object or controller to provide haptic feedback and engage users physically. For children, it could dynamically form a physical educational manipulative, such as a molec- ular or architectural model, to learn and explore topics, for example in a science museum. Designers could use it to ren- der a physical product to present to clients and interactively change the product’s design through direct manipulation. In this vision, Dynamic 3D printing is an environment in which the user thinks, designs, explores, and communicates through dynamic and interactive physical representation.\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-8-1.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-8-1.png" /></a>\n  </div>\n</div>\n\nDynamic 3D printing would enable a new design workflow for digital fabrication. One notable advantage of dynamic 3D printing is the capability of connecting and disconnecting building blocks through direct manipulation. The user can also define variables or abstract attributes for parametric design through direct and gestural interaction. By leveraging this capability, the user could interactively design and fabri- cate in a physical space, similar to the man-machine dialogue proposed by Frazer et al. and later tangible CAD interfaces.',bodyHtml:'\x3c!--\nLinks:\n[**[PDF](http://ryosuzuki.org/publications/uist-2018-dynablock.pdf)**]\n[**[ACM DL](https://dl.acm.org/citation.cfm?id=3242659)**]\n[**[Video](https://www.youtube.com/watch?v=7nPlr3O9xu8)**]\n[**[Slide](http://ryosuzuki.org/publications/uist-2018-dynablock-slide.pdf)**]\n[**[Talk](https://www.youtube.com/watch?v=R3FRUtOIiCQ)**]\n --\x3e\n<h1>Abstract</h1>\n<p>This paper introduces Dynamic 3D Printing, a fast and re- constructable shape formation system. Dynamic 3D Printing assembles an arbitrary three-dimensional shape from a large number of small physical elements. It can also disassemble the shape back to elements and reconstruct a new shape. Dynamic 3D Printing combines the capabilities of 3D printers and shape displays: Like conventional 3D printing, it can generate arbi- trary and graspable three-dimensional shapes, while allowing shapes to be rapidly formed and reformed as in a shape display. To demonstrate the idea, we describe the design and imple- mentation of Dynablock, a working prototype of a dynamic 3D printer. Dynablock can form a three-dimensional shape in seconds by assembling 3,000 9 mm blocks, leveraging a 24 x 16 pin-based shape display as a parallel assembler. Dynamic 3D printing is a step toward achieving our long term vision in which 3D printing becomes an interactive medium, rather than the means for fabrication that it is today. In this paper we explore possibilities for this vision by illustrating application scenarios that are difficult to achieve with conventional 3D printing or shape display systems.</p>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/dynablock/webm/top.webm" type="video/webm"></source>\n  <source src="/static/projects/dynablock/video/top.mp4" type="video/mp4"></source>\n</video>\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-1-1.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-1-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-1-2.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-1-2.jpg" /></a>\n  </div>\n</div>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-2-1.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-2-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-2-2.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-2-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-2-3.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-2-3.jpg" /></a>\n  </div>\n</div>\n<h1>Dynamic 3D Printing</h1>\n<p>What if 3D printers could form a physical object in seconds? What if the object, once it is no longer needed, could quickly and easily be disassembled and reconstructed as a new object? Today’s 3D printers take hours to print objects, and output a single static object. However, we envision a future in which 3D printing could instantly create objects from reusable and reconstructable materials.</p>\n<p>This paper develops this vision by proposing Dynamic 3D Printing, a class of systems for rapid and reconstructable shape formation. Dynamic 3D Printing assembles digital material elements to form recon- structable physical objects. Each element can be connected with and disconnected from neighboring elements, and ele- ments can be formed into an arbitrary three-dimensional phys- ical object. Dynamic 3D printing differs from existing 3D printing in speed and reconstructability: Dynamic 3D printing forms shapes in seconds, rather than minutes. In addition, because individual elements can be disconnected, the shape can be easily disassembled into its basic building blocks once the object is no longer needed.</p>\n<p>We define Dynamic 3D Printing as a class of systems that have the following properties:</p>\n<ul>\n<li>\n<p>Immediate: The system can form a physical shape in sec- onds.</p>\n</li>\n<li>\n<p>Reconstructable: Rendered shapes can be disassembled and reconstructed by hand or with the system, and the blocks are reusable.</p>\n</li>\n<li>\n<p>Arbitrary Shapes: It can create arbitrary three dimensional shapes.</p>\n</li>\n<li>\n<p>Graspable: The output shapes and structure are graspable and solid.</p>\n</li>\n</ul>\n<h1>Parallel Assembler</h1>\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-3-1.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-3-1.png" /></a>\n  </div>\n</div>\n<p>Dynamic 3D printing deploys a large number of small dis- crete material elements, which are assembled to form arbitrary shaped macro-scale objects. Individual elements are passive, which requires an external actuator to perform the assembly. As illustrated in the above Figure, the assembler consists of an N x N grid of motorized pins and linear actuators. The elements, which are the same size as the pins, are stacked on top of the pins (Figure 3 A). When stacked, the elements are connected in vertical direction, while discon- nected with nearby elements in horizontal direction. Similar to existing pin-based shape displays, the assembler can incrementally generate 2.5D shapes by individually moving pins to push elements to the surface.</p>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/dynablock/webm/mechanism.webm" type="video/webm"></source>\n  <source src="/static/projects/dynablock/video/mechanism.mp4" type="video/mp4"></source>\n</video>\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-4-1.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-4-1.png" /></a>\n  </div>\n  <p class="column">\n    This paper develops this vision by proposing Dynamic 3D Printing, a class of systems for rapid and reconstructable shape formation. Dynamic 3D Printing assembles digital material elements to form recon- structable physical objects. Each element can be connected with and disconnected from neighboring elements, and ele- ments can be formed into an arbitrary three-dimensional phys- ical object. Dynamic 3D printing differs from existing 3D printing in speed and reconstructability: Dynamic 3D printing forms shapes in seconds, rather than minutes. In addition, because individual elements can be disconnected, the shape can be easily disassembled into its basic building blocks once the object is no longer needed.\n  </p>\n</div>\n<h1>Implementation</h1>\n<p>The assembler consists of a 24 x 16 array of motor-driven pins. Each pin moves up and down, driven by a small DC motor (TTMotors TGPP06-D700) and a 3D printed lead screw (2 mm pitch, 4 starts, 120 mm in length). TGPP06-D700 is 6 mm in diameter and 29 mm in length and can rotate 47 rpm with 1:700 gear ratio. The 2 mm 4 starts lead screw can travel 12 mm per second without load, and each motor consumes approximately 60 mA. The pins are 3D printed with a nut at the bottom to travel along the lead screw. Each pin is 120 mm long and has a 7mm square cross section with a 5 mm diameter hole from top to bottom, and an N45 disk magnet (φ 3mm x 2.4 mm thickness) is attached at the top. Guide grids at the top prevent pins from rotating and ensure that pins travel vertically. The 24 x 16 guide grids have 7.5 mm square holes with 10.16 mm pitch and are cut from a 5 mm acrylic plate. We fabricated the pins, the lead screws, and blocks with an inkjet 3D printer (Keyence Agilista 3200) with water soluble support material. In total, we fabricated 384 (= 24 x 16) pins and lead screws, and 3,072 (= 24 x 16 x 8 layers) blocks. To create the magnetic blocks, we embedded spherical magnets in each block by hand and inserted disk magnets using a bench vice.</p>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-5-1.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-5-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-5-2.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-5-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-5-3.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-5-3.png" /></a>\n  </div>\n</div>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-7-1.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-7-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-7-2.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-7-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-7-3.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-7-3.jpg" /></a>\n  </div>\n</div>\n<h1>Future Vision</h1>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/dynablock/webm/claytronics.webm" type="video/webm"></source>\n  <source src="/static/projects/dynablock/video/claytronics.mp4" type="video/mp4"></source>\n</video>\n[Video Credit: Carnegie Mellon University, Claytronics Vision]\n<br/>\n<p>With these capabilities, a 3D printer would become an inter- active medium, rather than merely a fabrication device. For example, such a 3D printer could be used in a Virtual Real- ity or Augmented Reality application to dynamically form a tangible object or controller to provide haptic feedback and engage users physically. For children, it could dynamically form a physical educational manipulative, such as a molec- ular or architectural model, to learn and explore topics, for example in a science museum. Designers could use it to ren- der a physical product to present to clients and interactively change the product’s design through direct manipulation. In this vision, Dynamic 3D printing is an environment in which the user thinks, designs, explores, and communicates through dynamic and interactive physical representation.</p>\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-8-1.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-8-1.png" /></a>\n  </div>\n</div>\n<p>Dynamic 3D printing would enable a new design workflow for digital fabrication. One notable advantage of dynamic 3D printing is the capability of connecting and disconnecting building blocks through direct manipulation. The user can also define variables or abstract attributes for parametric design through direct and gestural interaction. By leveraging this capability, the user could interactively design and fabri- cate in a physical space, similar to the man-machine dialogue proposed by Frazer et al. and later tangible CAD interfaces.</p>\n',dir:"content/output/projects",base:"dynablock.json",ext:".json",sourceBase:"dynablock.md",sourceExt:".md"}},IMSK:function(e){e.exports={id:"phd-thesis",name:"Collective Shape-changing Interfaces",description:"Dynamic Shape Construction and Transformation with Collective Elements",title:"Dynamic Shape Construction and Transformation with Collective Elements",authors:["Ryo Suzuki"],year:2020,booktitle:"PhD Dissertation",publisher:"University of Colorado Boulder",pages:"1-287",conference:{name:"PhD Dissertation",fullname:"PhD Dissertation"},pdf:"phd-dissertation.pdf",slide:"phd-defense-slide.pdf",talk:"https://www.youtube.com/watch?v=FHmp7BIhXJI",pageCount:252,slideCount:200,related:{title:"Collective Shape-changing Interfaces",authors:["Ryo Suzuki"],year:2019,booktitle:"In Adjunct Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology (UIST '19 Doctoral Consortium)",publisher:"ACM, New York, NY, USA",pages:"154–157",doi:"https://doi.org/10.1145/3332167.3356877",pdf:"uist-2019-collective.pdf",suffix:"dc",pageCount:4},bodyContent:'# Abstract\nThis thesis explores dynamic and collective shape construction as a new way to **physicalize** digital information for interactive physical displays --- i.e., **shape-changing displays** enabled by a swarm of collective elements. Through physical form of digital objects, the user can directly touch, grasp, and manipulate digital information through rich tangible and embodied interactions, but at the same time, such physical objects can dynamically change their shape for an interactive computer display and interface through collective shape construction and transformation with a swarm of elements. The goal of this thesis is to envision and illustrate how such an interface might support human activities by transforming physical forms at various sizes, from millimeter to meter scale.\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-1.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-1.jpg" /></a>\n  </div>\n</div>\n\nTo achieve this goal, this thesis introduces **collective shape-changing interface**, a new type of shape-changing interfaces constructed by discrete, collective, physical elements. This proposed approach promises to address the current limitations of shape-changing interfaces --- wherein a swarm of modular elements enables us to decompose the large, monolithic shape-changing objects into a set of simple, distributed elements. At the same time, their swarm behaviors enable us to make an unbounded shape transformation for expressive representation. This thesis contributes to the first exploration of this new class of shape-changing interfaces and proposes two approaches: active and passive shape construction. In active shape construction, collective elements can dynamically move and reconfigure themselves to construct a three-dimensional shape. Passive shape construction instead leverages external actuation to assemble and transform collective passive objects for dynamic shape creation. I explore and demonstrate how active and passive collective shape construction can be used as a future of computer interfaces, by developing various prototypes built on top of novel hardware and software platforms. Given these investigations, I discuss the design implications and possible research directions towards the future of collective shape-changing user interfaces.\n\n\n# Introduction\n\nWhat if computer displays can represent information not only {\\it graphically} but also *physically*? What if such physical forms of information could be as malleable and programmable as the pixels on a computer screen? If so, it could be used as a dynamic physical medium to interact with the digital world.\n\nIvan Sutherland, a founder of virtual and augmented reality, once envisioned that the future of computer displays would be *"a room within which the computer can control the existence of matter"*\nThis radical vision has inspired many researchers over the decades, as such technologies could open up a new\nparadigm of human-machine interfaces.\n\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-2.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-2.jpg" /></a>\n  </div>\n</div>\n\nHowever, we are still far from this exciting future. Today\'s computer interfaces mostly focus on *screen-based* interaction, where the screens serve as a ``looking window\'\' of the digital world --- the user can see digital information through the glass, but a barrier between what is inside (digital world) and what is outside (physical world) confines how we interact with the digital world. Current technologies do not allow us to directly touch, feel, grasp, and manipulate digital objects, in the same way that humans have done with physical objects for hundreds of thousands of years.\n\n\n# Thesis Statement\n\nThe goal of this thesis is to bring Sutherland\'s vision closer to reality by developing a new form of interactive and dynamic physical displays, and to illustrate how such an interface might support human activities by transforming physical forms and environments at various scales.\n\nAs a step toward this vision, this thesis explores dynamic and collective shape construction as a new way to **physicalize** digital information for interactive physical displays --- i.e., **shape-changing displays** enabled by a swarm of collective elements.\nCollective elements refer to discrete physical objects that can construct a physical, three-dimensional shape.\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-3.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-3.jpg" /></a>\n  </div>\n</div>\n\nEach individual element can dynamically change its shape, position, and other physical properties through internal or external actuation, as to collectively construct and transform the overall physical shape.\nThis enables a new way of representing digital information.\nSuch physical shapes allow the user not only to see information, but to touch, feel, grasp, construct, and manipulate it, in the same way that interact with physical objects.\nAt the same time, these physical objects must also embody dynamic computation. Collective elements can dynamically and programmatically reconfigure themselves, as if they are rendered in an interactive computer display and interface.\n\nIn contrast to shape changes made of monolithic materials, constructing shapes out of discrete elements enables rich expressiveness in representing information.\nLike pixels on a screen, they make shapes by collectively transforming the overall structure.\nAdditionally, their components can be simple and interchangeable, thus allowing for scale.\nThese elements can also interact with existing environments, and they make everyday objects and environments more dynamic, adaptive, and interactive by collectively actuating and reconfiguring them in a programmable fashion.\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-4.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-4.jpg" /></a>\n  </div>\n</div>\n\nMaking shapes out of discrete collective elements is not a new idea.\nThere is a long history of modular self-reconfigurable robots and swarm robotics.\nThese areas of research have explored the idea of collective and general-purpose shape transformation for robotic applications, such as space exploration, rescue, and navigation.\nHowever, there are many critical challenges when we apply this approach for **interactive interfaces**.\n\nFor example, the speed of transformation needs to be much faster than for robotic applications, as the interactive system must change and respond to the user in real-time (e.g., in seconds, not minutes or hours). Another consideration is scalability.\nTo display meaningful information, it may require a relatively large number of elements, which often introduces implementation problems.\nFinally, unlike autonomous systems, interactive systems must consider the interaction between humans and objects --- there remains work to be done in understanding how we might interact with such collective elements and how these interfaces could support everyday human activities.\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-5.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-5.jpg" /></a>\n  </div>\n</div>\n\nThis thesis addresses these questions by investigating how collective shape construction and transformation can be used for {\\it interactive computer interfaces}.\nTo this end, this thesis introduces **collective shape-changing interfaces**, a new class of shape-changing interfaces constructed by a swarm of discrete physical elements.\nThe main contribution of this thesis is the first exploration of this new class of shape-changing interfaces in the following four domains:\n\n1. **Shape representation**: <br/>\nexplore what types of shape representations are possible to display information\n\n\n2. **Reconfiguration methods**: <br/>\nexplore how both active and passive elements can be used to construct a shape for interactive interfaces\n\n\n3. **Interaction**: <br/>\nexplore how the user can interact with many collective elements through direct physical manipulation,\n\n\n4. **Applications**: <br/>\nexplore, illustrate, and demonstrate what kind of applications are achievable for human-computer interaction.\n\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-1.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-1.jpg" /></a>\n  </div>\n</div>\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-2-1.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-2-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-9.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-9.jpg" /></a>\n  </div>\n</div>\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-7.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-7.jpg" /></a>\n  </div>\n</div>\n\nThis new class of shape-changing interfaces promises to address some of the limitations of the current shape-changing interfaces. For example, a swarm of modular elements enables us to decompose the large, monolithic shape-changing objects into a set of simple, distributed elements. This significantly contributes to the deployability of the system in everyday environments. In addition, the swarm behaviors enable us to make unbounded shape transformations for expressive representation.\nThrough my explorations of various proof-of-concept prototypes, I demonstrate how we can push the boundary of the current shape-changing interfaces by leveraging the collective behaviors of both active and passive elements.\nI also demonstrate how these dynamic shapes can support a range of application scenarios, such as interactive displays, adaptive environments, dynamic data physicalization, and accessibility support for people with visual impairments.\nFinally, I discuss the challenges and opportunities for using this approach towards the future of dynamic physical media.\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-6.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-6.jpg" /></a>\n  </div>\n</div>\n\n\n# Thesis Contributions\nThis thesis makes contributions to the field of Human-Computer Interaction in the following areas:\n\n1. A design space exploration of dynamic shape construction with collective elements\n\n\n2. A new taxonomy and investigation of active and passive shape construction and transformation with collective elements\n\n\n3. A novel technique for creating a dynamic shape with active shape-transformable swarm robots (e.g., ShapeBots, LiftTiles)\n\n\n4. A novel technique for constructing 3D shapes with an assembly of passive magnetically connectable blocks (e.g., Dynablock)\n\n\n5. A novel technique for actuating passive magnetic markers with scalable electro-magnetic actuation (e.g., FluxMarker, Reactile)\n\n\n6. A novel technique for actuating existing objects to reconfigure spatial layouts (e.g., RoomShift)\n\n\n7. A novel interaction technique for programming the dynamic shape construction on a 2D surface with direct physical manipulation (e.g., Reactile)\n\n\n<br/>\n\n<div class="figures ui four column grid">\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-1.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-2.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-3.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-3.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-4.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-4.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-5.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-5.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-6.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-6.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-7.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-7.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-8.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-8.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-9.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-9.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-10.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-10.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-11.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-11.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-12.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-12.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-13.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-13.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-14.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-14.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-15.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-15.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-16.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-16.jpg" /></a>\n  </div>\n</div>\n\n# PhD Dissertation Defense\n\n<div class="video-container" style="display: block">\n  <iframe\n    class="embed"\n    width="100%"\n    height="315"\n    src="https://www.youtube.com/embed/FHmp7BIhXJI"\n    frameborder="0"\n    allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"\n    allowFullScreen={true}\n    mozAllowFullScreen={true}\n    msAllowFullScreen={true}\n    oAllowFullScreen={true}\n    webkitAllowFullScreen={true}\n  ></iframe>\n</div>\n\n<br/>\n\nThesis Committee:\n\n- **Daniel Leithinger** (CU Boulder, chair)\n\n\n- **Mark Gross** (CU Boulder)\n\n\n- **Tom Yeh** (CU Boulder)\n\n\n- **Hiroshi Ishii** (MIT Media Lab)\n\n\n- **Takeo Igarashi** (The University of Tokyo)\n\n<br/>\n\nDate: May 13th, 2020',bodyHtml:'<h1>Abstract</h1>\n<p>This thesis explores dynamic and collective shape construction as a new way to <strong>physicalize</strong> digital information for interactive physical displays --- i.e., <strong>shape-changing displays</strong> enabled by a swarm of collective elements. Through physical form of digital objects, the user can directly touch, grasp, and manipulate digital information through rich tangible and embodied interactions, but at the same time, such physical objects can dynamically change their shape for an interactive computer display and interface through collective shape construction and transformation with a swarm of elements. The goal of this thesis is to envision and illustrate how such an interface might support human activities by transforming physical forms at various sizes, from millimeter to meter scale.</p>\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-1.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-1.jpg" /></a>\n  </div>\n</div>\n<p>To achieve this goal, this thesis introduces <strong>collective shape-changing interface</strong>, a new type of shape-changing interfaces constructed by discrete, collective, physical elements. This proposed approach promises to address the current limitations of shape-changing interfaces --- wherein a swarm of modular elements enables us to decompose the large, monolithic shape-changing objects into a set of simple, distributed elements. At the same time, their swarm behaviors enable us to make an unbounded shape transformation for expressive representation. This thesis contributes to the first exploration of this new class of shape-changing interfaces and proposes two approaches: active and passive shape construction. In active shape construction, collective elements can dynamically move and reconfigure themselves to construct a three-dimensional shape. Passive shape construction instead leverages external actuation to assemble and transform collective passive objects for dynamic shape creation. I explore and demonstrate how active and passive collective shape construction can be used as a future of computer interfaces, by developing various prototypes built on top of novel hardware and software platforms. Given these investigations, I discuss the design implications and possible research directions towards the future of collective shape-changing user interfaces.</p>\n<h1>Introduction</h1>\n<p>What if computer displays can represent information not only {\\it graphically} but also <em>physically</em>? What if such physical forms of information could be as malleable and programmable as the pixels on a computer screen? If so, it could be used as a dynamic physical medium to interact with the digital world.</p>\n<p>Ivan Sutherland, a founder of virtual and augmented reality, once envisioned that the future of computer displays would be <em>&quot;a room within which the computer can control the existence of matter&quot;</em>\nThis radical vision has inspired many researchers over the decades, as such technologies could open up a new\nparadigm of human-machine interfaces.</p>\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-2.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-2.jpg" /></a>\n  </div>\n</div>\n<p>However, we are still far from this exciting future. Today\'s computer interfaces mostly focus on <em>screen-based</em> interaction, where the screens serve as a ``looking window\'\' of the digital world --- the user can see digital information through the glass, but a barrier between what is inside (digital world) and what is outside (physical world) confines how we interact with the digital world. Current technologies do not allow us to directly touch, feel, grasp, and manipulate digital objects, in the same way that humans have done with physical objects for hundreds of thousands of years.</p>\n<h1>Thesis Statement</h1>\n<p>The goal of this thesis is to bring Sutherland\'s vision closer to reality by developing a new form of interactive and dynamic physical displays, and to illustrate how such an interface might support human activities by transforming physical forms and environments at various scales.</p>\n<p>As a step toward this vision, this thesis explores dynamic and collective shape construction as a new way to <strong>physicalize</strong> digital information for interactive physical displays --- i.e., <strong>shape-changing displays</strong> enabled by a swarm of collective elements.\nCollective elements refer to discrete physical objects that can construct a physical, three-dimensional shape.</p>\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-3.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-3.jpg" /></a>\n  </div>\n</div>\n<p>Each individual element can dynamically change its shape, position, and other physical properties through internal or external actuation, as to collectively construct and transform the overall physical shape.\nThis enables a new way of representing digital information.\nSuch physical shapes allow the user not only to see information, but to touch, feel, grasp, construct, and manipulate it, in the same way that interact with physical objects.\nAt the same time, these physical objects must also embody dynamic computation. Collective elements can dynamically and programmatically reconfigure themselves, as if they are rendered in an interactive computer display and interface.</p>\n<p>In contrast to shape changes made of monolithic materials, constructing shapes out of discrete elements enables rich expressiveness in representing information.\nLike pixels on a screen, they make shapes by collectively transforming the overall structure.\nAdditionally, their components can be simple and interchangeable, thus allowing for scale.\nThese elements can also interact with existing environments, and they make everyday objects and environments more dynamic, adaptive, and interactive by collectively actuating and reconfiguring them in a programmable fashion.</p>\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-4.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-4.jpg" /></a>\n  </div>\n</div>\n<p>Making shapes out of discrete collective elements is not a new idea.\nThere is a long history of modular self-reconfigurable robots and swarm robotics.\nThese areas of research have explored the idea of collective and general-purpose shape transformation for robotic applications, such as space exploration, rescue, and navigation.\nHowever, there are many critical challenges when we apply this approach for <strong>interactive interfaces</strong>.</p>\n<p>For example, the speed of transformation needs to be much faster than for robotic applications, as the interactive system must change and respond to the user in real-time (e.g., in seconds, not minutes or hours). Another consideration is scalability.\nTo display meaningful information, it may require a relatively large number of elements, which often introduces implementation problems.\nFinally, unlike autonomous systems, interactive systems must consider the interaction between humans and objects --- there remains work to be done in understanding how we might interact with such collective elements and how these interfaces could support everyday human activities.</p>\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-5.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-5.jpg" /></a>\n  </div>\n</div>\n<p>This thesis addresses these questions by investigating how collective shape construction and transformation can be used for {\\it interactive computer interfaces}.\nTo this end, this thesis introduces <strong>collective shape-changing interfaces</strong>, a new class of shape-changing interfaces constructed by a swarm of discrete physical elements.\nThe main contribution of this thesis is the first exploration of this new class of shape-changing interfaces in the following four domains:</p>\n<ol>\n<li>\n<p><strong>Shape representation</strong>: <br/>\nexplore what types of shape representations are possible to display information</p>\n</li>\n<li>\n<p><strong>Reconfiguration methods</strong>: <br/>\nexplore how both active and passive elements can be used to construct a shape for interactive interfaces</p>\n</li>\n<li>\n<p><strong>Interaction</strong>: <br/>\nexplore how the user can interact with many collective elements through direct physical manipulation,</p>\n</li>\n<li>\n<p><strong>Applications</strong>: <br/>\nexplore, illustrate, and demonstrate what kind of applications are achievable for human-computer interaction.</p>\n</li>\n</ol>\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-1.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-1.jpg" /></a>\n  </div>\n</div>\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-2-1.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-2-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-9.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-9.jpg" /></a>\n  </div>\n</div>\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-7.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-7.jpg" /></a>\n  </div>\n</div>\n<p>This new class of shape-changing interfaces promises to address some of the limitations of the current shape-changing interfaces. For example, a swarm of modular elements enables us to decompose the large, monolithic shape-changing objects into a set of simple, distributed elements. This significantly contributes to the deployability of the system in everyday environments. In addition, the swarm behaviors enable us to make unbounded shape transformations for expressive representation.\nThrough my explorations of various proof-of-concept prototypes, I demonstrate how we can push the boundary of the current shape-changing interfaces by leveraging the collective behaviors of both active and passive elements.\nI also demonstrate how these dynamic shapes can support a range of application scenarios, such as interactive displays, adaptive environments, dynamic data physicalization, and accessibility support for people with visual impairments.\nFinally, I discuss the challenges and opportunities for using this approach towards the future of dynamic physical media.</p>\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-6.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-6.jpg" /></a>\n  </div>\n</div>\n<h1>Thesis Contributions</h1>\n<p>This thesis makes contributions to the field of Human-Computer Interaction in the following areas:</p>\n<ol>\n<li>\n<p>A design space exploration of dynamic shape construction with collective elements</p>\n</li>\n<li>\n<p>A new taxonomy and investigation of active and passive shape construction and transformation with collective elements</p>\n</li>\n<li>\n<p>A novel technique for creating a dynamic shape with active shape-transformable swarm robots (e.g., ShapeBots, LiftTiles)</p>\n</li>\n<li>\n<p>A novel technique for constructing 3D shapes with an assembly of passive magnetically connectable blocks (e.g., Dynablock)</p>\n</li>\n<li>\n<p>A novel technique for actuating passive magnetic markers with scalable electro-magnetic actuation (e.g., FluxMarker, Reactile)</p>\n</li>\n<li>\n<p>A novel technique for actuating existing objects to reconfigure spatial layouts (e.g., RoomShift)</p>\n</li>\n<li>\n<p>A novel interaction technique for programming the dynamic shape construction on a 2D surface with direct physical manipulation (e.g., Reactile)</p>\n</li>\n</ol>\n<br/>\n<div class="figures ui four column grid">\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-1.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-2.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-3.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-3.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-4.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-4.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-5.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-5.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-6.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-6.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-7.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-7.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-8.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-8.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-9.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-9.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-10.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-10.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-11.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-11.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-12.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-12.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-13.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-13.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-14.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-14.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-15.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-15.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/phd-thesis/figure-10-16.jpg" data-lightbox="lightbox"><img src="/static/projects/phd-thesis/figure-10-16.jpg" /></a>\n  </div>\n</div>\n<h1>PhD Dissertation Defense</h1>\n<div class="video-container" style="display: block">\n  <iframe\n    class="embed"\n    width="100%"\n    height="315"\n    src="https://www.youtube.com/embed/FHmp7BIhXJI"\n    frameborder="0"\n    allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"\n    allowFullScreen={true}\n    mozAllowFullScreen={true}\n    msAllowFullScreen={true}\n    oAllowFullScreen={true}\n    webkitAllowFullScreen={true}\n  ></iframe>\n</div>\n<br/>\n<p>Thesis Committee:</p>\n<ul>\n<li>\n<p><strong>Daniel Leithinger</strong> (CU Boulder, chair)</p>\n</li>\n<li>\n<p><strong>Mark Gross</strong> (CU Boulder)</p>\n</li>\n<li>\n<p><strong>Tom Yeh</strong> (CU Boulder)</p>\n</li>\n<li>\n<p><strong>Hiroshi Ishii</strong> (MIT Media Lab)</p>\n</li>\n<li>\n<p><strong>Takeo Igarashi</strong> (The University of Tokyo)</p>\n</li>\n</ul>\n<br/>\n<p>Date: May 13th, 2020</p>\n',dir:"content/output/projects",base:"phd-thesis.json",ext:".json",sourceBase:"phd-thesis.md",sourceExt:".md"}},Jg5j:function(e){e.exports={id:"trace-diff",name:"TraceDiff",description:"Debugging Unexpected Code Behavior Using Trace Divergences",title:"TraceDiff: Debugging Unexpected Code Behavior Using Trace Divergences",authors:["Ryo Suzuki","Gustavo Soares","Andrew Head","Elena Glassman","Ruan Reis","Melina Mongiovi","Loris D’Antoni","Bjöern Hartmann"],year:2017,booktitle:"In Proceedings of 2017 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC '17)",publisher:"IEEE Press, Piscataway, NJ, USA",pages:"107-115",doi:"https://doi.org/10.1109/VLHCC.2017.8103457",conference:{name:"VL/HCC 2017",fullname:"IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC 2017)",url:"https://sites.google.com/site/vlhcc2017/"},pdf:"vlhcc-2017-tracediff.pdf",slide:"vlhcc-2017-tracediff-slide.pdf",github:"https://github.com/ryosuzuki/trace-diff",demo:"https://ryosuzuki.github.io/trace-diff/",ieee:"http://ieeexplore.ieee.org/document/8103457/",arxiv:"https://arxiv.org/abs/1708.03786a",related:{title:"Exploring the Design Space of Automatically Synthesized Hints for Introductory Programming Assignments",authors:["Ryo Suzuki","Gustavo Soares","Elena Glassman","Andrew Head","Loris D'Antoni","Björn Hartmann"],year:2017,booktitle:"In Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems (CHI EA '17)",publisher:"ACM, New York, NY, USA",pages:"2951-2958",doi:"https://doi.org/10.1145/3027063.3053187",pdf:"chi-2017-lbw.pdf",suffix:"lbw",pageCount:6},pageCount:9,slideCount:77,bodyContent:'<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/trace-diff/top.mp4" type="video/mp4"></source>\n</video>\n\n# Abstract\n\nRecent advances in program synthesis offer means to automatically debug student submissions and generate personalized feedback in massive programming classrooms. When automatically generating feedback for programming assignments, a key challenge is designing pedagogically useful hints that are as effective as the manual feedback given by teachers. Through an analysis of teachers’ hint-giving practices in 132 online Q&A posts, we establish three design guidelines that an effective feedback design should follow. Based on these guidelines, we develop a feedback system that leverages both program synthesis and visualization techniques. Our system compares the dynamic code execution of both incorrect and fixed code and highlights how the error leads to a difference in behavior and where the incorrect code trace diverges from the expected solution. Results from our study suggest that our system enables students to detect and fix bugs that are not caught by students using another existing visual debugging tool.',bodyHtml:'<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/trace-diff/top.mp4" type="video/mp4"></source>\n</video>\n<h1>Abstract</h1>\n<p>Recent advances in program synthesis offer means to automatically debug student submissions and generate personalized feedback in massive programming classrooms. When automatically generating feedback for programming assignments, a key challenge is designing pedagogically useful hints that are as effective as the manual feedback given by teachers. Through an analysis of teachers’ hint-giving practices in 132 online Q&amp;A posts, we establish three design guidelines that an effective feedback design should follow. Based on these guidelines, we develop a feedback system that leverages both program synthesis and visualization techniques. Our system compares the dynamic code execution of both incorrect and fixed code and highlights how the error leads to a difference in behavior and where the incorrect code trace diverges from the expected solution. Results from our study suggest that our system enables students to detect and fix bugs that are not caught by students using another existing visual debugging tool.</p>\n',dir:"content/output/projects",base:"trace-diff.json",ext:".json",sourceBase:"trace-diff.md",sourceExt:".md"}},LrW7:function(e){e.exports={}},MyTI:function(e){e.exports=[{period:"August, 2015 --- August, 2020",role:"RA",logo:"cu-boulder.png",institute:{name:"CU Boulder",url:"https://colorado.edu"},lab:{name:"THING Lab",url:"https://www.colorado.edu/atlas/thing-lab"},advisors:[{name:"Daniel Lightinger",url:"http://leithinger.com/"},{name:"Mark D. Gross",url:"http://mdgross.net/"},{name:"Tom Yeh",url:"http://tomyeh.info/"}]},{period:"May, 2020 --- August, 2020",role:"Intern",logo:"microsoft.png",institute:{name:"Microsoft Research",url:"https://www.microsoft.com/en-us/research/"},lab:{name:"EPIC Group",url:"https://www.microsoft.com/en-us/research/group/epic/"},advisors:[{name:"Mar Gonzalez Franco",url:"https://www.microsoft.com/en-us/research/people/margon/"},{name:"Eyal Ofek",url:"https://www.microsoft.com/en-us/research/people/eyalofek/"},{name:"Ken Hinckley",url:"https://www.microsoft.com/en-us/research/people/kenh/"}]},{period:"May, 2019 --- August, 2019",role:"Intern",logo:"adobe.png",institute:{name:"Adobe Research",url:"https://colorado.edu"},lab:{name:"Creative Intelligence Lab",url:"https://research.adobe.com/"},advisors:[{name:"Rubaiat Habib",url:"https://rubaiathabib.me/"},{name:"Li-Yi Wei",url:"https://www.liyiwei.org/"},{name:"Stephen Diverdi",url:"http://www.stephendiverdi.com/"},{name:"Danny Kaufman",url:"http://dannykaufman.io/"}]},{period:"December, 2017 --- October, 2018",role:"Intern",logo:"ut.png",institute:{name:"University of Tokyo",url:"https://colorado.edu"},lab:{name:"ERATO UIN",url:"http://www.jst.go.jp/erato/kawahara"},advisors:[{name:"Yasuaki Kakehi",url:"http://www.xlab.sfc.keio.ac.jp/"},{name:"Yoshihiro Kawahara",url:"http://www.akg.t.u-tokyo.ac.jp/"},{name:"Ryuma Niiyama",url:"https://scholar.google.co.jp/citations?user=0NMf5sgAAAAJ&hl=en"}]},{period:"May, 2016 --- August, 2016",role:"Intern",logo:"uc-berkeley.png",institute:{name:"UC Berkeley",url:null},lab:{name:"BiD Lab",url:"http://bid.berkeley.edu/"},advisors:[{name:"Bjoern Hartmann",url:"http://people.eecs.berkeley.edu/~bjoern/"}]},{period:"May, 2015 --- August, 2015",role:"Intern",logo:"stanford-2.png",institute:{name:"Stanford University",url:"https://stanford.edu"},lab:{name:"HCI Group",url:"http://hci.stanford.edu/"},advisors:[{name:"Michael Bernstein",url:"http://hci.stanford.edu/msb/"}]},{period:"October, 2014 --- May, 2015",role:"RA",logo:"ut.png",institute:{name:"University of Tokyo",url:null},lab:{name:"IIS Lab",url:"http://iis-lab.org/"},advisors:[{name:"Koji Yatani",url:"http://iis-lab.org/member/koji-yatani/"}]},{period:"December, 2014 --- March, 2015",role:"Intern",logo:"aist.png",institute:{name:"AIST",url:null},lab:{name:"Media Interaction",url:"https://staff.aist.go.jp/m.goto/MIG/index-j.html"},advisors:[{name:"Jun Kato",url:"http://junkato.jp/"}]}]},PSd4:function(e){e.exports={id:"mixed-initiative",name:"Mixed-Initiative Code Feedback",description:"Writing Reusable Code Feedback at Scale with Mixed-Initiative Program Synthesis",title:"Writing Reusable Code Feedback at Scale with Mixed-Initiative Program Synthesis",authors:["Andrew Head","Elena Glassman","Gustavo Soares","Ryo Suzuki","Lucas Figueredo","Loris D’Antoni","Björn Hartmann"],note:"(the first three authors equally contributed)",year:2017,booktitle:"In Proceedings of the Fourth (2017) ACM Conference on Learning @ Scale (L@S '17)",publisher:"ACM, New York, NY, USA",pages:"89-98",doi:"https://doi.org/10.1145/3051457.3051467",conference:{name:"L@S 2017",fullname:"The ACM Conference on Learning at Scale (L@S 2017)",url:"http://learningatscale.acm.org/las2017"},pdf:"las-2017-mixed.pdf",slide:"las-2017-mixed-slide.pdf","acm-dl":"http://dl.acm.org/citation.cfm?id=3051467",pageCount:10,slideCount:62,image:"mixed-initiative.png",bodyContent:"",bodyHtml:"",dir:"content/output/projects",base:"mixed-initiative.json",ext:".json",sourceBase:"mixed-initiative.md",sourceExt:".md"}},RHEb:function(e,t,i){"use strict";i.r(t);for(var a=i("0iUn"),s=i("sLSF"),o=i("MI3g"),n=i("a7VT"),r=i("Tit0"),c=i("q1tI"),l=i.n(c),p=(i("IujW"),i("6T/A"),[]),h=0,d=["roomshift","lift-tiles","shapebots","morphio","dynablock","tabby","reactile","pep","flux-marker","trace-diff","mixed-initiative","refazer","atelier"];h<d.length;h++){var g=d[h],u=i("o0EK")("./".concat(g,".json"));p.push(u)}var m=function(e){function t(){return Object(a.default)(this,t),Object(o.default)(this,Object(n.default)(t).apply(this,arguments))}return Object(r.default)(t,e),Object(s.default)(t,[{key:"componentDidMount",value:function(){}},{key:"render",value:function(){return l.a.createElement("div",{id:"projects"},l.a.createElement("h1",null,"PhD Dissertation"),l.a.createElement("div",{className:"project ui vertical segment stackable grid","data-id":""},l.a.createElement("div",{className:"six wide column"},l.a.createElement("a",{href:"/phd-thesis"},l.a.createElement("img",{className:"ui rounded images",src:"/static/images/collective.jpg"}))),l.a.createElement("div",{className:"ten wide column"},l.a.createElement("a",{href:"phd-thesis"},l.a.createElement("h1",{className:"ui header",style:{marginBottom:"10px"}},"Collective Shape-changing Interfaces"),l.a.createElement("h2",{style:{margin:"5px 0"}},"Dynamic Shape Construction and Transformation with Collective Elements")),l.a.createElement("p",null,l.a.createElement("strong",null,"Ryo Suzuki")))),l.a.createElement("h1",null,"Full Papers"),p.map(function(e){return l.a.createElement("div",{className:"project ui vertical segment stackable grid","data-id":e.id},l.a.createElement("div",{className:"six wide column"},e.image&&l.a.createElement("a",{href:"/".concat(e.id)},l.a.createElement("img",{className:"ui rounded images",src:"/static/images/".concat(e.image)})),!e.image&&l.a.createElement("video",{poster:"/static/posters/".concat(e.id,".png"),autoplay:"",loop:"loop",muted:"true",playsinline:"",width:"100%",onclick:"this.play()",onmouseover:"this.play()"},l.a.createElement("source",{src:"/static/video/".concat(e.id,".mp4"),type:"video/mp4"}))),l.a.createElement("div",{className:"ten wide column"},l.a.createElement("a",{href:"/".concat(e.id)},l.a.createElement("h1",{className:"ui header",style:{marginBottom:"10px"}},l.a.createElement("span",null,e.name),l.a.createElement("span",{className:"ui big label"},e.conference.name),l.a.createElement("span",{className:"ui teal large label",style:{display:["morphio"].includes(e.id)?"inline-block":"none"}},"Best Paper Award")),l.a.createElement("h2",{style:{margin:"5px 0"}},e.description)),l.a.createElement("p",null,e.authors.map(function(e){return e.includes("Ryo Suzuki")?l.a.createElement("strong",null,e):l.a.createElement("span",null,e)}).reduce(function(e,t){return[e,", ",t]}),"   ",l.a.createElement("span",{style:{color:"gray"}},e.note))))}))}}]),t}(l.a.Component);t.default=m},RNiq:function(e,t,i){"use strict";i.r(t);var a=i("0iUn"),s=i("sLSF"),o=i("MI3g"),n=i("a7VT"),r=i("Tit0"),c=i("q1tI"),l=i.n(c),p=i("IujW"),h=i.n(p),d=(i("TIwn"),i("MyTI"),i("yC4j")),g=i("+a94"),u=(i("LrW7"),i("qg4i")),m=i("e3jU"),f=i("W+IF"),b=i("RHEb"),v=i("42TL"),y=i("Dx8+"),w=function(e){function t(){return Object(a.default)(this,t),Object(o.default)(this,Object(n.default)(t).apply(this,arguments))}return Object(r.default)(t,e),Object(s.default)(t,[{key:"componentDidMount",value:function(){}},{key:"render",value:function(){return l.a.createElement("div",null,l.a.createElement("title",null,"Ryo Suzuki | University of Colorado Boulder"),l.a.createElement("div",{className:"ui stackable grid"},l.a.createElement("div",{className:"one wide column"}),l.a.createElement("div",{className:"eleven wide column centered"},l.a.createElement(f.default,null),l.a.createElement("section",{id:"container"},l.a.createElement(b.default,null),l.a.createElement("div",{className:"ui divider"}),l.a.createElement("div",{id:"posters"},l.a.createElement("h1",null,"Posters and Demos"),l.a.createElement("div",{className:"ui vertical segment"},l.a.createElement("div",{className:"ui bulleted list"},u.map(function(e){return l.a.createElement("div",{className:"item",target:"_blank",style:{lineHeight:"1.8rem"}},l.a.createElement("b",null,"[",e.series,"]"),l.a.createElement("br",null),l.a.createElement("a",{href:"/publications/"+e.pdf},l.a.createElement("b",null,e.title)),l.a.createElement("br",null),e.author,", ",l.a.createElement("i",null,e.booktitle," (",e.series,")"),". ",e.publisher,", ",e.address,", ",e.pages,".",l.a.createElement("br",null),l.a.createElement("a",{href:"/publications/"+e.pdf,target:"_blank",style:{marginRight:"5px",display:e.pdf?"inline":"none"}},"[PDF]"),l.a.createElement("a",{href:"/publications/"+e.poster,target:"_blank",style:{marginRight:"5px",display:e.poster?"inline":"none"}},"[Poster]"),l.a.createElement("a",{href:"/publications/"+e.slide,target:"_blank",style:{marginRight:"5px",display:e.slide?"inline":"none"}},"[Slide]"),l.a.createElement("a",{href:e.url,target:"_blank",style:{marginRight:"5px",display:e.url?"inline":"none"}},"[DOI]"))})))),l.a.createElement("div",{className:"ui divider"}),l.a.createElement("div",{id:"press"},l.a.createElement("h1",null,"Selected Press Coverage"),l.a.createElement("div",{className:"ui vertical segment"},l.a.createElement("div",{className:"ui bulleted list"},m.map(function(e){return l.a.createElement("div",{className:"item"},l.a.createElement("a",{href:e.url,target:"_blank"},"[",e.date,"] ",l.a.createElement("b",null,e.media)," ",l.a.createElement("i",null,e.title)))}))),l.a.createElement("div",{className:"ui divider"}),l.a.createElement("div",{id:"activities"},l.a.createElement("h1",null,"Professional Activities"),l.a.createElement("div",{className:"ui vertical segment"},l.a.createElement(h.a,{source:g.bodyContent}))),l.a.createElement("div",{className:"ui divider"}),l.a.createElement("div",{id:"fellowship"},l.a.createElement("h1",null,"Funding and Fellowship"),l.a.createElement("div",{className:"ui vertical segment"},l.a.createElement(h.a,{source:d.bodyContent})))))),l.a.createElement("div",{id:"side",className:"three wide column centered",style:{marginTop:"50px"}},l.a.createElement(v.default,null),l.a.createElement(y.default,null),l.a.createElement("br",null),l.a.createElement("a",{className:"twitter-timeline",height:"1500px",href:"https://twitter.com/ryosuzk?ref_src=twsrc%5Etfw"},"Tweets by @ryosuzk")," ",l.a.createElement("script",{async:!0,src:"https://platform.twitter.com/widgets.js",charset:"utf-8"})),l.a.createElement("div",{className:"one wide column"})),l.a.createElement("div",{className:"ui stackable grid"},l.a.createElement("div",{className:"sixteen wide column centered"},l.a.createElement("p",{style:{textAlign:"center"}}))))}}]),t}(l.a.Component);t.default=w},"W+IF":function(e,t,i){"use strict";i.r(t);var a=i("0iUn"),s=i("sLSF"),o=i("MI3g"),n=i("a7VT"),r=i("Tit0"),c=i("q1tI"),l=i.n(c),p=function(e){function t(){return Object(a.default)(this,t),Object(o.default)(this,Object(n.default)(t).apply(this,arguments))}return Object(r.default)(t,e),Object(s.default)(t,[{key:"render",value:function(){return l.a.createElement("header",{className:"ui stackable grid"},l.a.createElement("div",{className:"ui sixteen wide column"},l.a.createElement("h1",{className:"ui huge header"},l.a.createElement("img",{style:{maxWidth:"62px",marginRight:"15px"},src:"/static/images/profile.png",className:"ui circular image"}),l.a.createElement("div",{className:"content"},"Ryo Suzuki",l.a.createElement("div",{className:"sub header",style:{fontSize:"1.5rem"}},"University of Colorado Boulder"))),l.a.createElement("video",{id:"top-video",poster:"/static/posters/top.png",preload:"metadata",autoPlay:!0,loop:!0,muted:!0,playsInline:!0,"webkit-playsinline":""},l.a.createElement("source",{src:"/static/video/top.mp4",type:"video/mp4"})),l.a.createElement("div",{id:"profile",style:{fontSize:"1.3rem"}},l.a.createElement("p",null,"I am a Ph.D. candidate at the ",l.a.createElement("a",{href:"https://www.colorado.edu/cs/",target:"_blank"},l.a.createElement("b",null,"University of Colorado Boulder")),", Department of Computer Science, advised by ",l.a.createElement("a",{href:"http://leithinger.com/",target:"_blank"},l.a.createElement("b",null,"Daniel Leithinger"))," and ",l.a.createElement("a",{href:"http://mdgross.net/",target:"_blank"},l.a.createElement("b",null,"Mark D. Gross"))," in ",l.a.createElement("a",{href:"https://www.colorado.edu/atlas/thing-lab",target:"_blank"},l.a.createElement("b",null,"THING Lab"))," and ",l.a.createElement("a",{href:"http://hcc.colorado.edu/",target:"_blank"},l.a.createElement("b",null,"Human-Computer Interaction Group"))),l.a.createElement("p",null,"My research interest lies on the intersection between ",l.a.createElement("b",null,"Human-Computer Interaction (HCI)")," and ",l.a.createElement("b",null,"Robotics"),". During my PhD, I have developed novel ",l.a.createElement("b",null,"shape-changing interfaces")," that leverage ",l.a.createElement("b",null,"swarm and soft robots at various scales")," (i.e., from mm- to m-scale). My research explores how we can make the ",l.a.createElement("b",null,"physical environment more adaptive")," with context-aware swarm robotic assistant that can be embedded into our everyday life.")),l.a.createElement("div",{className:"ui horizontal list",style:{marginTop:"20px"}},l.a.createElement("div",{className:"item"},l.a.createElement("a",{href:"https://scholar.google.com/citations?user=klWjaQIAAAAJ",target:"_blank",style:{fontSize:"1.2em"}},l.a.createElement("i",{className:"fas fa-graduation-cap fa-fw"}),"Google Scholar")),l.a.createElement("div",{className:"item"},l.a.createElement("a",{href:"/cv.pdf",target:"_blank",style:{fontSize:"1.2em"}},l.a.createElement("i",{className:"far fa-file fa-fw"}),"CV")),l.a.createElement("div",{className:"item"},l.a.createElement("a",{href:"mailto:ryo.suzuki@colorado.edu",target:"_blank",style:{fontSize:"1.2em"}},l.a.createElement("i",{className:"far fa-envelope fa-fw"}),"Email")),l.a.createElement("div",{className:"item"},l.a.createElement("a",{href:"https://www.facebook.com/ryosuzk",target:"_blank",style:{fontSize:"1.2em"}},l.a.createElement("i",{className:"fab fa-facebook-square fa-fw"}),"ryosuzk")),l.a.createElement("div",{className:"item"},l.a.createElement("a",{href:"https://twitter.com/ryosuzk",target:"_blank",style:{fontSize:"1.2em"}},l.a.createElement("i",{className:"fab fa-twitter fa-fw"}),"ryosuzk")),l.a.createElement("div",{className:"item"},l.a.createElement("a",{href:"https://twitter.com/HCI_Comics",target:"_blank",style:{fontSize:"1.2em"}},l.a.createElement("i",{className:"fab fa-twitter fa-fw"}),"HCI_Comics (ja)")),l.a.createElement("div",{className:"item"},l.a.createElement("a",{href:"https://github.com/ryosuzuki",target:"_blank",style:{fontSize:"1.2em"}},l.a.createElement("i",{className:"fab fa-github-alt fa-fw"}),"ryosuzuki")),l.a.createElement("div",{className:"item"},l.a.createElement("a",{href:"https://www.linkedin.com/in/ryosuzuki/",target:"_blank",style:{fontSize:"1.2em"}},l.a.createElement("i",{className:"fab fa-linkedin-in fa-fw"}),"ryosuzuki")))),l.a.createElement("div",{className:"one wide column"}))}}]),t}(l.a.Component);t.default=p},"W/HP":function(e){e.exports={id:"pep",name:"PEP",description:"3D Printed Electronic Papercrafts - An Integrated Approach for 3D Sculpting Paper-based Electronic Devices",title:"PEP (3D Printed Electronic Papercrafts): An Integrated Approach for 3D Sculpting Paper-based Electronic Devices",authors:["Hyunjoo Oh","Tung D. Ta","Ryo Suzuki","Mark D. Gross","Yoshihiro Kawahara","Lining Yao"],year:2018,booktitle:"In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (CHI '18)",publisher:"ACM, New York, NY, USA",pages:"Paper 441, 12 pages",doi:"https://doi.org/10.1145/3173574.3174015",conference:{name:"CHI 2018",fullname:"The ACM CHI Conference on Human Factors in Computing Systems (CHI 2018)",url:"https://chi2018.acm.org/"},pdf:"chi-2018-pep.pdf",video:"https://vimeo.com/252080903",embed:"https://www.youtube.com/embed/DTd863suDN0","short-video":"https://www.youtube.com/watch?v=DTd863suDN0","acm-dl":"https://dl.acm.org/citation.cfm?id=3174015",pageCount:12,slideCount:0,bodyContent:"",bodyHtml:"",dir:"content/output/projects",base:"pep.json",ext:".json",sourceBase:"pep.md",sourceExt:".md"}},"X0/d":function(e){e.exports={id:"morphio",name:"MorphIO",description:"Entirely Soft Sensing and Actuation Modules for Programming Shape Changes through Tangible Interaction",title:"MorphIO: Entirely Soft Sensing and Actuation Modules for Programming Shape Changes through Tangible Interaction",authors:["Ryosuke Nakayama*","Ryo Suzuki*","Satoshi Nakamaru","Ryuma Niiyama","Yoshihiro Kawahara","Yasuaki Kakehi"],note:"(the first two authors equally contributed)",year:2018,booktitle:"In Proceedings of the 2019 on Designing Interactive Systems Conference (DIS '19)",publisher:"ACM, New York, NY, USA",pages:"975-986",doi:"https://doi.org/10.1145/3322276.3322337",conference:{name:"DIS 2019",fullname:"The ACM conference on Designing Interactive Systems (DIS 2019) - Best Paper Award",url:"https://dis2019.com/"},pdf:"dis-2019-morphio.pdf",slide:"dis-2019-morphio-slide.pdf",video:"https://www.youtube.com/watch?v=ZkCcazfFD-M",embed:"https://www.youtube.com/embed/ZkCcazfFD-M","acm-dl":"https://dl.acm.org/citation.cfm?id=3322337",pageCount:12,slideCount:52,bodyContent:'# Abstract\n\nWe introduce **MorphIO, entirely soft sensing and actuation modules** for programming by demonstration of soft robots and shape-changing interfaces. MorphIO’s hardware consists of a **soft pneumatic actuator containing a conductive sponge sensor**. This allows both input and output of three-dimensional deformation of a soft material. Leveraging this capability, MorphIO enables a user to **record and later playback physical motion** of programmable shape-changing materials. In addition, the modular design of MorphIO’s unit allows the user to construct various shapes and topologies through magnetic connection. We demonstrate several application scenarios, including tangible character animation, locomotion experiment of a soft robot, and prototyping tools for animated soft objects. Our user study with six participants confirms the benefits of MorphIO, as compared to the existing programming paradigm.\n\n\n<video poster="/static/projects/morphio/video-poster/top.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/top.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/top.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-1-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-1-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-1-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-1-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-1-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-1-3.png" /></a>\n  </div>\n</div>\n\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-1-4.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-1-4.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-1-5.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-1-5.png" /></a>\n  </div>\n</div>\n\n# Introduction\n\n**Programmable soft materials** have a great potential for many application domains, such as soft robotics, material interfaces, accessibility, and haptic interfaces.\n**However, programming of such materials is hard.**\nThe dominant programming paradigm of soft robots and material interfaces is largely confined within a digital screen, leaving little room for users to interactively explore physical motion through tangible interaction. In such a workflow—compiling code on a digital screen then trans- ferring it into the physical object—users need to repeatedly switch between the digital and physical worlds. This leaves a large gulf of execution in their programming experiences.\nThus, the traditional programming paradigm significantly limits the user’s ability to experiment with the design of expressive motion. Moreover, due to this barrier, such an opportunity is largely limited to highly skilled programmers and researchers who are proficient in hardware programming.\n\n\n# MorphIO\n\nThis paper introduces **MorphIO, entirely soft sensing and actuation modules** for programming by demonstration of soft robots and shape-changing interfaces.\nMorphIO’s hardware consists of a soft pneumatic actuator containing a conductive sponge sensor. This allows for integrated and entirely soft shape-changing modules that can both sense and actuate a variety of three-dimensional deformations. Leveraging this capability, MorphIO enables the user to program behaviors by **recording and later playing back physical motions** through tangible interaction. In addition, the modular design of MorphIO’s unit allows the user to construct various shapes and topologies through magnetic connection, then **synthesize multiple recorded motions to achieve more complex behaviors**, such as bending, gripping, and walking.\n\n\n<video poster="/static/projects/morphio/video-poster/module.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/module.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/module.mp4" type="video/mp4"></source>\n</video>\n\n<br/>\n\n<video poster="/static/projects/morphio/video-poster/bear.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/bear.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/bear.mp4" type="video/mp4"></source>\n</video>\n\n\n# System Overview\n\nThe programming workflow with MorphIO is the following:\n\n- **Step 1:** A user starts manipulating the MorphIO unit.\n\n- **Step 2:** The demonstrated motion is detected and recorded through internal sensors, and the recorded sensor values are stored in the software.\n\n- **Step 3:** Once the user clicks play in the graphical user interface, the pneumatic pump starts supplying air.\n\n- **Step 4:** By controlling the air flow through switching on and off the solenoid valves, the system can control the behavior of the pneumatic actuator as it plays back the recorded motion.\n\nThe MorphIO system consists of the following components: A sensor and actuation unit, a sensing and actuation control unit, a microcontroller, software to control these units, and a visual interface for users to control behaviors. Figure illustrates the overview architecture of MorphIO.\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-6-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-6-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-6-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-6-2.png" /></a>\n  </div>\n</div>\n\n\n# Entirely Soft Sensing and Actuation Modules\n\nOur main contribution is a design and fabrication method for **a conductive sponge sensor** that can be embedded into an air chamber in the pneumatic actuator. The conductive sponge sensor leverages the porous structure to **sense the three-dimensional deformation by measuring the internal resistance value**; when contracted, the resistance value be- tween the top and bottom surfaces drops, and when extended, it increases. In contrast to existing sensing techniques, an elastic sponge allows for a higher degree of freedom in sensing capability (e.g., stretching, bending, and compression) without sacrificing the softness of the interface.\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-2-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-2-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-2-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-2-2.png" /></a>\n  </div>\n</div>\n\n\n<video poster="/static/projects/morphio/video-poster/mechanism.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/mechanism.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/mechanism.mp4" type="video/mp4"></source>\n</video>\n\n\n<br />\n\nMoreover, our **modular design** and **graphical interface** allows for easy experiments involving multiple units. For example, the system can visualize multiple recorded sensor values, so that the user can see, customize, and synthesize recorded motion to construct more complex behaviors. These hardware and software designs were informed by our formative study, wherein we interviewed five experienced researchers from the robotics and HCI communities.\n\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-3-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-3-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-3-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-3-2.png" /></a>\n  </div>\n</div>\n\n<video poster="/static/projects/morphio/video-poster/unit-x2.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/unit-x2.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/unit-x2.mp4" type="video/mp4"></source>\n</video>\n\n<video poster="/static/projects/morphio/video-poster/unit-x3.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/unit-x3.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/unit-x3.mp4" type="video/mp4"></source>\n</video>\n\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-4-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-4-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-4-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-4-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-4-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-4-3.png" /></a>\n  </div>\n</div>\n\n\n\n\n# Fabrication Process\n\nThe fabrication process follows three steps: 1) Fabricate an elastic sponge, 2) impregnate into conductive ink, and 3) attach electrodes and wires.\nTo fabricate an elastic sponge, we first prepare 6.0 g of elastomer prepolymer solution and 29.1 g of sodium-chloride, then mix them together by using a planetary centrifugal mixer. The mixed solution is injected into a 3D printed cylindrical mold (16mm diameter, 40mm height). Then we dry the material with an oven at 100 C degrees for one hour. Once dried, we immerse the sponge in water, so that the sodium chloride can melt, leaving a porous structure within the elastomer sponge.\n\n<video poster="/static/projects/morphio/video-poster/fabrication.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/fabrication.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/fabrication.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui four column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-3.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-4.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-4.png" /></a>\n  </div>\n</div>\n\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-5.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-5.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-6.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-6.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-7.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-7.png" /></a>\n  </div>\n</div>\n\n\n\n# Applications\n\nWe demonstrate several possible applications scenarios with MorphIO. 1) Tangible character animation, 2) Animating existing soft objects, 3) Remote manipulation of soft grippers, 4) Locomotion experimentation with soft robots.\n\n<div class="figures ui four column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-10-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-10-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-10-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-10-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-10-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-10-3.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-10-4.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-10-4.png" /></a>\n  </div>\n</div>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-11-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-11-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-11-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-11-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-11-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-11-3.png" /></a>\n  </div>\n</div>\n\n<video poster="/static/projects/morphio/video-poster/locomotion.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/locomotion.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/locomotion.mp4" type="video/mp4"></source>\n</video>\n\n\n# Evaluation\n\nWe conducted a user evaluation study to understand the bene- fits and limitations of MorphIO. In this study, we focused on answering the following research questions:\n\n- **RQ1:** Does MorphIO save time and reduce the number of iterations to program the target behavior, compared to the existing approach?\n\n- **RQ2:** Does MorphIO increase the expressiveness of the physical motion?\n\nTo answer these questions, we conducted a controlled experiment where we compared MorphIO (left) with the current programming approach. We chose Arduino IDE (right) as a base condition for the comparison, as this is the most common programming approach identified through our formative study. We provide three basic tasks to construct a program. For each task, the participants were asked to program three differ- ent emotions—happiness, anger, and sadness—of an animated character. We chose these emotions based on Ekman’s basic emotions for communication.\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-12-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-12-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-12-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-12-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-12-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-12-3.png" /></a>\n  </div>\n</div>\n\nThe average time of task completion time of MorphIO was 2m 19s, compared to 5m 21s with the control condition. The average number of iterations of MorphIO was 4.4 times, compared to 6.4 with the control condition, which confirms that MorphIO is significantly efficient in terms of task completion time and the number of iterations. When asked about the achievement of the expressions using a 9-point Likert scale, the average score with MorphIO was 6.5, compared to 6.3 with the control condition. We did not find differences between the two conditions. Thus, we conclude the result of our study as follows: **RQ1: Yes, RQ2: No**.\n\nBased on our post interviews, we discuss the benefits and limitations of our approach: **1) tangible interactions are suitable for sculpting rough motion**, **2) programming allows for fine-tuning more precise adjustments**. Thus, for future research, systems might allow users to quickly make a rough motion, which can automatically be converted into digital parameters so that the user can also precisely control and adjust the motion. The same human- computer cooperation approach can be applied to other design domains: For example, when designing an object, the user can quickly make rough shapes with clay, while letting a machine finish the details. We believe this insight can lead the HCI community to further explore design approaches wherein users and machines cooperate for enhanced interaction design.\n\n\n# Future Vision\nWe believe this approach’s potential for lowering the barrier and opening new opportunities for a larger community to begin designing, prototyping, and exploring soft material motion—not by coding on a screen, but by sculpting behaviors in the physical world.\nWe envision the future where people can interactively explore various behaviors through tangible interactions, **just like sculpting behaviors with clay**.\n\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-14.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-14.png" /></a>\n  </div>\n</div>',bodyHtml:'<h1>Abstract</h1>\n<p>We introduce <strong>MorphIO, entirely soft sensing and actuation modules</strong> for programming by demonstration of soft robots and shape-changing interfaces. MorphIO’s hardware consists of a <strong>soft pneumatic actuator containing a conductive sponge sensor</strong>. This allows both input and output of three-dimensional deformation of a soft material. Leveraging this capability, MorphIO enables a user to <strong>record and later playback physical motion</strong> of programmable shape-changing materials. In addition, the modular design of MorphIO’s unit allows the user to construct various shapes and topologies through magnetic connection. We demonstrate several application scenarios, including tangible character animation, locomotion experiment of a soft robot, and prototyping tools for animated soft objects. Our user study with six participants confirms the benefits of MorphIO, as compared to the existing programming paradigm.</p>\n<video poster="/static/projects/morphio/video-poster/top.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/top.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/top.mp4" type="video/mp4"></source>\n</video>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-1-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-1-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-1-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-1-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-1-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-1-3.png" /></a>\n  </div>\n</div>\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-1-4.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-1-4.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-1-5.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-1-5.png" /></a>\n  </div>\n</div>\n<h1>Introduction</h1>\n<p><strong>Programmable soft materials</strong> have a great potential for many application domains, such as soft robotics, material interfaces, accessibility, and haptic interfaces.\n<strong>However, programming of such materials is hard.</strong>\nThe dominant programming paradigm of soft robots and material interfaces is largely confined within a digital screen, leaving little room for users to interactively explore physical motion through tangible interaction. In such a workflow—compiling code on a digital screen then trans- ferring it into the physical object—users need to repeatedly switch between the digital and physical worlds. This leaves a large gulf of execution in their programming experiences.\nThus, the traditional programming paradigm significantly limits the user’s ability to experiment with the design of expressive motion. Moreover, due to this barrier, such an opportunity is largely limited to highly skilled programmers and researchers who are proficient in hardware programming.</p>\n<h1>MorphIO</h1>\n<p>This paper introduces <strong>MorphIO, entirely soft sensing and actuation modules</strong> for programming by demonstration of soft robots and shape-changing interfaces.\nMorphIO’s hardware consists of a soft pneumatic actuator containing a conductive sponge sensor. This allows for integrated and entirely soft shape-changing modules that can both sense and actuate a variety of three-dimensional deformations. Leveraging this capability, MorphIO enables the user to program behaviors by <strong>recording and later playing back physical motions</strong> through tangible interaction. In addition, the modular design of MorphIO’s unit allows the user to construct various shapes and topologies through magnetic connection, then <strong>synthesize multiple recorded motions to achieve more complex behaviors</strong>, such as bending, gripping, and walking.</p>\n<video poster="/static/projects/morphio/video-poster/module.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/module.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/module.mp4" type="video/mp4"></source>\n</video>\n<br/>\n<video poster="/static/projects/morphio/video-poster/bear.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/bear.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/bear.mp4" type="video/mp4"></source>\n</video>\n<h1>System Overview</h1>\n<p>The programming workflow with MorphIO is the following:</p>\n<ul>\n<li>\n<p><strong>Step 1:</strong> A user starts manipulating the MorphIO unit.</p>\n</li>\n<li>\n<p><strong>Step 2:</strong> The demonstrated motion is detected and recorded through internal sensors, and the recorded sensor values are stored in the software.</p>\n</li>\n<li>\n<p><strong>Step 3:</strong> Once the user clicks play in the graphical user interface, the pneumatic pump starts supplying air.</p>\n</li>\n<li>\n<p><strong>Step 4:</strong> By controlling the air flow through switching on and off the solenoid valves, the system can control the behavior of the pneumatic actuator as it plays back the recorded motion.</p>\n</li>\n</ul>\n<p>The MorphIO system consists of the following components: A sensor and actuation unit, a sensing and actuation control unit, a microcontroller, software to control these units, and a visual interface for users to control behaviors. Figure illustrates the overview architecture of MorphIO.</p>\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-6-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-6-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-6-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-6-2.png" /></a>\n  </div>\n</div>\n<h1>Entirely Soft Sensing and Actuation Modules</h1>\n<p>Our main contribution is a design and fabrication method for <strong>a conductive sponge sensor</strong> that can be embedded into an air chamber in the pneumatic actuator. The conductive sponge sensor leverages the porous structure to <strong>sense the three-dimensional deformation by measuring the internal resistance value</strong>; when contracted, the resistance value be- tween the top and bottom surfaces drops, and when extended, it increases. In contrast to existing sensing techniques, an elastic sponge allows for a higher degree of freedom in sensing capability (e.g., stretching, bending, and compression) without sacrificing the softness of the interface.</p>\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-2-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-2-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-2-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-2-2.png" /></a>\n  </div>\n</div>\n<video poster="/static/projects/morphio/video-poster/mechanism.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/mechanism.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/mechanism.mp4" type="video/mp4"></source>\n</video>\n<br />\n<p>Moreover, our <strong>modular design</strong> and <strong>graphical interface</strong> allows for easy experiments involving multiple units. For example, the system can visualize multiple recorded sensor values, so that the user can see, customize, and synthesize recorded motion to construct more complex behaviors. These hardware and software designs were informed by our formative study, wherein we interviewed five experienced researchers from the robotics and HCI communities.</p>\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-3-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-3-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-3-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-3-2.png" /></a>\n  </div>\n</div>\n<video poster="/static/projects/morphio/video-poster/unit-x2.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/unit-x2.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/unit-x2.mp4" type="video/mp4"></source>\n</video>\n<video poster="/static/projects/morphio/video-poster/unit-x3.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/unit-x3.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/unit-x3.mp4" type="video/mp4"></source>\n</video>\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-4-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-4-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-4-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-4-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-4-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-4-3.png" /></a>\n  </div>\n</div>\n<h1>Fabrication Process</h1>\n<p>The fabrication process follows three steps: 1) Fabricate an elastic sponge, 2) impregnate into conductive ink, and 3) attach electrodes and wires.\nTo fabricate an elastic sponge, we first prepare 6.0 g of elastomer prepolymer solution and 29.1 g of sodium-chloride, then mix them together by using a planetary centrifugal mixer. The mixed solution is injected into a 3D printed cylindrical mold (16mm diameter, 40mm height). Then we dry the material with an oven at 100 C degrees for one hour. Once dried, we immerse the sponge in water, so that the sodium chloride can melt, leaving a porous structure within the elastomer sponge.</p>\n<video poster="/static/projects/morphio/video-poster/fabrication.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/fabrication.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/fabrication.mp4" type="video/mp4"></source>\n</video>\n<div class="figures ui four column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-3.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-4.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-4.png" /></a>\n  </div>\n</div>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-5.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-5.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-6.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-6.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-7.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-7.png" /></a>\n  </div>\n</div>\n<h1>Applications</h1>\n<p>We demonstrate several possible applications scenarios with MorphIO. 1) Tangible character animation, 2) Animating existing soft objects, 3) Remote manipulation of soft grippers, 4) Locomotion experimentation with soft robots.</p>\n<div class="figures ui four column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-10-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-10-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-10-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-10-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-10-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-10-3.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-10-4.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-10-4.png" /></a>\n  </div>\n</div>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-11-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-11-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-11-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-11-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-11-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-11-3.png" /></a>\n  </div>\n</div>\n<video poster="/static/projects/morphio/video-poster/locomotion.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/locomotion.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/locomotion.mp4" type="video/mp4"></source>\n</video>\n<h1>Evaluation</h1>\n<p>We conducted a user evaluation study to understand the bene- fits and limitations of MorphIO. In this study, we focused on answering the following research questions:</p>\n<ul>\n<li>\n<p><strong>RQ1:</strong> Does MorphIO save time and reduce the number of iterations to program the target behavior, compared to the existing approach?</p>\n</li>\n<li>\n<p><strong>RQ2:</strong> Does MorphIO increase the expressiveness of the physical motion?</p>\n</li>\n</ul>\n<p>To answer these questions, we conducted a controlled experiment where we compared MorphIO (left) with the current programming approach. We chose Arduino IDE (right) as a base condition for the comparison, as this is the most common programming approach identified through our formative study. We provide three basic tasks to construct a program. For each task, the participants were asked to program three differ- ent emotions—happiness, anger, and sadness—of an animated character. We chose these emotions based on Ekman’s basic emotions for communication.</p>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-12-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-12-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-12-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-12-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-12-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-12-3.png" /></a>\n  </div>\n</div>\n<p>The average time of task completion time of MorphIO was 2m 19s, compared to 5m 21s with the control condition. The average number of iterations of MorphIO was 4.4 times, compared to 6.4 with the control condition, which confirms that MorphIO is significantly efficient in terms of task completion time and the number of iterations. When asked about the achievement of the expressions using a 9-point Likert scale, the average score with MorphIO was 6.5, compared to 6.3 with the control condition. We did not find differences between the two conditions. Thus, we conclude the result of our study as follows: <strong>RQ1: Yes, RQ2: No</strong>.</p>\n<p>Based on our post interviews, we discuss the benefits and limitations of our approach: <strong>1) tangible interactions are suitable for sculpting rough motion</strong>, <strong>2) programming allows for fine-tuning more precise adjustments</strong>. Thus, for future research, systems might allow users to quickly make a rough motion, which can automatically be converted into digital parameters so that the user can also precisely control and adjust the motion. The same human- computer cooperation approach can be applied to other design domains: For example, when designing an object, the user can quickly make rough shapes with clay, while letting a machine finish the details. We believe this insight can lead the HCI community to further explore design approaches wherein users and machines cooperate for enhanced interaction design.</p>\n<h1>Future Vision</h1>\n<p>We believe this approach’s potential for lowering the barrier and opening new opportunities for a larger community to begin designing, prototyping, and exploring soft material motion—not by coding on a screen, but by sculpting behaviors in the physical world.\nWe envision the future where people can interactively explore various behaviors through tangible interactions, <strong>just like sculpting behaviors with clay</strong>.</p>\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-14.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-14.png" /></a>\n  </div>\n</div>',dir:"content/output/projects",base:"morphio.json",ext:".json",sourceBase:"morphio.md",sourceExt:".md"}},e3jU:function(e){e.exports=[{date:"2020-01",tag:"lift-tiles",media:"Arduino Blog",title:"Prototype room-scale, shape-changing interfaces with LiftTiles",url:"https://blog.arduino.cc/2020/01/27/prototype-room-scale-shape-changing-interfaces-with-lifttiles/"},{date:"2020-01",tag:"lift-tiles",media:"TechXplore",title:"LiftTiles: Actuator-based Building Blocks for Shape-changing Interfaces",url:"https://techxplore.com/news/2020-01-lifttiles-actuator-based-blocks-shape-changing-interfaces.html"},{date:"2020-01",tag:"shapebots",media:"ITMedia News",title:"A Swarm of Self-transforming Robots to Assist People",url:"https://www.itmedia.co.jp/news/articles/2001/15/news032.html"},{date:"2019-10",tag:"lift-tiles",media:"Hackster.io",title:"LiftTiles Turn Walls and Floors Into Reconfigurable Structures on Demand",url:"https://www.hackster.io/news/lifttiles-turn-walls-and-floors-into-reconfigurable-structures-on-demand-4a226d58bc74"},{date:"2019-10",tag:"lift-tiles",media:"Element 14",title:"Engineers Develop LiftTiles, a Scale Shape-changing Interface",url:"https://www.element14.com/community/community/applications/industrial-automation-space/blog/2019/10/25/engineers-develop-lifttiles-a-scale-shape-changing-interface?CMP=SOM-PRG-TWITTER-BLOG-CATWELL-LIFT-TILES-COMM"},{date:"2019-11",tag:"shapebots",media:"Bouncy",title:"Swarm Robots that can Change Shape to Visualize Data",url:"https://bouncy.news/53532?fbclid=IwAR0jyfBKo8LJ3aiUidDfZUsQqJ5-oSMxRuiZyJju0g_F6A_hi1tOeboPM4E"},{date:"2019-10",tag:"shapebots",media:"Hackster.io",title:"Swarming Robots Can Change Their Configuration to Handle Different Tasks",url:"https://www.hackster.io/news/shapebots-swarming-robots-can-change-their-configuration-to-handle-different-tasks-59a5ae926e1d"},{date:"2019-09",tag:"shapebots",media:"TechXplore",title:"ShapeBots: A Swarm of Shape-shifting Robots that Visually Display Data",url:"https://techxplore.com/news/2019-09-shapebots-swarm-shape-shifting-robots-visually.html"},{date:"2019-09",tag:"shapebots",media:"Hackaday",title:"Tiny Robots that Grow Taller and Wider",url:"https://hackaday.com/2019/10/04/tiny-robots-that-grow-taller-and-wider/"},{date:"2019-09",media:"Robotic Gizmo",title:"ShapeBots: Shape-changing Swarm Robots",url:"https://www.roboticgizmos.com/shapebots/"},{date:"2019-09",tag:"shapebots",media:"Gadgetify",title:"ShapeBots: Shape Changing Swarm Robots",url:"http://www.gadgetify.com/shapebots/"},{date:"2018-10",tag:"dynablock",media:"3DPrint.com",title:"Dynablock: 3D Prints That Assemble and Disassemble in Seconds",url:"https://3dprint.com/227781/3d-prints-that-assemble-in-seconds/"},{date:"2018-10",tag:"dynablock",media:"Hackster.io",title:"The Dynamic 3D Printing That Assembles and Disassembles Objects in Seconds",url:"https://www.hackster.io/news/dynablock-the-dynamic-3d-printing-that-assembles-and-disassembles-objects-in-seconds-a7f7d4bf6cad"},{date:"2018-10",tag:"dynablock",media:"Arduino Blog",title:"Create Shapes Over and Over with the Dynablock 3D Printer",url:"https://blog.arduino.cc/2018/10/22/create-shapes-over-and-over-with-the-dynablock-3d-printer/"},{date:"2018-10",tag:"dynablock",media:"3DRuck.com",title:"Dynablock: Dynamic 3D Printer Creates Objects in Seconds",url:"https://3druck.com/forschung/dynablock-dynamischer-3d-drucker-erstellt-objekte-in-sekunden-2776738/"},{date:"2018-10",tag:"dynablock",media:"World Business Satellite",title:"Repeatable 3D Printer",url:"https://txbiz.tv-tokyo.co.jp/wbs/trend_tamago/post_168589/"},{date:"2018-10",tag:"dynablock",media:"Nikkei Newspaper",title:"Modeling 3D Objects with Magnet-Embedded Blocks",url:"https://active.nikkeibp.co.jp/atclact/active/17/071100318/101600559/"},{date:"2016-06",tag:"atelier",media:"Wired",title:"It’s Not Just Robots: Skilled Jobs Are Going to Meatware",url:"https://www.wired.com/2016/06/its-not-just-robots-skilled-jobs-are-going-to-meatware/"}]},ejaO:function(e){e.exports={id:"tabby",name:"Tabby",description:"Explorable Design for 3D Printing Textures",title:"Tabby: Explorable Design for 3D Printing Textures",authors:["Ryo Suzuki","Koji Yatani","Mark D. Gross","Tom Yeh"],year:2018,booktitle:"The Pacific Conference on Computer Graphics and Applications (Pacific Graphics 2018)",publisher:"The Eurographics Association",pages:"1-4",doi:"https://doi.org/10.2312/pg.20181273",conference:{name:"Pacific Graphics 2018",fullname:"The Pacific Conference on Computer Graphics and Applications (Pacific Graphics 2018)",url:"http://sweb.cityu.edu.hk/pg2018/"},pdf:"pg-2018-tabby.pdf",video:"https://www.youtube.com/watch?v=rRgw8lH74CA",embed:"https://www.youtube.com/embed/rRgw8lH74CA","acm-dl":"https://diglib.eg.org/handle/10.2312/pg20181273",slide:"pg-2018-tabby-slide.pdf",pageCount:4,slideCount:40,bodyContent:'# Abstract\nThis paper presents **Tabby, an interactive and explorable design tool for 3D printing textures**. Tabby allows texture design with direct manipulation in the following workflow: 1) select a target surface, 2) sketch and manipulate a texture with 2D drawings, and then 3) generate 3D printing textures onto an arbitrary curved surface. To enable efficient texture creation, Tabby leverages an auto-completion approach which automates the tedious, repetitive process of applying texture, while allowing flexible customization. Our user evaluation study with seven participants confirms that Tabby can effectively support the design exploration of different patterns for both novice and experienced users.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/tabby/top.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui four column grid">\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-1.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-2.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-3.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-3.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-4.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-4.png" /></a>\n  </div>\n</div>\n\n<div class="figures ui four column grid">\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-5.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-5.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-6.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-6.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-7.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-7.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-8.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-8.png" /></a>\n  </div>\n</div>',bodyHtml:'<h1>Abstract</h1>\n<p>This paper presents <strong>Tabby, an interactive and explorable design tool for 3D printing textures</strong>. Tabby allows texture design with direct manipulation in the following workflow: 1) select a target surface, 2) sketch and manipulate a texture with 2D drawings, and then 3) generate 3D printing textures onto an arbitrary curved surface. To enable efficient texture creation, Tabby leverages an auto-completion approach which automates the tedious, repetitive process of applying texture, while allowing flexible customization. Our user evaluation study with seven participants confirms that Tabby can effectively support the design exploration of different patterns for both novice and experienced users.</p>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/tabby/top.mp4" type="video/mp4"></source>\n</video>\n<div class="figures ui four column grid">\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-1.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-2.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-3.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-3.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-4.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-4.png" /></a>\n  </div>\n</div>\n<div class="figures ui four column grid">\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-5.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-5.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-6.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-6.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-7.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-7.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-8.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-8.png" /></a>\n  </div>\n</div>',dir:"content/output/projects",base:"tabby.json",ext:".json",sourceBase:"tabby.md",sourceExt:".md"}},jEBx:function(e){e.exports={id:"reactile",name:"Reactile",description:"Programming Swarm User Interfaces through Direct Physical Manipulation",title:"Reactile: Programming Swarm User Interfaces through Direct Physical Manipulation",authors:["Ryo Suzuki","Jun Kato","Mark D. Gross","Tom Yeh"],year:2018,booktitle:"In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (CHI '18)",publisher:"ACM, New York, NY, USA",pages:"Paper 199, 13 pages",doi:"https://doi.org/10.1145/3173574.3173773",conference:{name:"CHI 2018",fullname:"The ACM CHI Conference on Human Factors in Computing Systems (CHI 2018)",url:"https://chi2018.acm.org/"},pdf:"chi-2018-reactile.pdf",video:"https://www.youtube.com/watch?v=Gb7brajKCVE",embed:"https://www.youtube.com/embed/Gb7brajKCVE","short-video":"https://www.youtube.com/watch?v=YT7vMJZjohU",slide:"chi-2018-reactile-slide.pdf","acm-dl":"https://dl.acm.org/citation.cfm?id=3173773",github:"https://github.com/ryosuzuki/reactile",pageCount:12,slideCount:56,bodyContent:'# Abstract\n\nWe explore a new approach to programming swarm user interfaces (Swarm UI) by leveraging direct physical manipulation. Existing Swarm UI applications are written using a robot programming framework: users work on a computer screen and think in terms of low-level controls. In contrast, our approach allows programmers to work in physical space by directly manipulating objects and think in terms of high- level interface design. Inspired by current UI programming practices, we introduce a four-step workflow—create elements, abstract attributes, specify behaviors, and propagate changes—for Swarm UI programming. We propose a set of direct physi- cal manipulation techniques to support each step in this work- flow. To demonstrate these concepts, we developed Reac- tile, a Swarm UI programming environment that actuates a swarm of small magnets and displays spatial information of program states using a DLP projector. Two user studies—an in-class survey with 148 students and a lab interview with eight participants—confirm that our approach is intuitive and understandable for programming Swarm UIs.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/reactile/top.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-1-1.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-1-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-1-2.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-1-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-1-3.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-1-3.png" /></a>\n  </div>\n</div>\n\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-2-1.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-2-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-2-2.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-2-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-2-3.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-2-3.png" /></a>\n  </div>\n</div>\n\n\n# Introduction\n\nIn recent years, **Swarm User Interfaces** have emerged as a new paradigm of human-computer interaction. While the idea of coordinated miniature robots was originally proposed in the literature of swarm and micro-robotic systems, HCI researchers have explored the use of these robots as a user interface.\nHowever, this opportunity is currently limited to highly skilled programmers who are proficient in robot programming. For typical programmers inexperienced in robot programming who wish to build a Swarm UI application, it is unclear if the robot programming approach is the most appropriate for UI programming. To design interactive UI applications, pro- grammers often must think in terms of higher-level design for user interaction, whereas robot programming tends to focus on low-level controls of sensors and actuators. Historically, a novel UI platform is adopted only after the advent of an effective programming tool that empowers a larger developer community, and even end-users, to create many applications for the platform; for example, HyperCard for interactive hyper- media, Phidgets for physical interfaces, and Interface Builder for GUI applications. We stipulate that current approaches to programming Swarm UI are too robot-centric to be effec- tive for building rich and interactive applications. Then, what would be a better alternative?\n\n\n# Reactile\n\nThis paper introduces Reactile, a programming environment for Swarm UI applications.\nThe goal of Reactile is to explore a new approach to programming Swarm UI applications. To design an appropriate workflow for Swarm UI programming, we look into existing UI programming paradigm for inspiration. The common workflow of UI programming can be decomposed into four basic steps: create elements, abstract attributes, specify behaviors, and propagate changes. Based on these insights, we propose the following four-step workflow for Swarm UI programming: 1) creates shapes, 2) abstracts shape attributes as variables, 3) specifies data-bindings be- tween dynamic attributes, and 4) the system changes shapes in response to user inputs. With this workflow, a programmer can think in terms of high-level interface and interaction design to build interactive Swarm UI appli- cations, compared to existing, low-level, robot programming approaches.\n\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-3.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-3.png" /></a>\n  </div>\n</div>\n\nThe workflow of Swarm UI programming is inspired by the existing UI programming paradigm. We first review the common workflow of UI programming and decompose it into four basic elements that represent high-level steps. Then we discuss how to apply this workflow to Swarm UI programming.\nAs we see in well-known design patterns for interactive UI ap- plications such as reactive programming paradigm, the Model-View-Controller, and the observer pattern, they share a com- mon workflow consisting of four basic elements: 1) create elements, 2) abstract attributes, 3) specify behaviors, and 4) propagate changes.\n\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-1-4.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-1-4.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-1-5.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-1-5.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-1-6.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-1-6.png" /></a>\n  </div>\n</div>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-2-4.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-2-4.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-2-5.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-2-5.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-2-6.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-2-6.png" /></a>\n  </div>\n</div>\n\n# Implementation\n\nReactile actuates a swarm of small magnetic markers to move on a 2D canvas with electromagnetic force. We designed and fabricated a board of electromagnetic coil arrays (3,200 coils), which covers an 80 cm x 40 cm area. Reactile tracks the marker positions and detects interactions between a user and swarm markers using a standard RGB camera and computer vision techniques. The system displays spatial information using a DLP projector to allow a programmer to see program states in the same physical context.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/reactile/coil.mp4" type="video/mp4"></source>\n</video>\n\n<br />\n\nIn Reactile, a user interface consists of a swarm of passive magnetic markers which move on a 2D workspace driven by electromagnetic forces. Reactile uses a grid of electromagnetic coils to actuate these magnetic markers. Running current through the circuit coils generates a local magnetic field so that each coil can attract a single magnet located within its area. Each coil is aligned with a certain offset in both horizontal and vertical direction with an effective area overlap, which allows the coil to attract the magnet located in the adjacent coil. We design electromagnetic coil arrays to be fabricated with a standard printed circuit board (PCB) manufacturing. This reduces the cost and fabrication complexity, making it easy for the actuation area to scale up.\n\nOur PCB design is a 4-layer board, and each layer contains a set of coils, each of which has an identical circular shape with a 15 mm diameter and a 2.5 mm overlap between nearby coils. Each coil has 15 turns with 0.203 mm (8 mils) spacing between lines, and the distance between centers of two coils is approximately 10 mm, which makes a 10 mm grid for attractive points. The final prototype covers an 80 cm x 40 cm area with 80 x 40 coils by aligning five identical boards horizontally. The fabrication of each board costs approximately $80 USD, including manufacturing of PCB and electronic components.\n\n<br />\n\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/reactile/mechanism.mp4" type="video/mp4"></source>\n</video>',bodyHtml:'<h1>Abstract</h1>\n<p>We explore a new approach to programming swarm user interfaces (Swarm UI) by leveraging direct physical manipulation. Existing Swarm UI applications are written using a robot programming framework: users work on a computer screen and think in terms of low-level controls. In contrast, our approach allows programmers to work in physical space by directly manipulating objects and think in terms of high- level interface design. Inspired by current UI programming practices, we introduce a four-step workflow—create elements, abstract attributes, specify behaviors, and propagate changes—for Swarm UI programming. We propose a set of direct physi- cal manipulation techniques to support each step in this work- flow. To demonstrate these concepts, we developed Reac- tile, a Swarm UI programming environment that actuates a swarm of small magnets and displays spatial information of program states using a DLP projector. Two user studies—an in-class survey with 148 students and a lab interview with eight participants—confirm that our approach is intuitive and understandable for programming Swarm UIs.</p>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/reactile/top.mp4" type="video/mp4"></source>\n</video>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-1-1.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-1-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-1-2.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-1-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-1-3.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-1-3.png" /></a>\n  </div>\n</div>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-2-1.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-2-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-2-2.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-2-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-2-3.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-2-3.png" /></a>\n  </div>\n</div>\n<h1>Introduction</h1>\n<p>In recent years, <strong>Swarm User Interfaces</strong> have emerged as a new paradigm of human-computer interaction. While the idea of coordinated miniature robots was originally proposed in the literature of swarm and micro-robotic systems, HCI researchers have explored the use of these robots as a user interface.\nHowever, this opportunity is currently limited to highly skilled programmers who are proficient in robot programming. For typical programmers inexperienced in robot programming who wish to build a Swarm UI application, it is unclear if the robot programming approach is the most appropriate for UI programming. To design interactive UI applications, pro- grammers often must think in terms of higher-level design for user interaction, whereas robot programming tends to focus on low-level controls of sensors and actuators. Historically, a novel UI platform is adopted only after the advent of an effective programming tool that empowers a larger developer community, and even end-users, to create many applications for the platform; for example, HyperCard for interactive hyper- media, Phidgets for physical interfaces, and Interface Builder for GUI applications. We stipulate that current approaches to programming Swarm UI are too robot-centric to be effec- tive for building rich and interactive applications. Then, what would be a better alternative?</p>\n<h1>Reactile</h1>\n<p>This paper introduces Reactile, a programming environment for Swarm UI applications.\nThe goal of Reactile is to explore a new approach to programming Swarm UI applications. To design an appropriate workflow for Swarm UI programming, we look into existing UI programming paradigm for inspiration. The common workflow of UI programming can be decomposed into four basic steps: create elements, abstract attributes, specify behaviors, and propagate changes. Based on these insights, we propose the following four-step workflow for Swarm UI programming: 1) creates shapes, 2) abstracts shape attributes as variables, 3) specifies data-bindings be- tween dynamic attributes, and 4) the system changes shapes in response to user inputs. With this workflow, a programmer can think in terms of high-level interface and interaction design to build interactive Swarm UI appli- cations, compared to existing, low-level, robot programming approaches.</p>\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-3.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-3.png" /></a>\n  </div>\n</div>\n<p>The workflow of Swarm UI programming is inspired by the existing UI programming paradigm. We first review the common workflow of UI programming and decompose it into four basic elements that represent high-level steps. Then we discuss how to apply this workflow to Swarm UI programming.\nAs we see in well-known design patterns for interactive UI ap- plications such as reactive programming paradigm, the Model-View-Controller, and the observer pattern, they share a com- mon workflow consisting of four basic elements: 1) create elements, 2) abstract attributes, 3) specify behaviors, and 4) propagate changes.</p>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-1-4.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-1-4.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-1-5.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-1-5.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-1-6.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-1-6.png" /></a>\n  </div>\n</div>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-2-4.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-2-4.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-2-5.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-2-5.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-2-6.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-2-6.png" /></a>\n  </div>\n</div>\n<h1>Implementation</h1>\n<p>Reactile actuates a swarm of small magnetic markers to move on a 2D canvas with electromagnetic force. We designed and fabricated a board of electromagnetic coil arrays (3,200 coils), which covers an 80 cm x 40 cm area. Reactile tracks the marker positions and detects interactions between a user and swarm markers using a standard RGB camera and computer vision techniques. The system displays spatial information using a DLP projector to allow a programmer to see program states in the same physical context.</p>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/reactile/coil.mp4" type="video/mp4"></source>\n</video>\n<br />\n<p>In Reactile, a user interface consists of a swarm of passive magnetic markers which move on a 2D workspace driven by electromagnetic forces. Reactile uses a grid of electromagnetic coils to actuate these magnetic markers. Running current through the circuit coils generates a local magnetic field so that each coil can attract a single magnet located within its area. Each coil is aligned with a certain offset in both horizontal and vertical direction with an effective area overlap, which allows the coil to attract the magnet located in the adjacent coil. We design electromagnetic coil arrays to be fabricated with a standard printed circuit board (PCB) manufacturing. This reduces the cost and fabrication complexity, making it easy for the actuation area to scale up.</p>\n<p>Our PCB design is a 4-layer board, and each layer contains a set of coils, each of which has an identical circular shape with a 15 mm diameter and a 2.5 mm overlap between nearby coils. Each coil has 15 turns with 0.203 mm (8 mils) spacing between lines, and the distance between centers of two coils is approximately 10 mm, which makes a 10 mm grid for attractive points. The final prototype covers an 80 cm x 40 cm area with 80 x 40 coils by aligning five identical boards horizontally. The fabrication of each board costs approximately $80 USD, including manufacturing of PCB and electronic components.</p>\n<br />\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/reactile/mechanism.mp4" type="video/mp4"></source>\n</video>',dir:"content/output/projects",base:"reactile.json",ext:".json",sourceBase:"reactile.md",sourceExt:".md"}},lJku:function(e){e.exports={id:"shapebots",name:"ShapeBots",description:"Shape-changing Swarm Robots",title:"ShapeBots: Shape-changing Swarm Robots",authors:["Ryo Suzuki","Clement Zheng","Yasuaki Kakehi","Tom Yeh","Ellen Yi-Luen Do","Mark D. Gross","Daniel Leithinger"],year:2019,booktitle:"In Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology (UIST '19)",publisher:"ACM, New York, NY, USA",pages:"1-13",doi:"https://doi.org/10.1145/3332165.3347911",conference:{name:"UIST 2019",fullname:"The ACM Symposium on User Interface Software and Technology (UIST 2019)",url:"http://uist.acm.org/uist2019"},pdf:"uist-2019-shapebots.pdf",slide:"uist-2019-shapebots-slide.pdf",video:"https://www.youtube.com/watch?v=cwPaof0kKdM",embed:"https://www.youtube.com/embed/cwPaof0kKdM",github:"https://github.com/ryosuzuki/shapebots",poster:"uist-2019-shapebots-poster.pdf",demo:"https://ryosuzuki.github.io/shapebots-simulator/",pageCount:13,slideCount:53,bodyContent:'# Abstract\n\nWe introduce *shape-changing swarm robots*. A swarm of self-transformable robots can both individually and collectively change their configuration to display information, actuate objects, act as tangible controllers, visualize data, and provide physical affordances. ShapeBots is a concept prototype of shape-changing swarm robots. Each robot can change its shape by leveraging small linear actuators that are thin (2.5 cm) and highly extendable (up to 20cm) in both horizontal and vertical directions. The modular design of each actuator enables various shapes and geometries of self-transformation. We illustrate potential application scenarios and discuss how this type of interface opens up possibilities for the future of ubiquitous and distributed shape-changing interfaces.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/top.webm" type="video/webm"></source>\n  <source src="/static/projects/shapebots/video/top.mp4" type="video/mp4"></source>\n</video>\n\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-1-2.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-1-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-1-1.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-1-1.jpg" /></a>\n  </div>\n</div>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-2-1.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-2-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-2-2.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-2-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-2-3.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-2-3.jpg" /></a>\n  </div>\n</div>\n\n\n# Shape-changing Swarm Robots\n\nThis paper introduces **shape-changing swarm robots** for dis- tributed shape-changing interfaces. Shape-changing swarm robots can both **individually** and **collectively** change their shape, so that they can collectively present information, act as controllers, actuate objects, represent data, and provide dynamic physical affordances.\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-3.png" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-3.png" /></a>\n  </div>\n</div>\n\nThis paper specifically focuses on the user interface aspect of such systems, which we refer to shape-changing swarm user interfaces. We identified three core aspects of shape-changing swarm robots: 1) locomotion, 2) self-transformation, and 3) collective behaviors of many individual elements.\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-4.png" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-4.png" /></a>\n  </div>\n</div>\n\n# ShapeBots\n\n**ShapeBots are self-transformable swarm robots** with modular linear actuators. To enable a large deformation capability of tiny swarm robots, we developed a miniature reel-based linear actuator that is thin (2.5 cm) and fits into the small footprint (3 cm x 3 cm), while able to extend up to 20 cm in both horizontal and vertical directions.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/unit.webm" type="video/webm"></source>\n  <source src="/static/projects/shapebots/video/unit.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-5-1.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-5-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-5-2.png" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-5-2.png" /></a>\n  </div>\n</div>\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-6-1.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-6-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-6-2.png" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-6-2.png" /></a>\n  </div>\n</div>\n\nThe modular design of each linear actuator unit enables the construction of various shapes and geometries of individual shape transformation (e.g., horizontal lines, vertical lines, curved lines, 2D area expan- sion, and 3D volumetric change). Based on these capabilities, we demonstrate application scenarios showing how a swarm of distributed self-transformable robots can support everyday interactions.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/transformation.webm" type="video/webm"></source>\n  <source src="/static/projects/shapebots/video/transformation.mp4" type="video/mp4"></source>\n</video>\n\n\n# Tracking and Control\n\nTo track the position and orientation of the swarm robots, we used computer vision and a fiducial marker attached to the bottom of the robot. We used the ArUco fiducial marker printed on a sheet of paper and taped to the bottom of the robot. Our prototype used a 1.5 cm x 1.5 cm size marker with a 4 x 4 grid pattern, which can provide up to 50 unique patterns. For tracking software, we used the OpenCV library and ArUco python module. It can track the position of the markers at 60 frames per second.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/tracking.webm" type="video/webm"></source>\n  <source src="/static/projects/shapebots/video/tracking.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-7-1.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-7-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-7-2.png" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-7-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-7-3.png" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-7-3.png" /></a>\n  </div>\n</div>\n\nTo enable the user to easily specify a target shape, we created a web-based interface where users draw a shape or upload an SVG image (Figure 10). The user draws a set of lines, then the main computer calculates target positions, orientations, and actuator lengths to start sending commands.\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-8.png" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-8.png" /></a>\n  </div>\n</div>\n\n\nWe can use the same mechanism to track user input. The system supports four different types of user interaction that our system supports: place, move, orient, and pick-up.\nThe system recognizes as user inputs movement or rotation of a marker that it did not generate.\n\n\n\n# Applications: Robots as Dynamic Physical Media\nWe explore potential application scenarios for the future of human-robot interactions.\nOne interesting application area is to use these **robots as dynamic physical media**, such as showing **data visualization in the physical world**.\nFor example, ShapeBots on the USA map physicalize map data; each robot changes its height to show the population of the state it is on. Users can interact with the dataset by placing a new robot or moving a robot to a different state, and the robots update their physical forms to represent the respective population.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/dataphys.webm" type="video/webm"></source>\n  <source src="/static/projects/shapebots/video/dataphys.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-9-1.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-9-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-9-2.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-9-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-9-3.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-9-3.jpg" /></a>\n  </div>\n</div>\n\nSimilarly, ShapeBots can provide a physical preview of a CAD design. ShapeBots physicalizes the actual size of the box. The design and physical rendering are tightly coupled; as the user changes the height of the box in CAD software, the ShapeBots change heights accordingly. The user can change the parameters of the design by moving robots in the physical space, and these changes are reflected in the CAD design.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/cad.webm" type="video/webm"></source>\n  <source src="/static/projects/shapebots/video/cad.mp4" type="video/mp4"></source>\n</video>\n\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-9-4.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-9-4.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-9-5.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-9-5.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-9-6.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-9-6.jpg" /></a>\n  </div>\n</div>\n\n# Applications: Robots as Ambient Assistants\n\nAnother practical aspect of ShapeBots is the ability to actuate objects and act as physical constraints. As an example, the video shows two robots extending their linear actuators to wipe debris off a table, clearing a workspace for the user.\nIn these scenarios, these robots can help as an **ambient assistant for everyday life**.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/cleaning.webm" type="video/webm"></source>\n  <source src="/static/projects/shapebots/video/cleaning.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-11-1.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-11-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-11-2.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-11-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-11-3.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-11-3.jpg" /></a>\n  </div>\n</div>\n\nBy leveraging the capability of locomotion and height change of each robot, ShapeBots can create a dynamic fence to hide or encompass existing objects for affordances. For example, when the user pours hot coffee into a cup, the robots surround the cup and change their heights to create a vertical fence. The vertical fence visually and physically provides the affordance to indicate that the coffee is too hot and not ready to drink. Once it is ready, the robots start dispersing and allow the user to grab it. These scenarios illustrate how the distributed shape-changing robots can provide a new type of affordance, which we call **distributed dynamic physical affordances**.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/affordance.webm" type="video/webm"></source>\n  <source src="/static/projects/shapebots/video/affordance.mp4" type="video/mp4"></source>\n</video>\n\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-10-1.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-10-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-10-2.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-10-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-10-3.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-10-3.jpg" /></a>\n  </div>\n</div>\n\nShapeBots can also act as an interactive physical display. The following figures show how ShapeBots can render different shapes.\nWe highlight the advantage of ShapeBots for rendering contours compared to non self-transformable swarm robots. Using a software simulation, we demonstrate how ShapeBots renders an SVG input at different swarm sizes. You can also play with the [**explorable online simulator**](https://ryosuzuki.github.io/shapebots-simulator/) to see how these robots can render the shape ↓\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/explorable.webm" type="video/webm"></source>\n  <source src="/static/projects/shapebots/video/explorable.mp4" type="video/mp4"></source>\n</video>\n\n\n\n# Future Work\n\nShapeBots is just a single example of shape-changing swarm robots.\nThere is a broader design space of shape- changing swarm user interfaces.\nAs future work, we are interested in exploring the different aspct of shape-changing swarm robots.\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-12.png" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-12.png" /></a>\n  </div>\n</div>\n\n\x3c!--\n# Appendix\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/fabrication.mp4" type="video/mp4"></source>\n</video>\n\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/explorable.mp4" type="video/mp4"></source>\n</video>\n --\x3e',bodyHtml:'<h1>Abstract</h1>\n<p>We introduce <em>shape-changing swarm robots</em>. A swarm of self-transformable robots can both individually and collectively change their configuration to display information, actuate objects, act as tangible controllers, visualize data, and provide physical affordances. ShapeBots is a concept prototype of shape-changing swarm robots. Each robot can change its shape by leveraging small linear actuators that are thin (2.5 cm) and highly extendable (up to 20cm) in both horizontal and vertical directions. The modular design of each actuator enables various shapes and geometries of self-transformation. We illustrate potential application scenarios and discuss how this type of interface opens up possibilities for the future of ubiquitous and distributed shape-changing interfaces.</p>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/top.webm" type="video/webm"></source>\n  <source src="/static/projects/shapebots/video/top.mp4" type="video/mp4"></source>\n</video>\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-1-2.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-1-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-1-1.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-1-1.jpg" /></a>\n  </div>\n</div>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-2-1.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-2-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-2-2.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-2-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-2-3.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-2-3.jpg" /></a>\n  </div>\n</div>\n<h1>Shape-changing Swarm Robots</h1>\n<p>This paper introduces <strong>shape-changing swarm robots</strong> for dis- tributed shape-changing interfaces. Shape-changing swarm robots can both <strong>individually</strong> and <strong>collectively</strong> change their shape, so that they can collectively present information, act as controllers, actuate objects, represent data, and provide dynamic physical affordances.</p>\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-3.png" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-3.png" /></a>\n  </div>\n</div>\n<p>This paper specifically focuses on the user interface aspect of such systems, which we refer to shape-changing swarm user interfaces. We identified three core aspects of shape-changing swarm robots: 1) locomotion, 2) self-transformation, and 3) collective behaviors of many individual elements.</p>\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-4.png" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-4.png" /></a>\n  </div>\n</div>\n<h1>ShapeBots</h1>\n<p><strong>ShapeBots are self-transformable swarm robots</strong> with modular linear actuators. To enable a large deformation capability of tiny swarm robots, we developed a miniature reel-based linear actuator that is thin (2.5 cm) and fits into the small footprint (3 cm x 3 cm), while able to extend up to 20 cm in both horizontal and vertical directions.</p>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/unit.webm" type="video/webm"></source>\n  <source src="/static/projects/shapebots/video/unit.mp4" type="video/mp4"></source>\n</video>\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-5-1.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-5-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-5-2.png" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-5-2.png" /></a>\n  </div>\n</div>\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-6-1.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-6-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-6-2.png" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-6-2.png" /></a>\n  </div>\n</div>\n<p>The modular design of each linear actuator unit enables the construction of various shapes and geometries of individual shape transformation (e.g., horizontal lines, vertical lines, curved lines, 2D area expan- sion, and 3D volumetric change). Based on these capabilities, we demonstrate application scenarios showing how a swarm of distributed self-transformable robots can support everyday interactions.</p>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/transformation.webm" type="video/webm"></source>\n  <source src="/static/projects/shapebots/video/transformation.mp4" type="video/mp4"></source>\n</video>\n<h1>Tracking and Control</h1>\n<p>To track the position and orientation of the swarm robots, we used computer vision and a fiducial marker attached to the bottom of the robot. We used the ArUco fiducial marker printed on a sheet of paper and taped to the bottom of the robot. Our prototype used a 1.5 cm x 1.5 cm size marker with a 4 x 4 grid pattern, which can provide up to 50 unique patterns. For tracking software, we used the OpenCV library and ArUco python module. It can track the position of the markers at 60 frames per second.</p>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/tracking.webm" type="video/webm"></source>\n  <source src="/static/projects/shapebots/video/tracking.mp4" type="video/mp4"></source>\n</video>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-7-1.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-7-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-7-2.png" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-7-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-7-3.png" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-7-3.png" /></a>\n  </div>\n</div>\n<p>To enable the user to easily specify a target shape, we created a web-based interface where users draw a shape or upload an SVG image (Figure 10). The user draws a set of lines, then the main computer calculates target positions, orientations, and actuator lengths to start sending commands.</p>\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-8.png" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-8.png" /></a>\n  </div>\n</div>\n<p>We can use the same mechanism to track user input. The system supports four different types of user interaction that our system supports: place, move, orient, and pick-up.\nThe system recognizes as user inputs movement or rotation of a marker that it did not generate.</p>\n<h1>Applications: Robots as Dynamic Physical Media</h1>\n<p>We explore potential application scenarios for the future of human-robot interactions.\nOne interesting application area is to use these <strong>robots as dynamic physical media</strong>, such as showing <strong>data visualization in the physical world</strong>.\nFor example, ShapeBots on the USA map physicalize map data; each robot changes its height to show the population of the state it is on. Users can interact with the dataset by placing a new robot or moving a robot to a different state, and the robots update their physical forms to represent the respective population.</p>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/dataphys.webm" type="video/webm"></source>\n  <source src="/static/projects/shapebots/video/dataphys.mp4" type="video/mp4"></source>\n</video>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-9-1.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-9-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-9-2.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-9-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-9-3.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-9-3.jpg" /></a>\n  </div>\n</div>\n<p>Similarly, ShapeBots can provide a physical preview of a CAD design. ShapeBots physicalizes the actual size of the box. The design and physical rendering are tightly coupled; as the user changes the height of the box in CAD software, the ShapeBots change heights accordingly. The user can change the parameters of the design by moving robots in the physical space, and these changes are reflected in the CAD design.</p>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/cad.webm" type="video/webm"></source>\n  <source src="/static/projects/shapebots/video/cad.mp4" type="video/mp4"></source>\n</video>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-9-4.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-9-4.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-9-5.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-9-5.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-9-6.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-9-6.jpg" /></a>\n  </div>\n</div>\n<h1>Applications: Robots as Ambient Assistants</h1>\n<p>Another practical aspect of ShapeBots is the ability to actuate objects and act as physical constraints. As an example, the video shows two robots extending their linear actuators to wipe debris off a table, clearing a workspace for the user.\nIn these scenarios, these robots can help as an <strong>ambient assistant for everyday life</strong>.</p>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/cleaning.webm" type="video/webm"></source>\n  <source src="/static/projects/shapebots/video/cleaning.mp4" type="video/mp4"></source>\n</video>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-11-1.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-11-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-11-2.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-11-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-11-3.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-11-3.jpg" /></a>\n  </div>\n</div>\n<p>By leveraging the capability of locomotion and height change of each robot, ShapeBots can create a dynamic fence to hide or encompass existing objects for affordances. For example, when the user pours hot coffee into a cup, the robots surround the cup and change their heights to create a vertical fence. The vertical fence visually and physically provides the affordance to indicate that the coffee is too hot and not ready to drink. Once it is ready, the robots start dispersing and allow the user to grab it. These scenarios illustrate how the distributed shape-changing robots can provide a new type of affordance, which we call <strong>distributed dynamic physical affordances</strong>.</p>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/affordance.webm" type="video/webm"></source>\n  <source src="/static/projects/shapebots/video/affordance.mp4" type="video/mp4"></source>\n</video>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-10-1.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-10-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-10-2.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-10-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-10-3.jpg" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-10-3.jpg" /></a>\n  </div>\n</div>\n<p>ShapeBots can also act as an interactive physical display. The following figures show how ShapeBots can render different shapes.\nWe highlight the advantage of ShapeBots for rendering contours compared to non self-transformable swarm robots. Using a software simulation, we demonstrate how ShapeBots renders an SVG input at different swarm sizes. You can also play with the <a href="https://ryosuzuki.github.io/shapebots-simulator/"><strong>explorable online simulator</strong></a> to see how these robots can render the shape ↓</p>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/explorable.webm" type="video/webm"></source>\n  <source src="/static/projects/shapebots/video/explorable.mp4" type="video/mp4"></source>\n</video>\n<h1>Future Work</h1>\n<p>ShapeBots is just a single example of shape-changing swarm robots.\nThere is a broader design space of shape- changing swarm user interfaces.\nAs future work, we are interested in exploring the different aspct of shape-changing swarm robots.</p>\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/shapebots/figure-12.png" data-lightbox="lightbox"><img src="/static/projects/shapebots/figure-12.png" /></a>\n  </div>\n</div>\n\x3c!--\n# Appendix\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/fabrication.mp4" type="video/mp4"></source>\n</video>\n\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/shapebots/video/explorable.mp4" type="video/mp4"></source>\n</video>\n --\x3e',dir:"content/output/projects",base:"shapebots.json",ext:".json",sourceBase:"shapebots.md",sourceExt:".md"}},mRot:function(e){e.exports={id:"refazer",name:"Refazer",description:"Learning Syntactic Program Transformations from Examples",title:"Learning Syntactic Program Transformations from Examples",authors:["Reudismam Rolim","Gustavo Soares","Loris D'Antoni","Oleksandr Polozov","Sumit Gulwani","Rohit Gheyi","Ryo Suzuki","Björn Hartmann"],yera:2017,booktitle:"In Proceedings of the 39th International Conference on Software Engineering (ICSE '17)",publisher:"IEEE Press, Piscataway, NJ, USA",pages:"404-415",conference:{name:"ICSE 2017",fullname:"The International Conference on Software Engineering (ICSE 2017)",url:"http://icse2017.gatech.edu/"},pdf:"icse-2017-refazer.pdf","acm-dl":"http://dl.acm.org/citation.cfm?id=3097417",arxiv:"https://arxiv.org/abs/1608.09000",pageCount:12,slideCount:0,image:"refazer.png",bodyContent:"",bodyHtml:"",dir:"content/output/projects",base:"refazer.json",ext:".json",sourceBase:"refazer.md",sourceExt:".md"}},nWAr:function(e){e.exports={id:"atelier",name:"Atelier",description:"Repurposing Expert Crowdsourcing Tasks as Micro-internships",title:"Atelier: Repurposing Expert Crowdsourcing Tasks as Micro-internships",authors:["Ryo Suzuki","Niloufar Salehi","Michelle S. Lam","Juan C. Marroquin","Michael S. Bernstein"],year:2016,booktitle:"In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI '16)",publisher:"ACM, New York, NY, USA",pages:"2645-2656",conference:{name:"CHI 2016",fullname:"The ACM CHI Conference on Human Factors in Computing Systems (CHI 2016)",url:"https://chi2016.acm.org/wp/"},pdf:"chi-2016-atelier.pdf",video:"https://www.youtube.com/watch?v=tBojZejtFQo",embed:"https://www.youtube.com/embed/tBojZejtFQo",slide:"chi-2016-atelier-slide.pdf","acm-dl":"http://dl.acm.org/citation.cfm?id=2858121",arxiv:"https://arxiv.org/abs/1602.06634",pageCount:12,slideCount:56,image:"atelier.jpg",bodyContent:"",bodyHtml:"",dir:"content/output/projects",base:"atelier.json",ext:".json",sourceBase:"atelier.md",sourceExt:".md"}},o0EK:function(e,t,i){var a={"./atelier.json":"nWAr","./dynablock.json":"GbvX","./flux-marker.json":"CTYI","./lift-tiles.json":"3RXq","./mixed-initiative.json":"PSd4","./morphio.json":"X0/d","./pep.json":"W/HP","./phd-thesis.json":"IMSK","./reactile.json":"jEBx","./refazer.json":"mRot","./roomshift.json":"9gtZ","./shapebots.json":"lJku","./tabby.json":"ejaO","./trace-diff.json":"Jg5j"};function s(e){var t=o(e);return i(t)}function o(e){var t=a[e];if(!(t+1)){var i=new Error("Cannot find module '"+e+"'");throw i.code="MODULE_NOT_FOUND",i}return t}s.keys=function(){return Object.keys(a)},s.resolve=o,e.exports=s,s.id="o0EK"},qg4i:function(e){e.exports=[{author:"Ryo Suzuki,",title:"Collective Shape-changing Interfaces",pdf:"uist-2019-collective.pdf",booktitle:"Adjunct Proceedings of the 32nd Annual ACM Symposium on User Interface Software & Technology",series:"UIST '19 Doctoral Consortium",year:2019,isbn:"978-1-4503-4656-6",location:"New Orleans, Louisiana, USA",pages:"2951--2958",numpages:2,doi:"xxx/xxx",acmid:3053187,publisher:"ACM",address:"New York, NY, USA",keywords:"shape-changing interfaces"},{author:"Ryo Suzuki, Ryosuke Nakayama, Dan Liu, Yasuaki Kakehi, Mark D. Gross, and Daniel Leithinger,",title:"LiftTiles: Modular and Reconfigurable Room-scale Shape Displays through Retractable Inflatable Actuators",pdf:"uist-2019-lift-tiles.pdf",poster:"uist-2019-lift-tiles-poster.pdf",booktitle:"Adjunct Proceedings of the 32nd Annual ACM Symposium on User Interface Software & Technology",series:"UIST '19 Adjunct",year:2019,isbn:"978-1-4503-4656-6",location:"New Orleans, Louisiana, USA",pages:"2951--2958",numpages:2,doi:"xxx/xxx",acmid:3053187,publisher:"ACM",address:"New York, NY, USA",keywords:"inflatables, shape-changing interfaces,large-scale interactions"},{author:"Ryo Suzuki, Gustavo Soares, Elena Glassman, Andrew Head, Loris D'Antoni, and Bjoern Hartmann,",title:"Exploring the Design Space of Automatically Synthesized Hints for Introductory Programming Assignments",pdf:"chi-2017-lbw.pdf",poster:"chi-2017-lbw-poster.pdf",booktitle:"Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems",series:"CHI EA '17",year:2017,isbn:"978-1-4503-4656-6",location:"Denver, Colorado, USA",pages:"2951--2958",numpages:8,url:"http://doi.acm.org/10.1145/3027063.3053187",doi:"10.1145/3027063.3053187",acmid:3053187,publisher:"ACM",address:"New York, NY, USA",keywords:"automated feedback, program synthesis, programming education"},{author:"Stanford Crowd Research Collective",title:"Daemo: A Self-Governed Crowdsourcing Marketplace",pdf:"uist-2015-daemo.pdf",booktitle:"Adjunct Proceedings of the 28th Annual ACM Symposium on User Interface Software & Technology",series:"UIST '15 Adjunct",year:2015,isbn:"978-1-4503-3780-9",location:"Daegu, Kyungpook, Republic of Korea",pages:"101--102",numpages:2,url:"http://doi.acm.org/10.1145/2815585.2815739",doi:"10.1145/2815585.2815739",acmid:2815739,publisher:"ACM",address:"New York, NY, USA",keywords:"crowd research, crowd work., crowdsourcing"},{author:"Ryo Suzuki,",title:"Toward a Community Enhanced Programming Education",pdf:"chi-2015-workshop.pdf",slide:"chi-2015-workshop-slide.pdf",booktitle:"ACM CHI 2015 Symposium on Emerging Japanese HCI Research Collection",series:"CHI '15 Workshop",year:2015,location:"Seoul, Korea",publisher:"ACM",address:"New York, NY, USA"},{author:"Ryo Suzuki,",title:"Interactive and Collaborative Source Code Annotation",pdf:"icse-2015-cumiki.pdf",poster:"icse-2015-cumiki-poster.pdf",booktitle:"Proceedings of the 37th International Conference on Software Engineering - Volume 2",series:"ICSE '15 Poster",year:2015,location:"Florence, Italy",pages:"799--800",numpages:2,url:"http://dl.acm.org/citation.cfm?id=2819009.2819173",acmid:2819173,publisher:"IEEE Press",address:"Piscataway, NJ, USA"},{author:"Ryo Suzuki,",title:"Network Thresholds and Multiple Equilibria in the Diffusion of Content-Based Platforms",pdf:"wine-2014-network.pdf",poster:"wine-2014-network-poster.pdf",booktitle:"International Conference on Web and Internet Economics",series:"WINE '14 Poster",year:2014,location:"Beijing, China",publisher:"Springer",address:"New York, NY, USA"}]},vlRD:function(e,t,i){(window.__NEXT_P=window.__NEXT_P||[]).push(["/",function(){var e=i("RNiq");return{page:e.default||e}}])},yC4j:function(e){e.exports={bodyContent:"## Funding\n\n**Innovation Research Funding** - Ministry of Internal Affairs and Communications in Japan, 2020\n(*Title: Adaptive Physical Environments with Distributed Swarm Robots. , $30,000*)\n\n**Adobe Gift Funding**, 2019 (*$5,000*)\n\n**ACT-I Funding for Young Scholars** - JST, 2018 (*Title: Dynamic Physical Interfaces, $30,000 and Mentorship Opportunity - My mentor: Takeo Igarashi*)\n\n**Emerging Research Funding for AI and Interdisciplinary Research** - Leave a Nest Foundation, 2018 (*Title: Programmable Architecture with Soft Inflatable Actuator, $5,000*\n\n**KAKENHI Grants-in-Aid for Scientific Research** - JSPS, 2013-2015 (*Title: Network-based Diffusion Analysis for Online Community, $40,000*)\n\n## Fellowship\n\n**CU Boulder Travel Grant**, 2015-2020 (*$500-$1,200 for each conference travel*)\n\n**Nakajima Foundation Scholarship**, 2015-2020 (*$120,000 stipend for 5 years and 2 years tuition coverage*)\n\n**JSPS Research Fellow DC1**, 2013 (*$72,000 stipend for 2 years*)\n\n**JASSO Fellow**, 2013 (*Total Exemption for Outstanding Students - $20,000 stipend\nfor 2 years*)\n\n**Tohso Fellowship**, 2010 (*$3,600*)\n\n[//]: # (JBMC Microsoft Award, 2013)\n\n[//]: # (Tech Crunch Tokyo 2013 Finalist, 2013)\n\n[//]: # (1st Prize Winner of University of Tokyo Entrepreneur Dojo, 2012)\n\n[//]: # (Honer of MOVIDA School founded by Taizo Son, 2012)",bodyHtml:"<h2>Funding</h2>\n<p><strong>Innovation Research Funding</strong> - Ministry of Internal Affairs and Communications in Japan, 2020\n(<em>Title: Adaptive Physical Environments with Distributed Swarm Robots. , $30,000</em>)</p>\n<p><strong>Adobe Gift Funding</strong>, 2019 (<em>$5,000</em>)</p>\n<p><strong>ACT-I Funding for Young Scholars</strong> - JST, 2018 (<em>Title: Dynamic Physical Interfaces, $30,000 and Mentorship Opportunity - My mentor: Takeo Igarashi</em>)</p>\n<p><strong>Emerging Research Funding for AI and Interdisciplinary Research</strong> - Leave a Nest Foundation, 2018 (<em>Title: Programmable Architecture with Soft Inflatable Actuator, $5,000</em></p>\n<p><strong>KAKENHI Grants-in-Aid for Scientific Research</strong> - JSPS, 2013-2015 (<em>Title: Network-based Diffusion Analysis for Online Community, $40,000</em>)</p>\n<h2>Fellowship</h2>\n<p><strong>CU Boulder Travel Grant</strong>, 2015-2020 (<em>$500-$1,200 for each conference travel</em>)</p>\n<p><strong>Nakajima Foundation Scholarship</strong>, 2015-2020 (<em>$120,000 stipend for 5 years and 2 years tuition coverage</em>)</p>\n<p><strong>JSPS Research Fellow DC1</strong>, 2013 (<em>$72,000 stipend for 2 years</em>)</p>\n<p><strong>JASSO Fellow</strong>, 2013 (<em>Total Exemption for Outstanding Students - $20,000 stipend\nfor 2 years</em>)</p>\n<p><strong>Tohso Fellowship</strong>, 2010 (<em>$3,600</em>)</p>\n",title:"Funding",dir:"content/output",base:"fellowship.json",ext:".json",sourceBase:"fellowship.md",sourceExt:".md"}}},[["vlRD","5d41","9da1","ad9d"]]]);