(window.webpackJsonp=window.webpackJsonp||[]).push([["8d0d"],{"6T/A":function(e){e.exports={}},CTYI:function(e){e.exports={id:"flux-marker",name:"FluxMarker",description:"Enhancing Tactile Graphics with Dynamic Tactile Markers for Blind People",title:"FluxMarker: Enhancing Tactile Graphics with Dynamic Tactile Markers",authors:["Ryo Suzuki","Abigale Stangl","Mark D. Gross","Tom Yeh"],year:2017,booktitle:"In Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS '17)",publisher:"ACM, New York, NY, USA",pages:"190-199",doi:"https://doi.org/10.1145/3132525.3132548",conference:{name:"ASSETS 2017",fullname:"The International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS 2017)",url:"https://assets17.sigaccess.org/"},pdf:"assets-2017-fluxmarker.pdf",video:"https://www.youtube.com/watch?v=VbwIZ9V6i_g",embed:"https://www.youtube.com/embed/VbwIZ9V6i_g",slide:"assets-2017-fluxmarker-slide.pdf","acm-dl":"https://dl.acm.org/citation.cfm?id=3132548",arxiv:"https://arxiv.org/abs/1708.03783",pageCount:10,slideCount:53,bodyContent:"# Abstract\n\nFor people with visual impairments, tactile graphics are an impor- tant means to learn and explore information. However, raised line tactile graphics created with traditional materials such as emboss- ing are static. While available refreshable displays can dynamically change the content, they are still too expensive for many users, and are limited in size. These factors limit wide-spread adoption and the representation of large graphics or data sets. In this paper, we present FluxMaker, an inexpensive scalable system that renders dy- namic information on top of static tactile graphics with movable tactile markers. These dynamic tactile markers can be easily re- configured and used to annotate static raised line tactile graphics, including maps, graphs, and diagrams. We developed a hardware prototype that actuates magnetic tactile markers driven by low-cost and scalable electromagnetic coil arrays, which can be fabricated with standard printed circuit board manufacturing. We evaluate our prototype with six participants with visual impairments and found positive results across four application areas: location finding or navigating on tactile maps, data analysis, and physicalization, fea- ture identification for tactile graphics, and drawing support. The user study confirms advantages in application domains such as ed- ucation and data exploration.",bodyHtml:"<h1>Abstract</h1>\n<p>For people with visual impairments, tactile graphics are an impor- tant means to learn and explore information. However, raised line tactile graphics created with traditional materials such as emboss- ing are static. While available refreshable displays can dynamically change the content, they are still too expensive for many users, and are limited in size. These factors limit wide-spread adoption and the representation of large graphics or data sets. In this paper, we present FluxMaker, an inexpensive scalable system that renders dy- namic information on top of static tactile graphics with movable tactile markers. These dynamic tactile markers can be easily re- configured and used to annotate static raised line tactile graphics, including maps, graphs, and diagrams. We developed a hardware prototype that actuates magnetic tactile markers driven by low-cost and scalable electromagnetic coil arrays, which can be fabricated with standard printed circuit board manufacturing. We evaluate our prototype with six participants with visual impairments and found positive results across four application areas: location finding or navigating on tactile maps, data analysis, and physicalization, fea- ture identification for tactile graphics, and drawing support. The user study confirms advantages in application domains such as ed- ucation and data exploration.</p>\n",dir:"content/output/projects",base:"flux-marker.json",ext:".json",sourceBase:"flux-marker.md",sourceExt:".md"}},D85t:function(e,i,t){(window.__NEXT_P=window.__NEXT_P||[]).push(["/projects",function(){var e=t("RHEb");return{page:e.default||e}}])},GbvX:function(e){e.exports={id:"dynablock",name:"Dynablock",description:"Dynamic 3D Printing for Instant and Reconstructable Shape Formation",title:"Dynablock: Dynamic 3D Printing for Instant and Reconstructable Shape Formation",authors:["Ryo Suzuki","Junichi Yamaoka","Daniel Leithinger","Tom Yeh","Mark D. Gross","Yoshihiro Kawahara","Yasuaki Kakehi"],year:2018,booktitle:"In Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology (UIST '18)",publisher:"ACM, New York, NY, USA",pages:"99-111",doi:"https://doi.org/10.1145/3242587.3242659",conference:{name:"UIST 2018",fullname:"The ACM Symposium on User Interface Software and Technology (UIST 2018)",url:"http://uist.acm.org/uist2018"},pdf:"uist-2018-dynablock.pdf",video:"https://www.youtube.com/watch?v=7nPlr3O9xu8",embed:"https://www.youtube.com/embed/7nPlr3O9xu8","short-video":"https://www.youtube.com/watch?v=92eGI-gYYc4",slide:"uist-2018-dynablock-slide.pdf","acm-dl":"https://dl.acm.org/citation.cfm?id=3242659",talk:"https://www.youtube.com/watch?v=R3FRUtOIiCQ",pageCount:12,slideCount:52,bodyContent:'\x3c!--\nLinks:\n[**[PDF](http://ryosuzuki.org/publications/uist-2018-dynablock.pdf)**]\n[**[ACM DL](https://dl.acm.org/citation.cfm?id=3242659)**]\n[**[Video](https://www.youtube.com/watch?v=7nPlr3O9xu8)**]\n[**[Slide](http://ryosuzuki.org/publications/uist-2018-dynablock-slide.pdf)**]\n[**[Talk](https://www.youtube.com/watch?v=R3FRUtOIiCQ)**]\n --\x3e\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/dynablock/webm/top.webm" type="video/webm"></source>\n  <source src="/static/projects/dynablock/video/top.mp4" type="video/mp4"></source>\n</video>\n\n\n# Abstract\n\nThis paper introduces Dynamic 3D Printing, a fast and re- constructable shape formation system. Dynamic 3D Printing assembles an arbitrary three-dimensional shape from a large number of small physical elements. It can also disassemble the shape back to elements and reconstruct a new shape. Dynamic 3D Printing combines the capabilities of 3D printers and shape displays: Like conventional 3D printing, it can generate arbi- trary and graspable three-dimensional shapes, while allowing shapes to be rapidly formed and reformed as in a shape display. To demonstrate the idea, we describe the design and imple- mentation of Dynablock, a working prototype of a dynamic 3D printer. Dynablock can form a three-dimensional shape in seconds by assembling 3,000 9 mm blocks, leveraging a 24 x 16 pin-based shape display as a parallel assembler. Dynamic 3D printing is a step toward achieving our long term vision in which 3D printing becomes an interactive medium, rather than the means for fabrication that it is today. In this paper we explore possibilities for this vision by illustrating application scenarios that are difficult to achieve with conventional 3D printing or shape display systems.\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-1-1.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-1-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-1-2.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-1-2.jpg" /></a>\n  </div>\n</div>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-2-1.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-2-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-2-2.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-2-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-2-3.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-2-3.jpg" /></a>\n  </div>\n</div>\n\n# Dynamic 3D Printing\n\nWhat if 3D printers could form a physical object in seconds? What if the object, once it is no longer needed, could quickly and easily be disassembled and reconstructed as a new object? Today’s 3D printers take hours to print objects, and output a single static object. However, we envision a future in which 3D printing could instantly create objects from reusable and reconstructable materials.\n\nThis paper develops this vision by proposing Dynamic 3D Printing, a class of systems for rapid and reconstructable shape formation. Dynamic 3D Printing assembles digital material elements to form recon- structable physical objects. Each element can be connected with and disconnected from neighboring elements, and ele- ments can be formed into an arbitrary three-dimensional phys- ical object. Dynamic 3D printing differs from existing 3D printing in speed and reconstructability: Dynamic 3D printing forms shapes in seconds, rather than minutes. In addition, because individual elements can be disconnected, the shape can be easily disassembled into its basic building blocks once the object is no longer needed.\n\nWe define Dynamic 3D Printing as a class of systems that have the following properties:\n\n- Immediate: The system can form a physical shape in sec- onds.\n\n- Reconstructable: Rendered shapes can be disassembled and reconstructed by hand or with the system, and the blocks are reusable.\n\n- Arbitrary Shapes: It can create arbitrary three dimensional shapes.\n\n- Graspable: The output shapes and structure are graspable and solid.\n\n# Parallel Assembler\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-3-1.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-3-1.png" /></a>\n  </div>\n</div>\n\nDynamic 3D printing deploys a large number of small dis- crete material elements, which are assembled to form arbitrary shaped macro-scale objects. Individual elements are passive, which requires an external actuator to perform the assembly. As illustrated in the above Figure, the assembler consists of an N x N grid of motorized pins and linear actuators. The elements, which are the same size as the pins, are stacked on top of the pins (Figure 3 A). When stacked, the elements are connected in vertical direction, while discon- nected with nearby elements in horizontal direction. Similar to existing pin-based shape displays, the assembler can incrementally generate 2.5D shapes by individually moving pins to push elements to the surface.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/dynablock/webm/mechanism.webm" type="video/webm"></source>\n  <source src="/static/projects/dynablock/video/mechanism.mp4" type="video/mp4"></source>\n</video>\n\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-4-1.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-4-1.png" /></a>\n  </div>\n  <p class="column">\n    This paper develops this vision by proposing Dynamic 3D Printing, a class of systems for rapid and reconstructable shape formation. Dynamic 3D Printing assembles digital material elements to form recon- structable physical objects. Each element can be connected with and disconnected from neighboring elements, and ele- ments can be formed into an arbitrary three-dimensional phys- ical object. Dynamic 3D printing differs from existing 3D printing in speed and reconstructability: Dynamic 3D printing forms shapes in seconds, rather than minutes. In addition, because individual elements can be disconnected, the shape can be easily disassembled into its basic building blocks once the object is no longer needed.\n  </p>\n</div>\n\n\n# Implementation\n\nThe assembler consists of a 24 x 16 array of motor-driven pins. Each pin moves up and down, driven by a small DC motor (TTMotors TGPP06-D700) and a 3D printed lead screw (2 mm pitch, 4 starts, 120 mm in length). TGPP06-D700 is 6 mm in diameter and 29 mm in length and can rotate 47 rpm with 1:700 gear ratio. The 2 mm 4 starts lead screw can travel 12 mm per second without load, and each motor consumes approximately 60 mA. The pins are 3D printed with a nut at the bottom to travel along the lead screw. Each pin is 120 mm long and has a 7mm square cross section with a 5 mm diameter hole from top to bottom, and an N45 disk magnet (φ 3mm x 2.4 mm thickness) is attached at the top. Guide grids at the top prevent pins from rotating and ensure that pins travel vertically. The 24 x 16 guide grids have 7.5 mm square holes with 10.16 mm pitch and are cut from a 5 mm acrylic plate. We fabricated the pins, the lead screws, and blocks with an inkjet 3D printer (Keyence Agilista 3200) with water soluble support material. In total, we fabricated 384 (= 24 x 16) pins and lead screws, and 3,072 (= 24 x 16 x 8 layers) blocks. To create the magnetic blocks, we embedded spherical magnets in each block by hand and inserted disk magnets using a bench vice.\n\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-5-1.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-5-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-5-2.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-5-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-5-3.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-5-3.png" /></a>\n  </div>\n</div>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-7-1.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-7-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-7-2.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-7-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-7-3.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-7-3.jpg" /></a>\n  </div>\n</div>\n\n\n# Future Vision\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/dynablock/webm/claytronics.webm" type="video/webm"></source>\n  <source src="/static/projects/dynablock/video/claytronics.mp4" type="video/mp4"></source>\n</video>\n[Video Credit: Carnegie Mellon University, Claytronics Vision]\n\n<br/>\n\nWith these capabilities, a 3D printer would become an inter- active medium, rather than merely a fabrication device. For example, such a 3D printer could be used in a Virtual Real- ity or Augmented Reality application to dynamically form a tangible object or controller to provide haptic feedback and engage users physically. For children, it could dynamically form a physical educational manipulative, such as a molec- ular or architectural model, to learn and explore topics, for example in a science museum. Designers could use it to ren- der a physical product to present to clients and interactively change the product’s design through direct manipulation. In this vision, Dynamic 3D printing is an environment in which the user thinks, designs, explores, and communicates through dynamic and interactive physical representation.\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-8-1.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-8-1.png" /></a>\n  </div>\n</div>\n\nDynamic 3D printing would enable a new design workflow for digital fabrication. One notable advantage of dynamic 3D printing is the capability of connecting and disconnecting building blocks through direct manipulation. The user can also define variables or abstract attributes for parametric design through direct and gestural interaction. By leveraging this capability, the user could interactively design and fabri- cate in a physical space, similar to the man-machine dialogue proposed by Frazer et al. and later tangible CAD interfaces.',bodyHtml:'\x3c!--\nLinks:\n[**[PDF](http://ryosuzuki.org/publications/uist-2018-dynablock.pdf)**]\n[**[ACM DL](https://dl.acm.org/citation.cfm?id=3242659)**]\n[**[Video](https://www.youtube.com/watch?v=7nPlr3O9xu8)**]\n[**[Slide](http://ryosuzuki.org/publications/uist-2018-dynablock-slide.pdf)**]\n[**[Talk](https://www.youtube.com/watch?v=R3FRUtOIiCQ)**]\n --\x3e\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/dynablock/webm/top.webm" type="video/webm"></source>\n  <source src="/static/projects/dynablock/video/top.mp4" type="video/mp4"></source>\n</video>\n<h1>Abstract</h1>\n<p>This paper introduces Dynamic 3D Printing, a fast and re- constructable shape formation system. Dynamic 3D Printing assembles an arbitrary three-dimensional shape from a large number of small physical elements. It can also disassemble the shape back to elements and reconstruct a new shape. Dynamic 3D Printing combines the capabilities of 3D printers and shape displays: Like conventional 3D printing, it can generate arbi- trary and graspable three-dimensional shapes, while allowing shapes to be rapidly formed and reformed as in a shape display. To demonstrate the idea, we describe the design and imple- mentation of Dynablock, a working prototype of a dynamic 3D printer. Dynablock can form a three-dimensional shape in seconds by assembling 3,000 9 mm blocks, leveraging a 24 x 16 pin-based shape display as a parallel assembler. Dynamic 3D printing is a step toward achieving our long term vision in which 3D printing becomes an interactive medium, rather than the means for fabrication that it is today. In this paper we explore possibilities for this vision by illustrating application scenarios that are difficult to achieve with conventional 3D printing or shape display systems.</p>\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-1-1.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-1-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-1-2.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-1-2.jpg" /></a>\n  </div>\n</div>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-2-1.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-2-1.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-2-2.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-2-2.jpg" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-2-3.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-2-3.jpg" /></a>\n  </div>\n</div>\n<h1>Dynamic 3D Printing</h1>\n<p>What if 3D printers could form a physical object in seconds? What if the object, once it is no longer needed, could quickly and easily be disassembled and reconstructed as a new object? Today’s 3D printers take hours to print objects, and output a single static object. However, we envision a future in which 3D printing could instantly create objects from reusable and reconstructable materials.</p>\n<p>This paper develops this vision by proposing Dynamic 3D Printing, a class of systems for rapid and reconstructable shape formation. Dynamic 3D Printing assembles digital material elements to form recon- structable physical objects. Each element can be connected with and disconnected from neighboring elements, and ele- ments can be formed into an arbitrary three-dimensional phys- ical object. Dynamic 3D printing differs from existing 3D printing in speed and reconstructability: Dynamic 3D printing forms shapes in seconds, rather than minutes. In addition, because individual elements can be disconnected, the shape can be easily disassembled into its basic building blocks once the object is no longer needed.</p>\n<p>We define Dynamic 3D Printing as a class of systems that have the following properties:</p>\n<ul>\n<li>\n<p>Immediate: The system can form a physical shape in sec- onds.</p>\n</li>\n<li>\n<p>Reconstructable: Rendered shapes can be disassembled and reconstructed by hand or with the system, and the blocks are reusable.</p>\n</li>\n<li>\n<p>Arbitrary Shapes: It can create arbitrary three dimensional shapes.</p>\n</li>\n<li>\n<p>Graspable: The output shapes and structure are graspable and solid.</p>\n</li>\n</ul>\n<h1>Parallel Assembler</h1>\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-3-1.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-3-1.png" /></a>\n  </div>\n</div>\n<p>Dynamic 3D printing deploys a large number of small dis- crete material elements, which are assembled to form arbitrary shaped macro-scale objects. Individual elements are passive, which requires an external actuator to perform the assembly. As illustrated in the above Figure, the assembler consists of an N x N grid of motorized pins and linear actuators. The elements, which are the same size as the pins, are stacked on top of the pins (Figure 3 A). When stacked, the elements are connected in vertical direction, while discon- nected with nearby elements in horizontal direction. Similar to existing pin-based shape displays, the assembler can incrementally generate 2.5D shapes by individually moving pins to push elements to the surface.</p>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/dynablock/webm/mechanism.webm" type="video/webm"></source>\n  <source src="/static/projects/dynablock/video/mechanism.mp4" type="video/mp4"></source>\n</video>\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-4-1.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-4-1.png" /></a>\n  </div>\n  <p class="column">\n    This paper develops this vision by proposing Dynamic 3D Printing, a class of systems for rapid and reconstructable shape formation. Dynamic 3D Printing assembles digital material elements to form recon- structable physical objects. Each element can be connected with and disconnected from neighboring elements, and ele- ments can be formed into an arbitrary three-dimensional phys- ical object. Dynamic 3D printing differs from existing 3D printing in speed and reconstructability: Dynamic 3D printing forms shapes in seconds, rather than minutes. In addition, because individual elements can be disconnected, the shape can be easily disassembled into its basic building blocks once the object is no longer needed.\n  </p>\n</div>\n<h1>Implementation</h1>\n<p>The assembler consists of a 24 x 16 array of motor-driven pins. Each pin moves up and down, driven by a small DC motor (TTMotors TGPP06-D700) and a 3D printed lead screw (2 mm pitch, 4 starts, 120 mm in length). TGPP06-D700 is 6 mm in diameter and 29 mm in length and can rotate 47 rpm with 1:700 gear ratio. The 2 mm 4 starts lead screw can travel 12 mm per second without load, and each motor consumes approximately 60 mA. The pins are 3D printed with a nut at the bottom to travel along the lead screw. Each pin is 120 mm long and has a 7mm square cross section with a 5 mm diameter hole from top to bottom, and an N45 disk magnet (φ 3mm x 2.4 mm thickness) is attached at the top. Guide grids at the top prevent pins from rotating and ensure that pins travel vertically. The 24 x 16 guide grids have 7.5 mm square holes with 10.16 mm pitch and are cut from a 5 mm acrylic plate. We fabricated the pins, the lead screws, and blocks with an inkjet 3D printer (Keyence Agilista 3200) with water soluble support material. In total, we fabricated 384 (= 24 x 16) pins and lead screws, and 3,072 (= 24 x 16 x 8 layers) blocks. To create the magnetic blocks, we embedded spherical magnets in each block by hand and inserted disk magnets using a bench vice.</p>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-5-1.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-5-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-5-2.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-5-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-5-3.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-5-3.png" /></a>\n  </div>\n</div>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-7-1.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-7-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-7-2.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-7-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-7-3.jpg" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-7-3.jpg" /></a>\n  </div>\n</div>\n<h1>Future Vision</h1>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/dynablock/webm/claytronics.webm" type="video/webm"></source>\n  <source src="/static/projects/dynablock/video/claytronics.mp4" type="video/mp4"></source>\n</video>\n[Video Credit: Carnegie Mellon University, Claytronics Vision]\n<br/>\n<p>With these capabilities, a 3D printer would become an inter- active medium, rather than merely a fabrication device. For example, such a 3D printer could be used in a Virtual Real- ity or Augmented Reality application to dynamically form a tangible object or controller to provide haptic feedback and engage users physically. For children, it could dynamically form a physical educational manipulative, such as a molec- ular or architectural model, to learn and explore topics, for example in a science museum. Designers could use it to ren- der a physical product to present to clients and interactively change the product’s design through direct manipulation. In this vision, Dynamic 3D printing is an environment in which the user thinks, designs, explores, and communicates through dynamic and interactive physical representation.</p>\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/dynablock/figure-8-1.png" data-lightbox="lightbox"><img src="/static/projects/dynablock/figure-8-1.png" /></a>\n  </div>\n</div>\n<p>Dynamic 3D printing would enable a new design workflow for digital fabrication. One notable advantage of dynamic 3D printing is the capability of connecting and disconnecting building blocks through direct manipulation. The user can also define variables or abstract attributes for parametric design through direct and gestural interaction. By leveraging this capability, the user could interactively design and fabri- cate in a physical space, similar to the man-machine dialogue proposed by Frazer et al. and later tangible CAD interfaces.</p>\n',dir:"content/output/projects",base:"dynablock.json",ext:".json",sourceBase:"dynablock.md",sourceExt:".md"}},Jg5j:function(e){e.exports={id:"trace-diff",name:"TraceDiff",description:"Debugging Unexpected Code Behavior Using Trace Divergences",title:"TraceDiff: Debugging Unexpected Code Behavior Using Trace Divergences",authors:["Ryo Suzuki","Gustavo Soares","Andrew Head","Elena Glassman","Ruan Reis","Melina Mongiovi","Loris D’Antoni","Bjöern Hartmann"],year:2017,booktitle:"In Proceedings of 2017 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC '17)",publisher:"IEEE Press, Piscataway, NJ, USA",pages:"107-115",doi:"https://doi.org/10.1109/VLHCC.2017.8103457",conference:{name:"VL/HCC 2017",fullname:"IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC 2017)",url:"https://sites.google.com/site/vlhcc2017/"},pdf:"vlhcc-2017-tracediff.pdf",slide:"vlhcc-2017-tracediff-slide.pdf",github:"https://github.com/ryosuzuki/trace-diff",demo:"https://ryosuzuki.github.io/trace-diff/",ieee:"http://ieeexplore.ieee.org/document/8103457/",arxiv:"https://arxiv.org/abs/1708.03786a",related:{title:"Exploring the Design Space of Automatically Synthesized Hints for Introductory Programming Assignments",authors:["Ryo Suzuki","Gustavo Soares","Elena Glassman","Andrew Head","Loris D'Antoni","Björn Hartmann"],year:2017,booktitle:"In Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems (CHI EA '17)",publisher:"ACM, New York, NY, USA",pages:"2951-2958",doi:"https://doi.org/10.1145/3027063.3053187",pdf:"chi-2017-lbw.pdf",suffix:"lbw",pageCount:6},pageCount:9,slideCount:77,bodyContent:'<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/trace-diff/top.mp4" type="video/mp4"></source>\n</video>\n\n# Abstract\n\nRecent advances in program synthesis offer means to automatically debug student submissions and generate personalized feedback in massive programming classrooms. When automatically generating feedback for programming assignments, a key challenge is designing pedagogically useful hints that are as effective as the manual feedback given by teachers. Through an analysis of teachers’ hint-giving practices in 132 online Q&A posts, we establish three design guidelines that an effective feedback design should follow. Based on these guidelines, we develop a feedback system that leverages both program synthesis and visualization techniques. Our system compares the dynamic code execution of both incorrect and fixed code and highlights how the error leads to a difference in behavior and where the incorrect code trace diverges from the expected solution. Results from our study suggest that our system enables students to detect and fix bugs that are not caught by students using another existing visual debugging tool.',bodyHtml:'<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/trace-diff/top.mp4" type="video/mp4"></source>\n</video>\n<h1>Abstract</h1>\n<p>Recent advances in program synthesis offer means to automatically debug student submissions and generate personalized feedback in massive programming classrooms. When automatically generating feedback for programming assignments, a key challenge is designing pedagogically useful hints that are as effective as the manual feedback given by teachers. Through an analysis of teachers’ hint-giving practices in 132 online Q&amp;A posts, we establish three design guidelines that an effective feedback design should follow. Based on these guidelines, we develop a feedback system that leverages both program synthesis and visualization techniques. Our system compares the dynamic code execution of both incorrect and fixed code and highlights how the error leads to a difference in behavior and where the incorrect code trace diverges from the expected solution. Results from our study suggest that our system enables students to detect and fix bugs that are not caught by students using another existing visual debugging tool.</p>\n',dir:"content/output/projects",base:"trace-diff.json",ext:".json",sourceBase:"trace-diff.md",sourceExt:".md"}},PSd4:function(e){e.exports={id:"mixed-initiative",name:"Mixed-Initiative Code Feedback",description:"Writing Reusable Code Feedback at Scale with Mixed-Initiative Program Synthesis",title:"Writing Reusable Code Feedback at Scale with Mixed-Initiative Program Synthesis",authors:["Andrew Head","Elena Glassman","Gustavo Soares","Ryo Suzuki","Lucas Figueredo","Loris D’Antoni","Björn Hartmann"],note:"(the first three authors equally contributed)",year:2017,booktitle:"In Proceedings of the Fourth (2017) ACM Conference on Learning @ Scale (L@S '17)",publisher:"ACM, New York, NY, USA",pages:"89-98",doi:"https://doi.org/10.1145/3051457.3051467",conference:{name:"L@S 2017",fullname:"The ACM Conference on Learning at Scale (L@S 2017)",url:"http://learningatscale.acm.org/las2017"},pdf:"las-2017-mixed.pdf","acm-dl":"http://dl.acm.org/citation.cfm?id=3051467",pageCount:10,slideCount:62,image:"mixed-initiative.png",bodyContent:"",bodyHtml:"",dir:"content/output/projects",base:"mixed-initiative.json",ext:".json",sourceBase:"mixed-initiative.md",sourceExt:".md"}},RHEb:function(e,i,t){"use strict";t.r(i);for(var a=t("0iUn"),o=t("sLSF"),n=t("MI3g"),r=t("a7VT"),s=t("Tit0"),c=t("q1tI"),l=t.n(c),p=(t("IujW"),t("6T/A"),[]),d=0,g=["shapebots","morphio","dynablock","tabby","reactile","pep","flux-marker","trace-diff","mixed-initiative","refazer","atelier"];d<g.length;d++){var m=g[d],h=t("o0EK")("./".concat(m,".json"));p.push(h)}var u=function(e){function i(){return Object(a.default)(this,i),Object(n.default)(this,Object(r.default)(i).apply(this,arguments))}return Object(s.default)(i,e),Object(o.default)(i,[{key:"componentDidMount",value:function(){}},{key:"render",value:function(){return l.a.createElement("div",{id:"projects"},l.a.createElement("h1",null,"Full Papers"),p.map(function(e){return l.a.createElement("div",{className:"project ui vertical segment stackable grid","data-id":e.id},l.a.createElement("div",{className:"six wide column"},e.image&&l.a.createElement("a",{href:"/".concat(e.id)},l.a.createElement("img",{className:"ui rounded images",src:"/static/images/".concat(e.image)})),!e.image&&l.a.createElement("video",{poster:"/static/posters/".concat(e.id,".png"),autoplay:"",loop:"loop",muted:"true",playsinline:"",width:"100%",onclick:"this.play()",onmouseover:"this.play()"},l.a.createElement("source",{src:"/static/webm/".concat(e.id,".webm"),type:"video/webm"}),l.a.createElement("source",{src:"/static/video/".concat(e.id,".mp4"),type:"video/mp4"}))),l.a.createElement("div",{className:"ten wide column"},l.a.createElement("a",{href:"/".concat(e.id)},l.a.createElement("h1",{className:"ui header",style:{marginBottom:"10px"}},l.a.createElement("span",null,e.name),l.a.createElement("span",{className:"ui big label"},e.conference.name)),l.a.createElement("h2",{style:{margin:"5px 0"}},e.description)),l.a.createElement("p",null,e.authors.map(function(e){return"Ryo Suzuki"===e?l.a.createElement("strong",null,e):l.a.createElement("span",null,e)}).reduce(function(e,i){return[e,", ",i]}),"   ",l.a.createElement("span",{style:{color:"gray"}},e.note))))}))}}]),i}(l.a.Component);i.default=u},"W/HP":function(e){e.exports={id:"pep",name:"PEP",description:"3D Printed Electronic Papercrafts - An Integrated Approach for 3D Sculpting Paper-based Electronic Devices",title:"PEP (3D Printed Electronic Papercrafts): An Integrated Approach for 3D Sculpting Paper-based Electronic Devices",authors:["Hyunjoo Oh","Tung D. Ta","Ryo Suzuki","Mark D. Gross","Yoshihiro Kawahara","Lining Yao"],year:2018,booktitle:"In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (CHI '18)",publisher:"ACM, New York, NY, USA",pages:"Paper 441, 12 pages",doi:"https://doi.org/10.1145/3173574.3174015",conference:{name:"CHI 2018",fullname:"The ACM CHI Conference on Human Factors in Computing Systems (CHI 2018)",url:"https://chi2018.acm.org/"},pdf:"chi-2018-pep.pdf",video:"https://vimeo.com/252080903",embed:"https://www.youtube.com/embed/DTd863suDN0","short-video":"https://www.youtube.com/watch?v=DTd863suDN0","acm-dl":"https://dl.acm.org/citation.cfm?id=3174015",pageCount:12,slideCount:0,bodyContent:"",bodyHtml:"",dir:"content/output/projects",base:"pep.json",ext:".json",sourceBase:"pep.md",sourceExt:".md"}},"X0/d":function(e){e.exports={id:"morphio",name:"MorphIO",description:"Entirely Soft Sensing and Actuation Modules for Programming Shape Changes through Tangible Interaction",title:"MorphIO: Entirely Soft Sensing and Actuation Modules for Programming Shape Changes through Tangible Interaction",authors:["Ryosuke Nakayama","Ryo Suzuki","Satoshi Nakamaru","Ryuma Niiyama","Yoshihiro Kawahara","Yasuaki Kakehi"],note:"(the first two authors equally contributed)",year:2018,booktitle:"In Proceedings of the 2019 on Designing Interactive Systems Conference (DIS '19)",publisher:"ACM, New York, NY, USA",pages:"975-986",doi:"https://doi.org/10.1145/3322276.3322337",conference:{name:"DIS 2019",fullname:"The ACM conference on Designing Interactive Systems (DIS 2019) - Best Paper Award",url:"https://dis2019.com/"},pdf:"dis-2019-morphio.pdf",slide:"dis-2019-morphio-slide.pdf",video:"https://www.youtube.com/watch?v=ZkCcazfFD-M",embed:"https://www.youtube.com/embed/ZkCcazfFD-M","acm-dl":"https://dl.acm.org/citation.cfm?id=3322337",pageCount:12,slideCount:52,bodyContent:'<video poster="/static/projects/morphio/video-poster/top.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/top.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/top.mp4" type="video/mp4"></source>\n</video>\n\n\n# Abstract\n\nWe introduce **MorphIO, entirely soft sensing and actuation modules** for programming by demonstration of soft robots and shape-changing interfaces. MorphIO’s hardware consists of a **soft pneumatic actuator containing a conductive sponge sensor**. This allows both input and output of three-dimensional deformation of a soft material. Leveraging this capability, MorphIO enables a user to **record and later playback physical motion** of programmable shape-changing materials. In addition, the modular design of MorphIO’s unit allows the user to construct various shapes and topologies through magnetic connection. We demonstrate several application scenarios, including tangible character animation, locomotion experiment of a soft robot, and prototyping tools for animated soft objects. Our user study with six participants confirms the benefits of MorphIO, as compared to the existing programming paradigm.\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-1-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-1-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-1-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-1-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-1-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-1-3.png" /></a>\n  </div>\n</div>\n\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-1-4.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-1-4.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-1-5.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-1-5.png" /></a>\n  </div>\n</div>\n\n# Introduction\n\n**Programmable soft materials** have a great potential for many application domains, such as soft robotics, material interfaces, accessibility, and haptic interfaces.\n**However, programming of such materials is hard.**\nThe dominant programming paradigm of soft robots and material interfaces is largely confined within a digital screen, leaving little room for users to interactively explore physical motion through tangible interaction. In such a workflow—compiling code on a digital screen then trans- ferring it into the physical object—users need to repeatedly switch between the digital and physical worlds. This leaves a large gulf of execution in their programming experiences.\nThus, the traditional programming paradigm significantly limits the user’s ability to experiment with the design of expressive motion. Moreover, due to this barrier, such an opportunity is largely limited to highly skilled programmers and researchers who are proficient in hardware programming.\n\n\n# MorphIO\n\nThis paper introduces **MorphIO, entirely soft sensing and actuation modules** for programming by demonstration of soft robots and shape-changing interfaces.\nMorphIO’s hardware consists of a soft pneumatic actuator containing a conductive sponge sensor. This allows for integrated and entirely soft shape-changing modules that can both sense and actuate a variety of three-dimensional deformations. Leveraging this capability, MorphIO enables the user to program behaviors by **recording and later playing back physical motions** through tangible interaction. In addition, the modular design of MorphIO’s unit allows the user to construct various shapes and topologies through magnetic connection, then **synthesize multiple recorded motions to achieve more complex behaviors**, such as bending, gripping, and walking.\n\n\n<video poster="/static/projects/morphio/video-poster/module.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/module.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/module.mp4" type="video/mp4"></source>\n</video>\n\n<br/>\n\n<video poster="/static/projects/morphio/video-poster/bear.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/bear.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/bear.mp4" type="video/mp4"></source>\n</video>\n\n\n# System Overview\n\nThe programming workflow with MorphIO is the following:\n\n- **Step 1:** A user starts manipulating the MorphIO unit.\n\n- **Step 2:** The demonstrated motion is detected and recorded through internal sensors, and the recorded sensor values are stored in the software.\n\n- **Step 3:** Once the user clicks play in the graphical user interface, the pneumatic pump starts supplying air.\n\n- **Step 4:** By controlling the air flow through switching on and off the solenoid valves, the system can control the behavior of the pneumatic actuator as it plays back the recorded motion.\n\nThe MorphIO system consists of the following components: A sensor and actuation unit, a sensing and actuation control unit, a microcontroller, software to control these units, and a visual interface for users to control behaviors. Figure illustrates the overview architecture of MorphIO.\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-6-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-6-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-6-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-6-2.png" /></a>\n  </div>\n</div>\n\n\n# Entirely Soft Sensing and Actuation Modules\n\nOur main contribution is a design and fabrication method for **a conductive sponge sensor** that can be embedded into an air chamber in the pneumatic actuator. The conductive sponge sensor leverages the porous structure to **sense the three-dimensional deformation by measuring the internal resistance value**; when contracted, the resistance value be- tween the top and bottom surfaces drops, and when extended, it increases. In contrast to existing sensing techniques, an elastic sponge allows for a higher degree of freedom in sensing capability (e.g., stretching, bending, and compression) without sacrificing the softness of the interface.\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-2-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-2-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-2-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-2-2.png" /></a>\n  </div>\n</div>\n\n\n<video poster="/static/projects/morphio/video-poster/mechanism.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/mechanism.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/mechanism.mp4" type="video/mp4"></source>\n</video>\n\n\n<br />\n\nMoreover, our **modular design** and **graphical interface** allows for easy experiments involving multiple units. For example, the system can visualize multiple recorded sensor values, so that the user can see, customize, and synthesize recorded motion to construct more complex behaviors. These hardware and software designs were informed by our formative study, wherein we interviewed five experienced researchers from the robotics and HCI communities.\n\n\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-3-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-3-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-3-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-3-2.png" /></a>\n  </div>\n</div>\n\n<video poster="/static/projects/morphio/video-poster/unit-x2.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/unit-x2.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/unit-x2.mp4" type="video/mp4"></source>\n</video>\n\n<video poster="/static/projects/morphio/video-poster/unit-x3.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/unit-x3.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/unit-x3.mp4" type="video/mp4"></source>\n</video>\n\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-4-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-4-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-4-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-4-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-4-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-4-3.png" /></a>\n  </div>\n</div>\n\n\n\n\n# Fabrication Process\n\nThe fabrication process follows three steps: 1) Fabricate an elastic sponge, 2) impregnate into conductive ink, and 3) attach electrodes and wires.\nTo fabricate an elastic sponge, we first prepare 6.0 g of elastomer prepolymer solution and 29.1 g of sodium-chloride, then mix them together by using a planetary centrifugal mixer. The mixed solution is injected into a 3D printed cylindrical mold (16mm diameter, 40mm height). Then we dry the material with an oven at 100 C degrees for one hour. Once dried, we immerse the sponge in water, so that the sodium chloride can melt, leaving a porous structure within the elastomer sponge.\n\n<video poster="/static/projects/morphio/video-poster/fabrication.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/fabrication.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/fabrication.mp4" type="video/mp4"></source>\n</video>\n\n<div class="figures ui four column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-3.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-4.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-4.png" /></a>\n  </div>\n</div>\n\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-5.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-5.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-6.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-6.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-7.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-7.png" /></a>\n  </div>\n</div>\n\n\n\n# Applications\n\nWe demonstrate several possible applications scenarios with MorphIO. 1) Tangible character animation, 2) Animating existing soft objects, 3) Remote manipulation of soft grippers, 4) Locomotion experimentation with soft robots.\n\n<div class="figures ui four column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-10-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-10-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-10-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-10-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-10-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-10-3.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-10-4.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-10-4.png" /></a>\n  </div>\n</div>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-11-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-11-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-11-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-11-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-11-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-11-3.png" /></a>\n  </div>\n</div>\n\n<video poster="/static/projects/morphio/video-poster/locomotion.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/locomotion.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/locomotion.mp4" type="video/mp4"></source>\n</video>\n\n\n# Evaluation\n\nWe conducted a user evaluation study to understand the bene- fits and limitations of MorphIO. In this study, we focused on answering the following research questions:\n\n- **RQ1:** Does MorphIO save time and reduce the number of iterations to program the target behavior, compared to the existing approach?\n\n- **RQ2:** Does MorphIO increase the expressiveness of the physical motion?\n\nTo answer these questions, we conducted a controlled experiment where we compared MorphIO (left) with the current programming approach. We chose Arduino IDE (right) as a base condition for the comparison, as this is the most common programming approach identified through our formative study. We provide three basic tasks to construct a program. For each task, the participants were asked to program three differ- ent emotions—happiness, anger, and sadness—of an animated character. We chose these emotions based on Ekman’s basic emotions for communication.\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-12-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-12-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-12-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-12-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-12-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-12-3.png" /></a>\n  </div>\n</div>\n\nThe average time of task completion time of MorphIO was 2m 19s, compared to 5m 21s with the control condition. The average number of iterations of MorphIO was 4.4 times, compared to 6.4 with the control condition, which confirms that MorphIO is significantly efficient in terms of task completion time and the number of iterations. When asked about the achievement of the expressions using a 9-point Likert scale, the average score with MorphIO was 6.5, compared to 6.3 with the control condition. We did not find differences between the two conditions. Thus, we conclude the result of our study as follows: **RQ1: Yes, RQ2: No**.\n\nBased on our post interviews, we discuss the benefits and limitations of our approach: **1) tangible interactions are suitable for sculpting rough motion**, **2) programming allows for fine-tuning more precise adjustments**. Thus, for future research, systems might allow users to quickly make a rough motion, which can automatically be converted into digital parameters so that the user can also precisely control and adjust the motion. The same human- computer cooperation approach can be applied to other design domains: For example, when designing an object, the user can quickly make rough shapes with clay, while letting a machine finish the details. We believe this insight can lead the HCI community to further explore design approaches wherein users and machines cooperate for enhanced interaction design.\n\n\n# Future Vision\nWe believe this approach’s potential for lowering the barrier and opening new opportunities for a larger community to begin designing, prototyping, and exploring soft material motion—not by coding on a screen, but by sculpting behaviors in the physical world.\nWe envision the future where people can interactively explore various behaviors through tangible interactions, **just like sculpting behaviors with clay**.\n\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-14.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-14.png" /></a>\n  </div>\n</div>',bodyHtml:'<video poster="/static/projects/morphio/video-poster/top.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/top.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/top.mp4" type="video/mp4"></source>\n</video>\n<h1>Abstract</h1>\n<p>We introduce <strong>MorphIO, entirely soft sensing and actuation modules</strong> for programming by demonstration of soft robots and shape-changing interfaces. MorphIO’s hardware consists of a <strong>soft pneumatic actuator containing a conductive sponge sensor</strong>. This allows both input and output of three-dimensional deformation of a soft material. Leveraging this capability, MorphIO enables a user to <strong>record and later playback physical motion</strong> of programmable shape-changing materials. In addition, the modular design of MorphIO’s unit allows the user to construct various shapes and topologies through magnetic connection. We demonstrate several application scenarios, including tangible character animation, locomotion experiment of a soft robot, and prototyping tools for animated soft objects. Our user study with six participants confirms the benefits of MorphIO, as compared to the existing programming paradigm.</p>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-1-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-1-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-1-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-1-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-1-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-1-3.png" /></a>\n  </div>\n</div>\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-1-4.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-1-4.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-1-5.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-1-5.png" /></a>\n  </div>\n</div>\n<h1>Introduction</h1>\n<p><strong>Programmable soft materials</strong> have a great potential for many application domains, such as soft robotics, material interfaces, accessibility, and haptic interfaces.\n<strong>However, programming of such materials is hard.</strong>\nThe dominant programming paradigm of soft robots and material interfaces is largely confined within a digital screen, leaving little room for users to interactively explore physical motion through tangible interaction. In such a workflow—compiling code on a digital screen then trans- ferring it into the physical object—users need to repeatedly switch between the digital and physical worlds. This leaves a large gulf of execution in their programming experiences.\nThus, the traditional programming paradigm significantly limits the user’s ability to experiment with the design of expressive motion. Moreover, due to this barrier, such an opportunity is largely limited to highly skilled programmers and researchers who are proficient in hardware programming.</p>\n<h1>MorphIO</h1>\n<p>This paper introduces <strong>MorphIO, entirely soft sensing and actuation modules</strong> for programming by demonstration of soft robots and shape-changing interfaces.\nMorphIO’s hardware consists of a soft pneumatic actuator containing a conductive sponge sensor. This allows for integrated and entirely soft shape-changing modules that can both sense and actuate a variety of three-dimensional deformations. Leveraging this capability, MorphIO enables the user to program behaviors by <strong>recording and later playing back physical motions</strong> through tangible interaction. In addition, the modular design of MorphIO’s unit allows the user to construct various shapes and topologies through magnetic connection, then <strong>synthesize multiple recorded motions to achieve more complex behaviors</strong>, such as bending, gripping, and walking.</p>\n<video poster="/static/projects/morphio/video-poster/module.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/module.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/module.mp4" type="video/mp4"></source>\n</video>\n<br/>\n<video poster="/static/projects/morphio/video-poster/bear.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/bear.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/bear.mp4" type="video/mp4"></source>\n</video>\n<h1>System Overview</h1>\n<p>The programming workflow with MorphIO is the following:</p>\n<ul>\n<li>\n<p><strong>Step 1:</strong> A user starts manipulating the MorphIO unit.</p>\n</li>\n<li>\n<p><strong>Step 2:</strong> The demonstrated motion is detected and recorded through internal sensors, and the recorded sensor values are stored in the software.</p>\n</li>\n<li>\n<p><strong>Step 3:</strong> Once the user clicks play in the graphical user interface, the pneumatic pump starts supplying air.</p>\n</li>\n<li>\n<p><strong>Step 4:</strong> By controlling the air flow through switching on and off the solenoid valves, the system can control the behavior of the pneumatic actuator as it plays back the recorded motion.</p>\n</li>\n</ul>\n<p>The MorphIO system consists of the following components: A sensor and actuation unit, a sensing and actuation control unit, a microcontroller, software to control these units, and a visual interface for users to control behaviors. Figure illustrates the overview architecture of MorphIO.</p>\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-6-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-6-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-6-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-6-2.png" /></a>\n  </div>\n</div>\n<h1>Entirely Soft Sensing and Actuation Modules</h1>\n<p>Our main contribution is a design and fabrication method for <strong>a conductive sponge sensor</strong> that can be embedded into an air chamber in the pneumatic actuator. The conductive sponge sensor leverages the porous structure to <strong>sense the three-dimensional deformation by measuring the internal resistance value</strong>; when contracted, the resistance value be- tween the top and bottom surfaces drops, and when extended, it increases. In contrast to existing sensing techniques, an elastic sponge allows for a higher degree of freedom in sensing capability (e.g., stretching, bending, and compression) without sacrificing the softness of the interface.</p>\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-2-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-2-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-2-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-2-2.png" /></a>\n  </div>\n</div>\n<video poster="/static/projects/morphio/video-poster/mechanism.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/mechanism.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/mechanism.mp4" type="video/mp4"></source>\n</video>\n<br />\n<p>Moreover, our <strong>modular design</strong> and <strong>graphical interface</strong> allows for easy experiments involving multiple units. For example, the system can visualize multiple recorded sensor values, so that the user can see, customize, and synthesize recorded motion to construct more complex behaviors. These hardware and software designs were informed by our formative study, wherein we interviewed five experienced researchers from the robotics and HCI communities.</p>\n<div class="figures ui two column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-3-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-3-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-3-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-3-2.png" /></a>\n  </div>\n</div>\n<video poster="/static/projects/morphio/video-poster/unit-x2.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/unit-x2.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/unit-x2.mp4" type="video/mp4"></source>\n</video>\n<video poster="/static/projects/morphio/video-poster/unit-x3.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/unit-x3.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/unit-x3.mp4" type="video/mp4"></source>\n</video>\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-4-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-4-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-4-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-4-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-4-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-4-3.png" /></a>\n  </div>\n</div>\n<h1>Fabrication Process</h1>\n<p>The fabrication process follows three steps: 1) Fabricate an elastic sponge, 2) impregnate into conductive ink, and 3) attach electrodes and wires.\nTo fabricate an elastic sponge, we first prepare 6.0 g of elastomer prepolymer solution and 29.1 g of sodium-chloride, then mix them together by using a planetary centrifugal mixer. The mixed solution is injected into a 3D printed cylindrical mold (16mm diameter, 40mm height). Then we dry the material with an oven at 100 C degrees for one hour. Once dried, we immerse the sponge in water, so that the sodium chloride can melt, leaving a porous structure within the elastomer sponge.</p>\n<video poster="/static/projects/morphio/video-poster/fabrication.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/fabrication.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/fabrication.mp4" type="video/mp4"></source>\n</video>\n<div class="figures ui four column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-3.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-4.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-4.png" /></a>\n  </div>\n</div>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-5.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-5.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-6.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-6.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-5-7.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-5-7.png" /></a>\n  </div>\n</div>\n<h1>Applications</h1>\n<p>We demonstrate several possible applications scenarios with MorphIO. 1) Tangible character animation, 2) Animating existing soft objects, 3) Remote manipulation of soft grippers, 4) Locomotion experimentation with soft robots.</p>\n<div class="figures ui four column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-10-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-10-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-10-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-10-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-10-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-10-3.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-10-4.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-10-4.png" /></a>\n  </div>\n</div>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-11-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-11-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-11-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-11-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-11-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-11-3.png" /></a>\n  </div>\n</div>\n<video poster="/static/projects/morphio/video-poster/locomotion.png" preload="metadata" autoplay loop muted playsinline webkit-playsinline="">\n  <source src="/static/projects/morphio/webm/locomotion.webm" type="video/webm"></source>\n  <source src="/static/projects/morphio/video/locomotion.mp4" type="video/mp4"></source>\n</video>\n<h1>Evaluation</h1>\n<p>We conducted a user evaluation study to understand the bene- fits and limitations of MorphIO. In this study, we focused on answering the following research questions:</p>\n<ul>\n<li>\n<p><strong>RQ1:</strong> Does MorphIO save time and reduce the number of iterations to program the target behavior, compared to the existing approach?</p>\n</li>\n<li>\n<p><strong>RQ2:</strong> Does MorphIO increase the expressiveness of the physical motion?</p>\n</li>\n</ul>\n<p>To answer these questions, we conducted a controlled experiment where we compared MorphIO (left) with the current programming approach. We chose Arduino IDE (right) as a base condition for the comparison, as this is the most common programming approach identified through our formative study. We provide three basic tasks to construct a program. For each task, the participants were asked to program three differ- ent emotions—happiness, anger, and sadness—of an animated character. We chose these emotions based on Ekman’s basic emotions for communication.</p>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-12-1.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-12-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-12-2.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-12-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-12-3.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-12-3.png" /></a>\n  </div>\n</div>\n<p>The average time of task completion time of MorphIO was 2m 19s, compared to 5m 21s with the control condition. The average number of iterations of MorphIO was 4.4 times, compared to 6.4 with the control condition, which confirms that MorphIO is significantly efficient in terms of task completion time and the number of iterations. When asked about the achievement of the expressions using a 9-point Likert scale, the average score with MorphIO was 6.5, compared to 6.3 with the control condition. We did not find differences between the two conditions. Thus, we conclude the result of our study as follows: <strong>RQ1: Yes, RQ2: No</strong>.</p>\n<p>Based on our post interviews, we discuss the benefits and limitations of our approach: <strong>1) tangible interactions are suitable for sculpting rough motion</strong>, <strong>2) programming allows for fine-tuning more precise adjustments</strong>. Thus, for future research, systems might allow users to quickly make a rough motion, which can automatically be converted into digital parameters so that the user can also precisely control and adjust the motion. The same human- computer cooperation approach can be applied to other design domains: For example, when designing an object, the user can quickly make rough shapes with clay, while letting a machine finish the details. We believe this insight can lead the HCI community to further explore design approaches wherein users and machines cooperate for enhanced interaction design.</p>\n<h1>Future Vision</h1>\n<p>We believe this approach’s potential for lowering the barrier and opening new opportunities for a larger community to begin designing, prototyping, and exploring soft material motion—not by coding on a screen, but by sculpting behaviors in the physical world.\nWe envision the future where people can interactively explore various behaviors through tangible interactions, <strong>just like sculpting behaviors with clay</strong>.</p>\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/morphio/figure-14.png" data-lightbox="lightbox"><img src="/static/projects/morphio/figure-14.png" /></a>\n  </div>\n</div>',dir:"content/output/projects",base:"morphio.json",ext:".json",sourceBase:"morphio.md",sourceExt:".md"}},ejaO:function(e){e.exports={id:"tabby",name:"Tabby",description:"Explorable Design for 3D Printing Textures",title:"Tabby: Explorable Design for 3D Printing Textures",authors:["Ryo Suzuki","Koji Yatani","Mark D. Gross","Tom Yeh"],year:2018,booktitle:"The Pacific Conference on Computer Graphics and Applications (Pacific Graphics 2018)",publisher:"The Eurographics Association",pages:"1-4",doi:"https://doi.org/10.2312/pg.20181273",conference:{name:"Pacific Graphics 2018",fullname:"The Pacific Conference on Computer Graphics and Applications (Pacific Graphics 2018)",url:"http://sweb.cityu.edu.hk/pg2018/"},pdf:"pg-2018-tabby.pdf",video:"https://www.youtube.com/watch?v=rRgw8lH74CA",embed:"https://www.youtube.com/embed/rRgw8lH74CA","acm-dl":"https://diglib.eg.org/handle/10.2312/pg20181273",slide:"pg-2018-tabby-slide.pdf",pageCount:4,slideCount:40,bodyContent:'<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/tabby/top.mp4" type="video/mp4"></source>\n</video>\n\n# Abstract\nThis paper presents **Tabby, an interactive and explorable design tool for 3D printing textures**. Tabby allows texture design with direct manipulation in the following workflow: 1) select a target surface, 2) sketch and manipulate a texture with 2D drawings, and then 3) generate 3D printing textures onto an arbitrary curved surface. To enable efficient texture creation, Tabby leverages an auto-completion approach which automates the tedious, repetitive process of applying texture, while allowing flexible customization. Our user evaluation study with seven participants confirms that Tabby can effectively support the design exploration of different patterns for both novice and experienced users.\n\n<div class="figures ui four column grid">\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-1.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-2.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-3.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-3.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-4.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-4.png" /></a>\n  </div>\n</div>\n\n<div class="figures ui four column grid">\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-5.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-5.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-6.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-6.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-7.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-7.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-8.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-8.png" /></a>\n  </div>\n</div>',bodyHtml:'<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/tabby/top.mp4" type="video/mp4"></source>\n</video>\n<h1>Abstract</h1>\n<p>This paper presents <strong>Tabby, an interactive and explorable design tool for 3D printing textures</strong>. Tabby allows texture design with direct manipulation in the following workflow: 1) select a target surface, 2) sketch and manipulate a texture with 2D drawings, and then 3) generate 3D printing textures onto an arbitrary curved surface. To enable efficient texture creation, Tabby leverages an auto-completion approach which automates the tedious, repetitive process of applying texture, while allowing flexible customization. Our user evaluation study with seven participants confirms that Tabby can effectively support the design exploration of different patterns for both novice and experienced users.</p>\n<div class="figures ui four column grid">\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-1.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-2.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-3.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-3.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-4.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-4.png" /></a>\n  </div>\n</div>\n<div class="figures ui four column grid">\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-5.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-5.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-6.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-6.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-7.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-7.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/tabby/figure-1-8.png" data-lightbox="lightbox"><img src="/static/projects/tabby/figure-1-8.png" /></a>\n  </div>\n</div>',dir:"content/output/projects",base:"tabby.json",ext:".json",sourceBase:"tabby.md",sourceExt:".md"}},jEBx:function(e){e.exports={id:"reactile",name:"Reactile",description:"Programming Swarm User Interfaces through Direct Physical Manipulation",title:"Reactile: Programming Swarm User Interfaces through Direct Physical Manipulation",authors:["Ryo Suzuki","Jun Kato","Mark D. Gross","Tom Yeh"],year:2018,booktitle:"In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (CHI '18)",publisher:"ACM, New York, NY, USA",pages:"Paper 199, 13 pages",doi:"https://doi.org/10.1145/3173574.3173773",conference:{name:"CHI 2018",fullname:"The ACM CHI Conference on Human Factors in Computing Systems (CHI 2018)",url:"https://chi2018.acm.org/"},pdf:"chi-2018-reactile.pdf",video:"https://www.youtube.com/watch?v=Gb7brajKCVE",embed:"https://www.youtube.com/embed/Gb7brajKCVE","short-video":"https://www.youtube.com/watch?v=YT7vMJZjohU",slide:"chi-2018-reactile-slide.pdf","acm-dl":"https://dl.acm.org/citation.cfm?id=3173773",github:"https://github.com/ryosuzuki/reactile",pageCount:12,slideCount:56,bodyContent:'<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/reactile/top.mp4" type="video/mp4"></source>\n</video>\n\n# Abstract\n\nWe explore a new approach to programming swarm user interfaces (Swarm UI) by leveraging direct physical manipulation. Existing Swarm UI applications are written using a robot programming framework: users work on a computer screen and think in terms of low-level controls. In contrast, our approach allows programmers to work in physical space by directly manipulating objects and think in terms of high- level interface design. Inspired by current UI programming practices, we introduce a four-step workflow—create elements, abstract attributes, specify behaviors, and propagate changes—for Swarm UI programming. We propose a set of direct physi- cal manipulation techniques to support each step in this work- flow. To demonstrate these concepts, we developed Reac- tile, a Swarm UI programming environment that actuates a swarm of small magnets and displays spatial information of program states using a DLP projector. Two user studies—an in-class survey with 148 students and a lab interview with eight participants—confirm that our approach is intuitive and understandable for programming Swarm UIs.\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-1-1.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-1-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-1-2.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-1-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-1-3.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-1-3.png" /></a>\n  </div>\n</div>\n\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-2-1.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-2-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-2-2.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-2-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-2-3.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-2-3.png" /></a>\n  </div>\n</div>\n\n\n# Introduction\n\nIn recent years, **Swarm User Interfaces** have emerged as a new paradigm of human-computer interaction. While the idea of coordinated miniature robots was originally proposed in the literature of swarm and micro-robotic systems, HCI researchers have explored the use of these robots as a user interface.\nHowever, this opportunity is currently limited to highly skilled programmers who are proficient in robot programming. For typical programmers inexperienced in robot programming who wish to build a Swarm UI application, it is unclear if the robot programming approach is the most appropriate for UI programming. To design interactive UI applications, pro- grammers often must think in terms of higher-level design for user interaction, whereas robot programming tends to focus on low-level controls of sensors and actuators. Historically, a novel UI platform is adopted only after the advent of an effective programming tool that empowers a larger developer community, and even end-users, to create many applications for the platform; for example, HyperCard for interactive hyper- media, Phidgets for physical interfaces, and Interface Builder for GUI applications. We stipulate that current approaches to programming Swarm UI are too robot-centric to be effec- tive for building rich and interactive applications. Then, what would be a better alternative?\n\n\n# Reactile\n\nThis paper introduces Reactile, a programming environment for Swarm UI applications.\nThe goal of Reactile is to explore a new approach to programming Swarm UI applications. To design an appropriate workflow for Swarm UI programming, we look into existing UI programming paradigm for inspiration. The common workflow of UI programming can be decomposed into four basic steps: create elements, abstract attributes, specify behaviors, and propagate changes. Based on these insights, we propose the following four-step workflow for Swarm UI programming: 1) creates shapes, 2) abstracts shape attributes as variables, 3) specifies data-bindings be- tween dynamic attributes, and 4) the system changes shapes in response to user inputs. With this workflow, a programmer can think in terms of high-level interface and interaction design to build interactive Swarm UI appli- cations, compared to existing, low-level, robot programming approaches.\n\n\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-3.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-3.png" /></a>\n  </div>\n</div>\n\nThe workflow of Swarm UI programming is inspired by the existing UI programming paradigm. We first review the common workflow of UI programming and decompose it into four basic elements that represent high-level steps. Then we discuss how to apply this workflow to Swarm UI programming.\nAs we see in well-known design patterns for interactive UI ap- plications such as reactive programming paradigm, the Model-View-Controller, and the observer pattern, they share a com- mon workflow consisting of four basic elements: 1) create elements, 2) abstract attributes, 3) specify behaviors, and 4) propagate changes.\n\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-1-4.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-1-4.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-1-5.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-1-5.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-1-6.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-1-6.png" /></a>\n  </div>\n</div>\n\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-2-4.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-2-4.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-2-5.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-2-5.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-2-6.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-2-6.png" /></a>\n  </div>\n</div>\n\n# Implementation\n\nReactile actuates a swarm of small magnetic markers to move on a 2D canvas with electromagnetic force. We designed and fabricated a board of electromagnetic coil arrays (3,200 coils), which covers an 80 cm x 40 cm area. Reactile tracks the marker positions and detects interactions between a user and swarm markers using a standard RGB camera and computer vision techniques. The system displays spatial information using a DLP projector to allow a programmer to see program states in the same physical context.\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/reactile/coil.mp4" type="video/mp4"></source>\n</video>\n\n<br />\n\nIn Reactile, a user interface consists of a swarm of passive magnetic markers which move on a 2D workspace driven by electromagnetic forces. Reactile uses a grid of electromagnetic coils to actuate these magnetic markers. Running current through the circuit coils generates a local magnetic field so that each coil can attract a single magnet located within its area. Each coil is aligned with a certain offset in both horizontal and vertical direction with an effective area overlap, which allows the coil to attract the magnet located in the adjacent coil. We design electromagnetic coil arrays to be fabricated with a standard printed circuit board (PCB) manufacturing. This reduces the cost and fabrication complexity, making it easy for the actuation area to scale up.\n\nOur PCB design is a 4-layer board, and each layer contains a set of coils, each of which has an identical circular shape with a 15 mm diameter and a 2.5 mm overlap between nearby coils. Each coil has 15 turns with 0.203 mm (8 mils) spacing between lines, and the distance between centers of two coils is approximately 10 mm, which makes a 10 mm grid for attractive points. The final prototype covers an 80 cm x 40 cm area with 80 x 40 coils by aligning five identical boards horizontally. The fabrication of each board costs approximately $80 USD, including manufacturing of PCB and electronic components.\n\n<br />\n\n\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/reactile/mechanism.mp4" type="video/mp4"></source>\n</video>',bodyHtml:'<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/reactile/top.mp4" type="video/mp4"></source>\n</video>\n<h1>Abstract</h1>\n<p>We explore a new approach to programming swarm user interfaces (Swarm UI) by leveraging direct physical manipulation. Existing Swarm UI applications are written using a robot programming framework: users work on a computer screen and think in terms of low-level controls. In contrast, our approach allows programmers to work in physical space by directly manipulating objects and think in terms of high- level interface design. Inspired by current UI programming practices, we introduce a four-step workflow—create elements, abstract attributes, specify behaviors, and propagate changes—for Swarm UI programming. We propose a set of direct physi- cal manipulation techniques to support each step in this work- flow. To demonstrate these concepts, we developed Reac- tile, a Swarm UI programming environment that actuates a swarm of small magnets and displays spatial information of program states using a DLP projector. Two user studies—an in-class survey with 148 students and a lab interview with eight participants—confirm that our approach is intuitive and understandable for programming Swarm UIs.</p>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-1-1.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-1-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-1-2.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-1-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-1-3.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-1-3.png" /></a>\n  </div>\n</div>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-2-1.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-2-1.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-2-2.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-2-2.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-2-3.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-2-3.png" /></a>\n  </div>\n</div>\n<h1>Introduction</h1>\n<p>In recent years, <strong>Swarm User Interfaces</strong> have emerged as a new paradigm of human-computer interaction. While the idea of coordinated miniature robots was originally proposed in the literature of swarm and micro-robotic systems, HCI researchers have explored the use of these robots as a user interface.\nHowever, this opportunity is currently limited to highly skilled programmers who are proficient in robot programming. For typical programmers inexperienced in robot programming who wish to build a Swarm UI application, it is unclear if the robot programming approach is the most appropriate for UI programming. To design interactive UI applications, pro- grammers often must think in terms of higher-level design for user interaction, whereas robot programming tends to focus on low-level controls of sensors and actuators. Historically, a novel UI platform is adopted only after the advent of an effective programming tool that empowers a larger developer community, and even end-users, to create many applications for the platform; for example, HyperCard for interactive hyper- media, Phidgets for physical interfaces, and Interface Builder for GUI applications. We stipulate that current approaches to programming Swarm UI are too robot-centric to be effec- tive for building rich and interactive applications. Then, what would be a better alternative?</p>\n<h1>Reactile</h1>\n<p>This paper introduces Reactile, a programming environment for Swarm UI applications.\nThe goal of Reactile is to explore a new approach to programming Swarm UI applications. To design an appropriate workflow for Swarm UI programming, we look into existing UI programming paradigm for inspiration. The common workflow of UI programming can be decomposed into four basic steps: create elements, abstract attributes, specify behaviors, and propagate changes. Based on these insights, we propose the following four-step workflow for Swarm UI programming: 1) creates shapes, 2) abstracts shape attributes as variables, 3) specifies data-bindings be- tween dynamic attributes, and 4) the system changes shapes in response to user inputs. With this workflow, a programmer can think in terms of high-level interface and interaction design to build interactive Swarm UI appli- cations, compared to existing, low-level, robot programming approaches.</p>\n<div class="figures ui one column grid">\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-3.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-3.png" /></a>\n  </div>\n</div>\n<p>The workflow of Swarm UI programming is inspired by the existing UI programming paradigm. We first review the common workflow of UI programming and decompose it into four basic elements that represent high-level steps. Then we discuss how to apply this workflow to Swarm UI programming.\nAs we see in well-known design patterns for interactive UI ap- plications such as reactive programming paradigm, the Model-View-Controller, and the observer pattern, they share a com- mon workflow consisting of four basic elements: 1) create elements, 2) abstract attributes, 3) specify behaviors, and 4) propagate changes.</p>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-1-4.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-1-4.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-1-5.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-1-5.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-1-6.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-1-6.png" /></a>\n  </div>\n</div>\n<div class="figures ui three column grid">\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-2-4.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-2-4.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-2-5.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-2-5.png" /></a>\n  </div>\n  <div class="figure column">\n    <a href="/static/projects/reactile/figure-2-6.png" data-lightbox="lightbox"><img src="/static/projects/reactile/figure-2-6.png" /></a>\n  </div>\n</div>\n<h1>Implementation</h1>\n<p>Reactile actuates a swarm of small magnetic markers to move on a 2D canvas with electromagnetic force. We designed and fabricated a board of electromagnetic coil arrays (3,200 coils), which covers an 80 cm x 40 cm area. Reactile tracks the marker positions and detects interactions between a user and swarm markers using a standard RGB camera and computer vision techniques. The system displays spatial information using a DLP projector to allow a programmer to see program states in the same physical context.</p>\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/reactile/coil.mp4" type="video/mp4"></source>\n</video>\n<br />\n<p>In Reactile, a user interface consists of a swarm of passive magnetic markers which move on a 2D workspace driven by electromagnetic forces. Reactile uses a grid of electromagnetic coils to actuate these magnetic markers. Running current through the circuit coils generates a local magnetic field so that each coil can attract a single magnet located within its area. Each coil is aligned with a certain offset in both horizontal and vertical direction with an effective area overlap, which allows the coil to attract the magnet located in the adjacent coil. We design electromagnetic coil arrays to be fabricated with a standard printed circuit board (PCB) manufacturing. This reduces the cost and fabrication complexity, making it easy for the actuation area to scale up.</p>\n<p>Our PCB design is a 4-layer board, and each layer contains a set of coils, each of which has an identical circular shape with a 15 mm diameter and a 2.5 mm overlap between nearby coils. Each coil has 15 turns with 0.203 mm (8 mils) spacing between lines, and the distance between centers of two coils is approximately 10 mm, which makes a 10 mm grid for attractive points. The final prototype covers an 80 cm x 40 cm area with 80 x 40 coils by aligning five identical boards horizontally. The fabrication of each board costs approximately $80 USD, including manufacturing of PCB and electronic components.</p>\n<br />\n<video preload="metadata" autoPlay loop muted playsInline webkit-playsinline="">\n  <source src="/static/projects/reactile/mechanism.mp4" type="video/mp4"></source>\n</video>',dir:"content/output/projects",base:"reactile.json",ext:".json",sourceBase:"reactile.md",sourceExt:".md"}},lJku:function(e){e.exports={id:"shapebots",name:"ShapeBots",description:"Shape-changing Swarm Robots",title:"ShapeBots: Shape-changing Swarm Robots",authors:["Ryo Suzuki","Clement Zheng","Yasuaki Kakehi","Tom Yeh","Ellen Yi-Luen Do","Mark D. Gross","Daniel Leithinger"],year:2019,booktitle:"In Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology (UIST '19)",publisher:"ACM, New York, NY, USA",pages:"xxx",doi:"xxx",conference:{name:"UIST 2019",fullname:"The ACM Symposium on User Interface Software and Technology (UIST 2019)",url:"http://uist.acm.org/uist2019"},pageCount:0,slideCount:0,bodyContent:"(Conditionally accepted to UIST 2019. The details are coming soon)",bodyHtml:"<p>(Conditionally accepted to UIST 2019. The details are coming soon)</p>\n",dir:"content/output/projects",base:"shapebots.json",ext:".json",sourceBase:"shapebots.md",sourceExt:".md"}},mRot:function(e){e.exports={id:"refazer",name:"Refazer",description:"Learning Syntactic Program Transformations from Examples",title:"Learning Syntactic Program Transformations from Examples",authors:["Reudismam Rolim","Gustavo Soares","Loris D'Antoni","Oleksandr Polozov","Sumit Gulwani","Rohit Gheyi","Ryo Suzuki","Björn Hartmann"],yera:2017,booktitle:"In Proceedings of the 39th International Conference on Software Engineering (ICSE '17)",publisher:"IEEE Press, Piscataway, NJ, USA",pages:"404-415",conference:{name:"ICSE 2017",fullname:"The International Conference on Software Engineering (ICSE 2017)",url:"http://icse2017.gatech.edu/"},pdf:"icse-2017-refazer.pdf","acm-dl":"http://dl.acm.org/citation.cfm?id=3097417",arxiv:"https://arxiv.org/abs/1608.09000",pageCount:12,slideCount:0,image:"refazer.png",bodyContent:"",bodyHtml:"",dir:"content/output/projects",base:"refazer.json",ext:".json",sourceBase:"refazer.md",sourceExt:".md"}},nWAr:function(e){e.exports={id:"atelier",name:"Atelier",description:"Repurposing Expert Crowdsourcing Tasks as Micro-internships",title:"Atelier: Repurposing Expert Crowdsourcing Tasks as Micro-internships",authors:["Ryo Suzuki","Niloufar Salehi","Michelle S. Lam","Juan C. Marroquin","Michael S. Bernstein"],year:2016,booktitle:"In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI '16)",publisher:"ACM, New York, NY, USA",pages:"2645-2656",conference:{name:"CHI 2016",fullname:"The ACM CHI Conference on Human Factors in Computing Systems (CHI 2016)",url:"https://chi2016.acm.org/wp/"},pdf:"chi-2016-atelier.pdf",video:"https://www.youtube.com/watch?v=tBojZejtFQo",embed:"https://www.youtube.com/embed/tBojZejtFQo",slide:"chi-2016-atelier-slide.pdf","acm-dl":"http://dl.acm.org/citation.cfm?id=2858121",arxiv:"https://arxiv.org/abs/1602.06634",pageCount:12,slideCount:56,image:"atelier.jpg",bodyContent:"",bodyHtml:"",dir:"content/output/projects",base:"atelier.json",ext:".json",sourceBase:"atelier.md",sourceExt:".md"}},o0EK:function(e,i,t){var a={"./atelier.json":"nWAr","./dynablock.json":"GbvX","./flux-marker.json":"CTYI","./mixed-initiative.json":"PSd4","./morphio.json":"X0/d","./pep.json":"W/HP","./reactile.json":"jEBx","./refazer.json":"mRot","./shapebots.json":"lJku","./tabby.json":"ejaO","./trace-diff.json":"Jg5j"};function o(e){var i=n(e);return t(i)}function n(e){var i=a[e];if(!(i+1)){var t=new Error("Cannot find module '"+e+"'");throw t.code="MODULE_NOT_FOUND",t}return i}o.keys=function(){return Object.keys(a)},o.resolve=n,e.exports=o,o.id="o0EK"}},[["D85t","5d41","9da1"]]]);